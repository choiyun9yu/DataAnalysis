# Ada Boosting

## 1. What is AdaBoost?
- AdaBoost 는 Boosting 기법 중 하나로, 여러 약한 학습기를 결합하여 성능이 좋은 강한 학습기를 만드는 앙상블 기법이다.
- 특히 분류 문제에서 자주 사용되며, 약한 학습기는 보통 의사결정 트리와 같은 단순한 모델을 사용한다.
- AdaBoost 는 분류 문제를 해결하는 데 주로 사용되며, 각 학습기가 순차적으로 학습을 강화하여 성능을 향상시킨다.

### 1-1. Boosting 의 기본 개념
- Boosting 은 여러 개의 약한 학습기를 차례대로 학습시키고, 그 학습기들의 출력을 결합하여 최종 예측을 개선하는 방법이다.
- 각 약한 학습기는 이전 학습기들이 만든 오류를 보완하는 방식으로 학습해나간다.
  - 약한 학습기: 개별 성능은 낮지만, 오류율이 랜덤 추측보다는 나은 모델
  - 강한 학습기: 여러 약한 학습기를 결합하여 높은 정확도를 가지는 모델
- 핵심은 각 학습단계에서 잘못 예측되는 데이터 포인트에 더 큰 가중치를 부여하여, 다음 학습기가 이 오류를 고치도록 유도하는 것이다.

### 1-2. AdaBoost 작동 원리
#### 초기화
- 각 데이터 포인트에 대해 동일한 가중치를 부여하여 시작한다.
- 보통 각 데이터 포인트의 초기 가중치는 **$w_1(i) = \frac{i}{N}$** 이다.
  (여기서 N 은 전체 데이터 포인트의 수)

#### 반복 과정
- AdaBoost 는 여러 번의 반복(iteration)을 통해 모델을 개선한다.
  - **약한 학습기 훈련**: 주어진 데이터 셋과 가중치를 사용하여 약한 학습기를 훈련시킨다.
  - **오류율 계산**: 학습기가 예측한 결과를 바탕으로 가중치가 적용된 오류율 $\epsilon_t$ 를 계산한다.
    $$\epsilon_t = \frac{\sum_{i=1}^{N} w_t(i) \cdot I(y_i \neq h_t(x_i))}{\sum_{i=1}^{N} w_t(i)}$$  
    (여기서 I 는 예측이 잘못된 경우 1, 맞으면 0을 반환하는 지시 함수이다.)
  - **학습기의 가중치 계산**: 각 학습기의 중요도를 결정하는 가중치 $\alpha_t$ 를 계산한다.
    $$\alpha_t = \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t} \right)$$
    (이 가중치는 해당 학습기의 성능에 따라 조정되며, 오류율이 낮을수록 가중치가 높아진다.) 
  - **데이터 포인트 가중치 업데이트**: 잘못 분류된 데이터 포인트의 가중치를 증가시키과, 올바르게 분류된 포인트의 가중치를 감소시킨다.
    $$w_{t+1}(i) = w_t(i) \cdot \exp(\alpha_t \cdot I(y_i \neq h_t(x_i)))$$
    (이후 모든 가중치를 정규화하여 합이 1이 되도록 한다.)

#### 최종 모델
- 여러 약한 학습기의 가중치가 반영된 예측 결과를 합산하여 최종 예측을 수행한다.  
  $$H(x) = \text{sign} \left( \sum_{t=1}^{T} \alpha_t \cdot h_t(x) \right)$$  

### 1-3. 장단점
#### 특징 및 장점
- 적응적: AdaBoost 는 각 반복에서 가장 어려운 데이터 포인트에 집중하여 학습하는 방식으로, 이전에 잘못된 예측을 수정하는데 중점을 둔다.
- 간단한 구현: 기본적인 알고리즘이 비교적 간단하며, 다양한 약한 학습기를 사용할 수 있다.
- 강력한 성능: 노이즈에 민감할 순 있지만, 적절한 데이터 셋에서는 매우 강력한 분류 성능을 발휘한다.
- 
#### 단점
- 노이즈 민감도: AdaBoost 는 이상치나 노이즈에 민감하여, 데이터에 노이즈가 많으면 성능이 저하될 수 잇다.
- 계산 비용: 반복적으로 학습기를 훈련시키기 때문에 계산 비용이 높을 수 있다.

#### 정리
- AdaBoost 는 다양한 변형이 존재하며, 회귀 문제에 적용할 수 있는 AdaBoost.RT 와 같은 변형도 있다.
- 현재는 Gradient Boosting, XGBoost 와 같은 더 최신 부스팅 기법들이 많이 사용되지만,
  AdaBoost 는 여전히 중요한 기초 알고리즘으로 많이 언급된다.
