{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형1 : 데이터 전처리 3문제 * 10 [30점]\n",
    "- pandas를 활용한 데이터 탐색 작업에 대한 능력 확인\n",
    "- 주어진 데이터를 활용해 조건에 맞는 데이터를 추출하고 해당 데이터에 대한 통계량 구하거나 개수 세는 작업 요구\n",
    "- 1개의 정수로 답 print()  <- 언제든지 바뀔 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] 사용 라이브러리 import\n",
    "import pandas as pd\n",
    "# 데이터프레임 출력 사이즈 설정\n",
    "pd.options.display.max_rows = 500    \n",
    "pd.options.display.max_columns = 20  \n",
    "# 출력 format 지정 - 소수점아래 4자리까지\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일 불러오기 \n",
    "df = pd.read_csv('파일경로.csv')\n",
    "## 불러오면서 인덱스 컬럼 설정\n",
    "df_csv1 = pd.read_csv('파일경로.csv', index_col='인덱스로 삼을 칼럼명')\n",
    "## 불러오면서 구분자 지정\n",
    "df_csv2 = pd.read_csv('파일경로.csv', sep='\\t')\n",
    "## 불러올 때 언어 설정\n",
    "df_csv3 = pd.read_csv('파일경로.csv', encoding='euc-kr')  # 한글지원 : euc-kr, cp949\n",
    "## 불러올 때 결측치 필터 사용 끄기\n",
    "df_csv2 = pd.read_csv('파일경로.csv', na_filter=False)\n",
    "\n",
    "\n",
    "# excel 파일 불러오기 (엑셀은 엔진 설정해주기!!)\n",
    "df_excel = pd.read_excel('파일경로.xlsx', engine='openpyxl')\n",
    "\n",
    "# csv 파일 저장하기\n",
    "df.to_csv('경로.csv', index=False)\n",
    "# excel 파일 저장하기\n",
    "df.to_excel('경로.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 구조 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이타가 많은 경우 모두 출력 안되고 ... 으로 생략해서 출력됩니다.\n",
    "# 생략되지 않는 행, 열의 개수를 설정하여 생략되지 않고 출력되도록 합니다.\n",
    "pd.set_option('display.max_rows', 800)    #출력할 max row를 지정\n",
    "pd.set_option('display.max_columns', 100)  #출력할 max columns를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 n개 행의 데이터 확인\n",
    "df.head()\n",
    "# 끝부터 n개 행의 데이터 확인\n",
    "df.tail()\n",
    "\n",
    "# 데이터프레임 row개수, colum개수, Not null,dtype 등 정보 확인\n",
    "df.info(memory_usage='deep')\n",
    "# 데이터프레임 통계 정보 확인\n",
    "df.describe()\n",
    "\n",
    "# 데이터프레임의 행, 열의 수를 (행, 열)형태의 튜블로 반환\n",
    "df.shape\n",
    "# 데이터프레임의 (행 x 열)의 전체 데이터 수를 반환\n",
    "df.size\n",
    "# 데이터프레임 데이터 타입 확인\n",
    "df.dtypes\n",
    "\n",
    "# 시리즈\n",
    "sri = df['특정 컬럼']\n",
    "# 시리즈 데이터 타입 확인\n",
    "sri.dtypes\n",
    "\n",
    "#데이터프레임의 인덱스 확인 (보통 0으로 시작해서 몇으로 끝나고 스텝이 몇인지 등을 보여줌)\n",
    "df.index\n",
    "# 데이터프레임의 컬럼 확인\n",
    "df.columns          # 타입 index\n",
    "df.columns.values   # 타입 array\n",
    "# 데이터프레임의 구성요소 2차원으로 보기\n",
    "df.values           # 타입 array\n",
    "\n",
    "# 시리즈의 구성요소\n",
    "sri.values          # 타입 array 해당 시리즈 값 배열로 쭉 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 중 컬럼에 존재하는 값 종류만 확인(결측치 포함, ndrarray로 반환)\n",
    "sri.unique()       \n",
    "# 시리즈 중 컬럼에 존재하는 값 종류의 개수만 확인\n",
    "sri.nunique()\n",
    "# 시리즈 중 컬럼에 대해 값 별 개수 확인(결측치 미포함, Series 반환)\n",
    "sri.value_counts() \n",
    "\n",
    "# 해당 칼럼의 결측치 개수 까지 같이 보고 싶은 경우\n",
    "sri.value_counts(dropna=False)\n",
    "# 해당 값들의 비율을 보고싶은 경우(결측치 비율까지 같이 나옴)\n",
    "sri.value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 타입 변경\n",
    "데이터 타입 변경 전 데이터 조작이 필요할 수 있음 (예) 불필요 문자/콤마/공백 제거, 단위 변환 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 데이터 타입 변경 ('int', 'int32', 'int64', 'float', 'str', 'category' 등)\n",
    "## 넘파이 타입으로 하려면 넘파이 임포트 하여야함 (np.int16, np.float32, np.datetime64 등)\n",
    "sri.astype('타입')\n",
    "\n",
    "# 원하는 자료형으로 바꾸지 못하는 장애요소 제거하고 변환\n",
    "## replace 사용 시 regex=True 옵션을 사용하면 일부 내용만 변경대상으로 지정할 수 있음\n",
    "## 여기서 regex는 '정규표현식'을 의미\n",
    "sri.replace('변경전', '변경후', regex=True)\n",
    "# 변경할 내용이 두가지 이상일 때 list나 dic을 사용하여 변환\n",
    "list1 = ['변경전1', '변경전2']\n",
    "list2 = ['변경후1', '변경후2']\n",
    "sri.replace(list1, list2, regex=True)\n",
    "dic = {'변경전1':'변경후1', '변경전2':'변경후2'}\n",
    "sri.replace(dic, regex=True).astype('int64')\n",
    "\n",
    "## ,콤마는 메타 문자이므로 제거하고자 하는 경우 \\, 처럼 역슬레시 사용\n",
    "## 메타문자 종류 , . + ? * ^ $ 등\n",
    "sri.replace(['\\메타문자', '변견전2'], '', regex=True)  # 변경후 모습이 같다면 하나만 적어도 됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series는 Accessor라는 것을 가지고 있다. Accessor를 사용하기 위해서라도 데이터 타입 변경 필요\n",
    "- dt : Datetime, Timedelta, Period\n",
    "- str : String\n",
    "- cat : Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessor 사용 .str accessor 사용하면 각 행에 문자열 처럼 접근 \n",
    "sri.str[1:-1].astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 np.datetime64 나 category 타입 변경은 astype으로 변경 가능, 특수한 경우는 아래 방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime\n",
    "import numpy as np\n",
    "sri.astype(np.datetime64)  # 일/월/년 순으로 보기도 함 이런 경우 to_datatime을 사용\n",
    "# 내가 원하는 방식으로 날짜를 읽어오고 싶을 때\n",
    "## %Y: 4글자 년도, %y : 2글자 년도, %m : 2글자 월, %d : 2글자 일\n",
    "pd.to_datetime(sri, format='%y-%m-%d')\n",
    "\n",
    "# category\n",
    "sri.astype('category')  # 이렇게 변환한 자료에 sort_values 적용하면 가나다 순 정렬됨 \n",
    "# 카테고리 순서를 직접 지정하고 싶을 때\n",
    "temp = pd.Categorical(sri, categories=[\"범주1\", \"범주2\"], ordered=True)  # 이건 지금 시리즈는 아님\n",
    "sri = temp  # 이건 시리즈\n",
    "sri.sort_values() # 원하는 순서대로 정렬 할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_index() : 인덱스 정렬\n",
    "# [3-24] '측정일시'를 index로 설정하고,\n",
    "# index 기준으로 오름차순 정렬해서 df1으로 이름 붙입니다.\n",
    "# 그래프에서 y축으로 사용하려고 합니다.\n",
    "df1 = df.set_index('측정일시').sort_index()\n",
    "\n",
    "# sort_values() : 밸류 정렬, 디폴트(내림차순)\n",
    "df.sort_values('정렬기준칼럼', ascending=False)\n",
    "# 오름차순인 경우\n",
    "df.sort_values('정렬기준칼럼', ascending=True)\n",
    "# 정렬 기준이 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[False, False])  \n",
    "# 정렬 기준이 여러개, 내림차순 오름차순도 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[True, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 index, columns 및 Series의 index 는 대입연산을 사용하여 변경 가능 다만, 개수가 동일해야 함  \n",
    "  \n",
    "value는 인덱스를 이용해서 변경 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 개수 확인 -> 컬럼 이름 리스트를 대입 연산자로 넣기(단, 개수 동일하게)\n",
    "df.columns = ['컬럼1', '컬럼2', '컬럼3']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼이름 변경하기\n",
    "- DataFrame.rename(columns={'변경전이름':'변경후이름', ...})\n",
    "- DataFrame.rename({'변경전이름':'변경후이름', ...}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-17] bread의 '상세영업상태코드'라는 컬럼명을 '상태코드'로 변경한 뒤,\n",
    "# 첫 2개의 행을 확인합니다.\n",
    "bread = bread.rename(columns={'상세영업상태코드':'상태코드'})\n",
    "bread.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Series.argmax() : 가장 값이 큰 것의 integer index 구하기\n",
    "- Series.argmin() : 가장 값이 작은 것의 integer index 구하기\n",
    "- Series[Series.argmax()] : 가장 큰 값 구하기\n",
    "- Series[Series.argmin()] : 가장 작은 값 구하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인덱싱 할 때 레이블을 쓰면 ['전':'후'] 에서 후 까지 포함  \n",
    "레이블이 아닌 숫자를 쓰면 [1:9] 뒤에 9 미포함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 통계 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 통계 함수\n",
    "- axis=0 : 기본 값으로 행을 이동하면서, 행과 행의 연산을 수행한다.(수직으로 연산)\n",
    "- axis=1 : 컬럼을 이동하며 컬럼과 컬럼의 연산을 수행한다.(수평으로 연산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 통계량 확인\n",
    "df.describe()\n",
    "\n",
    "# 개수 세기\n",
    "df.count()\n",
    "# 합계\n",
    "df.sum()\n",
    "# 노적\n",
    "df.cumsum()\n",
    "# 평균\n",
    "df.mean()\n",
    "# 표준편차 \n",
    "df.std()\n",
    "# 분산\n",
    "df.var()\n",
    "# 중앙값\n",
    "df.median()\n",
    "# 최빈값\n",
    "df.mode()  # 시리즈로 나옴\n",
    "# 최빈값 1개\n",
    "df.mode()[0]\n",
    "# 최대값\n",
    "df.max()   # 최대값, 최소값은 문자열에 대해서도 반응하는데 이는 ord(문자) 코드 숫자 기반\n",
    "# 최소값\n",
    "df.min()\n",
    "\n",
    "# 분위수\n",
    "df.quantile([0.25, 0.5, 0.75])    # 1사분위 0.25, 2사분위 0.5, 3사분위 0.75, \n",
    "# IQR\n",
    "df.quantile(0.75) - sri.quantile(0.25)\n",
    "# 상위20% 이상, 하위 20%도 이하 모두 이상이하로 계산한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통계 함수 여러개 적용하고 싶을 때 1\n",
    "df.apply([\"min\", \"max\", \"mean\"])    # apply 사용하면 어떤 함수든 적용 가능\n",
    "# 통계 함수 여러개 적용하고 싶을 때 2\n",
    "df.agg([\"min\", \"max\", \"mean\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 그룹별 통계\n",
    "- df.groupby(그룹명).통계함수() : 적용가능한 모든 단위\n",
    "- df.groupby(그룹명)[칼럼].통계함수 : Series 단위\n",
    "- df.groupby(그룹명)[[[칼럼1, 칼럼2 ... ]].통계함수 : 특정 컬럼 단위  \n",
    "- 그룹별로 통계치 구할 땐 agg(['var', 'std', 'mode'])로 여러개 가능\n",
    "\n",
    "* 통계 함수는 numerical 값에 적용되기 때문에 object는 형변환이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-54] 대륙별 주류 소비량 중앙값을 계산해 봅니다.\n",
    "df.groupby('대륙').median()\n",
    "\n",
    "# [2-55] 대륙별 맥주 소비량 평균은?\n",
    "df.groupby('대륙')['맥주'].mean()\n",
    "\n",
    "# [2-56] 전세계 맥주 소비량 평균보다 많은 맥주를 소비하는 대륙은?\n",
    "temp = df.groupby('대륙')[['맥주']].mean()\n",
    "temp[temp['맥주']> df['맥주'].mean()]\n",
    "\n",
    "# 그룹바이 기준이 두개 이상인 경우 멀티 인덱스로 작업\n",
    "# [3-34] df_dust에서 '년', '월'별 '미세먼지(㎍/㎥)' 데이터의 평균을 구해\n",
    "# DataFrame으로 만들어 meandf 라는 이름을 지정합니다.ㅣ\n",
    "meandf = df_dust.groupby(['년', '월'])[['미세먼지(㎍/㎥)']].mean()\n",
    "meandf.head()\n",
    "\n",
    "# [3-35] meandf에서 2017년 6월까지의 데이터만 출력합니다.  \n",
    "meandf.loc[:(2017,6),:] # 멀티인덱스 인경우 튜플로 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3-39] df_dust의 일자(년, 월, 일)별 '미세먼지(㎍/㎥)'의 평균을 구합니다.\n",
    "# 인덱스가 유지되지 않음\n",
    "df_dust.groupby(['측정일시'])['미세먼지(㎍/㎥)'].mean()  # <- 동일한 값을 갖음.  MultiIndex 아님\n",
    "# 인덱스가 유지되면서 그룹별 함수 적용\n",
    "df_dust.groupby(['측정일시'])['미세먼지(㎍/㎥)'].transform('mean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 피벗 테이블 : 행, 열 모두에 그룹으 지정하여 통계값 구하기\n",
    "- df.pivo_table(index=행방향칼럼, columns=열방향칼럼, values=집계대상칼럼, aggfunc=통계함수)\n",
    "- index, columns는 범주형, values는 연속형 사용\n",
    "\n",
    "* df.pivot_table(index=행방향그룹열이름, columns=열방향그룹열이름, values=집계대상열이름, aggfunc=통계함수)\n",
    "* index, columns는 범주형, values는 연속형 사용\n",
    "* values, aggfunc의 경우 단독의 경우 출력에 표시되지 않으나, 목록은 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-58] pivot_table을 사용하여 대륙별(index), '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.pivot_table(index='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-59] pivot_table을 사용하여 대륙별(columns), '맥주'와 '와인'의 mean, median값을 구합니다.\n",
    "df.pivot_table(columns='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-60] groupby를 사용하여 대륙별, '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.groupby('대륙')['맥주', '와인'].agg(['mean', 'median', 'max'])\n",
    "# 이건 [2-58] 과 비슷하지만 피벗테이블을 쓰느냐 그룹바이를 쓰느냐에 따라 데이터프레임 구조가 미묘하게 다르다.\n",
    "\n",
    "# [3-48] df_dust의 월/년 별 미세먼지의 'mean', 'min', 'max' 구하기\n",
    "# pivot_table 사용, values의 경우 목록으로 지정시와 단독 지정시가 다르게 표시됨\n",
    "df_dust.pivot_table(index='월', columns='년', values='미세먼지(㎍/㎥)', aggfunc=['mean', 'min', 'max'])\n",
    "\n",
    "# [3-49] df_dust에서 '측정소명'이 '강남구'인 데이터의\n",
    "# 월별(index), 년별(columns), 미세먼지 농도 평균을 조회하여 temp로 저장합니다\n",
    "temp = df_dust.loc[df['측정소명']=='강남구'].pivot_table(index='월', columns='년', values=['미세먼지(㎍/㎥)'], aggfunc=['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3-50] 2016년 ~ 2020년도 미세먼지 농도가 가장 높은 월의 위치\n",
    "temp = df_dust.pivot_table(index='월', columns='년', values='미세먼지(㎍/㎥)', aggfunc='mean')\n",
    "for year in temp.columns:\n",
    "    idx=temp[year].argmax()\n",
    "    print(temp.index[idx])\n",
    "\n",
    "# [3-52] 2016년 ~ 2019년 월별 미세먼지 평균을 구해 temp (DataFrame)로 저장합니다.\n",
    "temp = df_dust.loc[df_dust['년']<=2019].groupby('월')[['미세먼지(㎍/㎥)']].mean() \n",
    "\n",
    "# [3-52] 2016년 ~ 2019년 월별 미세먼지 평균을 구해 temp (DataFrame)로 저장합니다.\n",
    "temp = df_dust.loc[df_dust['년']<=2019].groupby('월')[['미세먼지(㎍/㎥)']].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Index, Columns 상호변경"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Columns to Index : df.set_index(['인덱스로 이동시킬 컬럼 명'..])\n",
    "- Index to Columns : df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-32] 국가별 주류 소비량 합계(맥주, 증류주, 와인의 합)를 구해 봅니다.\n",
    "df.set_index('국가')[['맥주', '증류주', '와인']].sum(axis=1)\n",
    "\n",
    "# [2-33] df를 ['대륙', '국가']를 index로 지정하고, 대륙별, 국가명으로  정렬하여 df로 저장합니다.\n",
    "df = df.set_index(['대륙','국가']).sort_index()\n",
    "\n",
    "# [2-34] df의 index를 모두 columns로 이동합니다.\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 정제하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 결측치\n",
    "- nan : 넘파이 배열에서 결측치 나타내는 경우\n",
    "- NaN : 판다스에서 시리즈나 데이터프레임에서 결측치 나타내는 경우 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 찾기\n",
    "df.isna()\n",
    "df.isnull()\n",
    "# 결측치 합계 구하기\n",
    "df.isna().sum()\n",
    "\n",
    "# 결측치가 아닌 것 찾기\n",
    "df.notna()\n",
    "df.notnull()\n",
    "\n",
    "# 불리언 인덱싱으로 결측치만 찾기\n",
    "df[df['컬럼'].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 범주형 데이터 : 다른 범주로 만들어 채우기\n",
    "- 연속형 데이터 : 0으로 채우기, 평균값으로 채우기, 범주별 평균값으로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임의 결측치 채우기 (모두 같은 값으로)\n",
    "df = df.fillna('대체값')\n",
    "\n",
    "# 특정한 하나의 컬럼 결측치 채우기\n",
    "df['칼럼'] = df['칼럼'].fillna('대체값')\n",
    "df.loc[df['칼럼'].isna(), '대륙'] = '대체값'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "- 결측치 제거에 사용되는 메서드\n",
    "- how='any' : 결측치가 하나라도 포함된 행 삭제\n",
    "- how='all' : 모든 데이터가 결측치인 행 삭제\n",
    "- axis=1 : 컬럼에 대해 동작\n",
    "- thresh=숫자 : 숫자 이상의 데이터를 가진 행은 삭제 안함\n",
    "- subset=[컬럼이름1, ...] : subset으로 지정된 컬럼만 사용하여 삭제 대상 검색"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "df.dropna()\n",
    "\n",
    "# [3-30] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 모든 데이터가 결측치인 행을 제거하여 결과를 temp1으로 저장합니다\n",
    "temp1 = df_dust.dropna(how='all', axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])\n",
    "\n",
    "# [3-32] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 2개 이상의 데이터를 가진 행은 제거하지 않은 결과를 temp3로 저장합니다.\n",
    "# (= 3개의 정보 중 1개의 데이터만 가진 행을 제거함)\n",
    "temp3 = df_dust.dropna(thresh=2, axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 결측치 대체\n",
    "Series.mask(조건, 조건이 참일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 True인 것에 대해 다른 값을 변경합니다.\n",
    "- sri.isna() : NA값에 대해 True, NA아닌 것은 False\n",
    "\n",
    "Series.where(조건, 조건이 거짓일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 False인 것에 대해서 다른 값으로 변경합니다.\n",
    "- sri.notna() : NA값에 대해 False, NA아닌 것은 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp의 'A' 열에 대해서 결측치인 경우 'B'의 값으로 대체합니다.\n",
    "temp['A'].mask(temp['A'].isna(), temp['B'])\n",
    "# temp의 'A' 열에 대해서 결측치인 경우 'C'의 값으로 대체합니다.\n",
    "temp['A'].where(temp['A'].notna(), temp['C'])\n",
    "\n",
    "# [3-43] meandf에 '결측치대체' 및 '차이2'라는 컬럼을 추가합니다.\n",
    "# '결측치대체' 컬럼은 df_dust에서 '년', '월'별 '미세먼지(㎍/㎥)' 데이터의 평균을 사용합니다.\n",
    "# '차이2' 컬럼은 '미세먼지(㎍/㎥)' - '결측치대체'를 사용합니다.\n",
    "meandf['결측치대체'] = df_dust.groupby(['년','월'])['미세먼지(㎍/㎥)'].mean()\n",
    "\n",
    "# [3-45] df_dust의 '오존농도(ppm)', '초미세먼지(㎍/㎥)' 컬럼에 대해서도\n",
    "# '미세먼지(㎍/㎥)'와 같이 동일한 '년', '월', '일'의 평균 값으로 채우기 합니다.\n",
    "fine_dust = df_dust.groupby('측정일시')['오존농도(ppm)'].transform('mean')\n",
    "s = df_dust['오존농도(ppm)']\n",
    "df_dust['오존농도(ppm)'] = s.mask(s.isna(), fine_dust)\n",
    "\n",
    "fine_dust = df_dust.groupby('측정일시')['초미세먼지(㎍/㎥)'].transform('mean')\n",
    "s = df_dust['초미세먼지(㎍/㎥)']\n",
    "df_dust['초미세먼지(㎍/㎥)'] = s.mask(s.isna(), fine_dust)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 이상치"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 이상치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 시리즈의 이상치를 확인하는 박스플롯 그리기\n",
    "sri.plot.box(figsize=(3,4))\n",
    "plt.show()\n",
    "\n",
    "# 데이터프레임의 연속형 데이터에 대해 모두 박스플롯 그리기\n",
    "df.plot(kind='box', subplots=True)\n",
    "plt.show()\n",
    "\n",
    "# [5] ESD(Extream Studentized Diviate)를 이용한 방법\n",
    "# 평균으로 부터 3 표준편차 떨어진 값을 이상치로 판단\n",
    "# tip에 대한 이상치 구하기, 소수점 아래 2째자리까지 표기\n",
    "s = tips['tip']\n",
    "s_mean, s_std = s.mean(), s.std()\n",
    "e_lower =  round(s_mean - 3 * s_std, 2) # round(숫자, 소수이하표시자리)\n",
    "e_upper =  round(s_mean + 3 * s_std, 2)\n",
    "print(f'Lower: {e_lower}, Upper: {e_upper}')\n",
    "# 소수점 아래 2째자리 까지 표기는 출력시에만 그렇지 않으면 이상치 대체할 때 박스플롯 밖에 점 생성\n",
    "\n",
    "# [6] 사분위수를 이용한 방법\n",
    "# Q1 - 1.5*IQR 미만, Q3 + 1.5*IQR 초과 를 이상치로 판단 (IQR = Q3 - Q1)\n",
    "# tip에 대한 이상치 구하기, 소수점 아래 2째자리까지 표기\n",
    "s = tips['tip']\n",
    "Q1, Q3 = s.quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "q_lower = Q1 - 1.5 * IQR\n",
    "q_upper = Q3 + 1.5 * IQR\n",
    "print(f'Lower: {round(q_lower,2)}, Upper: {round(q_upper,2)}')\n",
    "# 소수점 아래 2째자리 까지 표기는 출력시에만 그렇지 않으면 이상치 대체할 때 박스플롯 밖에 점 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정상범주에 있는 데이터를 indexing 하는 방법\n",
    "# [7] 이상치 제거 - 정상범주에 있는 데이터를 indexing 하는 방법으로 처리 (q_lower, q_upper 사이값이 정상)\n",
    "condition1 = tips['tip'] >= q_lower\n",
    "condition2 = tips['tip'] <= q_upper\n",
    "tips2 = tips.loc[condition1 & condition2, 'tip']\n",
    "print(tips.shape, tips2.shape)\n",
    "\n",
    "# [8] 이상치 데이터 모음\n",
    "condition1 = tips['tip'] <= q_lower\n",
    "condition2 = tips['tip'] >= q_upper\n",
    "tips_outlier = tips.loc[condition1 | condition2]\n",
    "tips_outlier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 이상치 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9] 이상값 대체\n",
    "# q_upper 보다 큰 데이터는 q_upper, q_lower 보다 작은 데이터는 q_lower로 변경\n",
    "# tips3 데이터프레임에서 'tip' 값이 q_upper 보다 큰 것의 'tip'값을 q_upper 값으로 변경\n",
    "# tips3 데이터프레임에서 'tip' 값이 q_lower 보다 작은 것의 'tip'값을 q_lower 값으로 변경\n",
    "tips3 = tips.copy()\n",
    "tips3.loc[tips3['tip'] > q_upper, 'tip'] = q_upper\n",
    "tips3.loc[tips3['tip'] < q_lower, 'tip'] = q_lower\n",
    "\n",
    "tips3['tip'].plot.box()\n",
    "plt.show()\n",
    "# 소수점 아래 2째자리 까지 표기는 출력시에만 그렇지 않으면 이상치 대체할 때 박스플롯 밖에 점 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 데이터 변환\n",
    "이상치를 완화하거나, 정규분포가 되도록 하기위해 사용, numpy의 log1p, sqrt, expm1, power 등의 함수 사용\n",
    "- log1p, sqrt는 큰 값을 작게 만들어주며, 오른쪽 꼬리가 긴 근포를 정규분포로 변환하는데 사용, 큰 이상치를 작게 만들 수 있음\n",
    "- expm1(exp마이너스원), power는 작은 값을 크게 만들어 주며, 왼쪽 꼬리가 긴 분포를 정규분포로 변환하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] tips에서 tip의 분포 확인  (어느쪽 꼬리가 긴지 확인하기 위함)\n",
    "s = tips['tip']\n",
    "s.plot.hist(bins=20)    # bin은 몇개 구간으로 나눌지 나타냄\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] y = log(x) 이해 -> np.power(np.e, y) = x  \n",
    "# np.log(100) : log만 덩그러니 적혀있으면 자연로그e(np.e)로 해석,\n",
    "# np.log10(100) : 10을 몇번 거듭제곱해야 100이라는 숫자가 되는지 구함 \n",
    "# np.power(10, 2) : 앞의 수를 뒤의 수로 거듭제곱하면 나오는 수 구함\n",
    "print(np.log10(100), np.power(10,2))\n",
    "print(np.e, np.log(100), np.power(np.e, 4.605170185988092))\n",
    "\n",
    "# [3] y = exp(x) 이해 -> y = power(np.e, x)\n",
    "# exp(x)는 np.e에 x만큼 거듭제곱하라는 뜻\n",
    "print(np.exp(4.605170185988092), np.power(np.e, 4.605170185988092))\n",
    "\n",
    "# [4] log(0) -> x가 0인 경우 -inf 이기 때문에 x에 +1을 해서 동작하는 np.log1p를 사용함  // -inf는 무한대라는 뜻\n",
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] tip의 분포를 오른쪽 꼬리가 긴 것에 대해서 짧게 만들기 (로그)\n",
    "s = np.log1p(sri)   # 시리즈 데이터 상ㅂ입\n",
    "s.plot.hist(bins=20)\n",
    "plt.show()\n",
    "\n",
    "# [6] tip의 분포를 오른쪽 꼬리가 긴 것에 대해서 짧게 만들기  (제곱근)\n",
    "s = np.sqrt(tips['tip'])\n",
    "s.plot.hist(bins=20)\n",
    "plt.show()\n",
    "\n",
    "# [7] tips['tip'] 원본 -> log1p -> expm1 = 원본 (로그 취했던 것)\n",
    "a = tips['tip']\n",
    "b = np.log1p(a)  # 변환\n",
    "c = np.expm1(b)\n",
    "print(a[:3], b[:3], c[:3], sep='\\n\\n')\n",
    "\n",
    "# [8] tips['tip'] 원본 - sqrt - power = 원본 (제곱근 취했던 것)\n",
    "a = tips['tip']\n",
    "b = np.sqrt(a)\n",
    "c = np.power(b, 2)\n",
    "print(a[:3], b[:3], c[:3], sep='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 파생 변수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 포함된 목록 확인\n",
    "- sri.isin() : \n",
    "- '찾을내용' in str : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].isin(['TV/방송', '게임'])]  # 또는 조건 isin으로도 해결 가능\n",
    "\n",
    "# isin([]) 목록이 한개여도 리스트 타입으로 넣어줘야 함\n",
    "df.loc[df['category'].isin(['음악/댄스/가수'])].sort_values('subscriber', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-26] bread에서 '사업장명' 컬럼을 사용하여\n",
    "# '파리바게트', '파리바게뜨' 이름인 곳을 뽑아 paris로 이름 붙입니다.\n",
    "bread[bread['사업장명'].str.contains('파리바게트', '파리바게뜨')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Accessor 사용\n",
    "Series 타입에 .str을 붙여서 문자열 메소드 쓰는 것\n",
    "- str.contains('문자열') : 특정 문자열을 포함하는지 아닌지를 True/False로 반환 (Boolean Indexing 조건으로 사용 가능)\n",
    "- str.upper() : 영문자 소문자를 대문자로 변경  /  str.lower() : 영문자 대문자를 소문자로 변경  \n",
    "- 세부 내용 : https://pandas.pydata.org/docs/reference/series.html#string-handling\n",
    "\n",
    "Series의 데이터를 list 및 ndarray로 반환 (데이터프레임 말고 시리즈에서 변환하는 것)\n",
    "- Series.to_list() : Series의 values를 list로 반환\n",
    "- Series.to_numpy() : Series의 values를 ndarray로 반환 (Series.values 와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1-44] title에 'KBS'가 포함된 채널 명 목록을 만들어 봅니다.\n",
    "df[df['title'].str.contains('KBS')]\n",
    "# 대소문자 구분 없이 검색 하려면? !str 두번 쓰면 됨\n",
    "df[df['title'].str.lower().str.contains('kbs')]\n",
    "\n",
    "# Series 형태를 list나 array 형태로 변환하기\n",
    "df[df['title'].str.contains('KBS')]['title'].to_list()\n",
    "df[df['title'].str.contains('KBS')]['title'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime에서 년 정보만 가져오기\n",
    "df['측정일시2'].dt.year\n",
    "# datetime에서 월 정보만 가져오기\n",
    "df['측정일시2'].dt.month\n",
    "# datetime에서 일 정보만 가져오기\n",
    "df['측정일시2'].dt.day\n",
    "# 월요일이 0 일요일이 6\n",
    "df['측정일시2'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-8] bread 의 '소재지전체주소' 중 시/도에 대한 정보(목록)를 추출합니다.\n",
    "bread['소재지전체주소'].str.split(' ').str[0]   # str 여러번 붙여 써도 괜찮음\n",
    "\n",
    "# [4-9] bread에서 소재지전체주소의 처음이 '서울특별시'이면서,\n",
    "# '업태구분명'이 '제과점영업'인 것만 추출합니다.\n",
    "bread[(bread['업태구분명']=='제과점영업') & (bread['소재지전체주소'].str.split(' ').str[0]=='서울특별시')]\n",
    "# 이렇게 정리해도 괜찮은 듯\n",
    "condition1 = bread['업태구분명']=='제과점영업'\n",
    "condition2 = bread['소재지전체주소'].str.split(' ').str[0]=='서울특별시'\n",
    "bread = bread[condition1 & condition2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- str.strip('제거할 문자들') : 문자열의 앞/뒤에 불필요한 것을 제거함\n",
    "   - 제거할 문자들을 지정하지 않을 경우 whitespace를 제거함\n",
    "- str.split('구분자')\n",
    "   - 구분자를 지정하지 않을 경우 whitespace를 기준으로 분리함\n",
    "   - 각 구분된 내용은 str[0], str[1], .. 등으로 접근\n",
    "- str.join('구분자')\n",
    "   - 구분자 지정을 생략할 수 없음\n",
    "   - 분리된 문자열을 구분자를 사이에 넣어 하나의 문자열로 만듦\n",
    "- str.replace(전, 후)\n",
    "   - 문자열의 일부 내용을 변경 가능함\n",
    "   - 변경전 내용을 찾아 변경후 내용으로 바꿈   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'A': ['    김   수민 ', '  이  나라     ', '  황   소  라  '],\n",
    "        'B': ['  d2021-10-29.   ', '   \\n\\t\\r2021-10-30c    \\n', '2021-11-01c   '],\n",
    "        'C': ['*7', '6', '*7']}\n",
    "temp = pd.DataFrame(data)\n",
    "print(temp)\n",
    "\n",
    "# [1] 'A' 컬럼의 데이터를 빈칸 없는 이름으로 만들어 temp의 'A'컬럼 뒤에 'A-01'컬럼으로 추가해 보세요.\n",
    "temp.insert(1, column=' A-01', \n",
    "            value=temp['A'].str.split().str.join(''))        \n",
    "\n",
    "# [2] 'B' 컬럼의 데이티를 2021-10-29 처럼 앞/뒤에 공백이나 다른 문자('.dc')가 없도록 만들어\n",
    "# temp에 'B-01' 컬럼으로 추가해 보세요.\n",
    "temp.insert(3, column='B-01', \n",
    "            value=temp['B'].str.strip().str.strip('d.c'))\n",
    "\n",
    "# [3] 'B-01' 컬럼의 데이터에서 '-'를 '/'로 수정해 temp에 'B-02' 컬럼으로 추가해 보세요.\n",
    "temp.insert(4, column='B-02',\n",
    "            value=temp['B-01'].str.replace('-', '/'))\n",
    "\n",
    "# [4] 'C' 컬럼에서 *을 제거하고 숫자로 변경해 'C-01'컬럼으로 추가해 보세요.\n",
    "temp['C-01'] = temp['C'].str.replace('*','').astype(int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 불리언 인덱싱\n",
    "- df.loc[조건] : 조건은 boolean dtype이어야함 (기호 : | 선언, & 연언, ~ 부정) !!참 거짓으로 나올 수 있어야함\n",
    "- df.iloc[행범위, 열범위] : 인덱스 번호로 인덱싱 혹은 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예시 1-36] 'category'가 '음악/댄스/가수'인 채널의 subscriber TOP5를 알아봅니다.\n",
    "df.loc[df['category']=='음악/댄스/가수'].sort_values('subscriber').head()\n",
    "\n",
    "# [2-23] 와인 소비량이 맥주 + 증류주 소비량보다 큰 나라를 검색해,'대륙'을 기준으로 정렬해 보자\n",
    "df.loc[(df['와인']) > (df['맥주']+df['증류주'])].sort_values('대륙')\n",
    "\n",
    "# [2-24] 맥주 소비량이 230 초과이면서, 와인 소비량이 230 초과인 나라를 검색해 보자\n",
    "df.loc[(df['맥주']>230) & (df['와인']>230)]['국가']\n",
    "\n",
    "# [2-25] 대륙이 'AS'인 국가들의 정보를 검색해 보자\n",
    "df.loc[df['대륙']=='AS'].info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참 거짓으로 나오지 않는 사칙연산 같은 건 loc 쓰지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-27] 주류소비량2 = ['맥주', '증류주', '와인']에 대해 DataFrame.sum(axis=1) 함수 사용\n",
    "df['주류소비량2'] = df[['맥주', '증류주', '와인']].sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 추가, 제거 및 병합\n",
    "- 행 추가 : df.append(추가할 데이터프레임)\n",
    "- 컬럼 추가1 : df['새로운칼럼명'] = \n",
    "- 특정 위치에 컬럼 추가2 : df.insert(위치, 컬럼, 값) 단, inplace 동작됨\n",
    "- 열 제거1 : df = df.drop('열이름')    \n",
    "- 열 제거2 : df = df.drop(rows=['열이름1', '열이름2' ...])\n",
    "- 컬럼 제거1 : df = df.drop('칼럼명', axis=1)\n",
    "- 컬럼 제거2 : df = df.drop(columns=[컬럼명...])\n",
    "- 컬럼 제거3 : del df['제거할칼럼명'] 단, inplce 동작됨\n",
    "- 데이터프레임 행/열 전환 : df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-40] 세계의 각 컬럼별 평균을 구하여 DataFrame으로 만들고,\n",
    "# worldwide라는 이름을 지정합니다\n",
    "# 세계의 각 컬럼별 평균은 DataFrame.mean()을 사용합니다.\n",
    "worldwide = pd.DataFrame(df.mean(axis=0))\n",
    "\n",
    "# [2-41] worldwide의 행과 열을 전환해 wwT로 저장합니다.\n",
    "wwT = worldwide.T\n",
    "wwT\n",
    "\n",
    "# [2-42] wwT의 맨 앞에 '국가' 컬럼을 'World Wide' 값으로 추가합니다.\n",
    "# 여러 번 추가하면 안됨\n",
    "wwT.insert(0,'국가','World Wide') # 값을 여러개 줄 땐 리스트로 묶어줘야 한다.\n",
    "wwT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행 병합 : concat([합칠행1, 합칠행2] ..., axis=0)\n",
    "- 열 병합 : concat([합칠열1, 합칠열2] ..., axis=1)\n",
    "* df.append()와 달리 여러 개의 df를 목록을 주어 한 번에 합칠 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-44] wwT와 korea 를 합쳐 하나의 DataFrame을 생성하여 df2로 저장합니다.\n",
    "df2 = pd.concat([wwT,korea], axis=0)\n",
    "df2\n",
    "\n",
    "# [3-5] df2016, df2017, df2018, df2019를 합쳐 한 개의 DataFrame으로 만들어 df라는 이름을 지정합니다.\n",
    "dfList = [df2016, df2017, df2018, df2019]\n",
    "df = pd.concat(dfList, axis=0)\n",
    "\n",
    "# concat을 하더라도 인덱스 번호는 유지되기 때문에 인덱스번호 다시 설정해줘야함\n",
    "df.index = pd.RangeIndex(len(df))\n",
    "# concat 할때부터 옵션으로 인덱스 잡아주기\n",
    "df = pd.concat(dfList, ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-20] bread에 '설립년도' 및 '폐업년도' 컬럼을 추가합니다.\n",
    "# '인허가일자'//10000, '폐업일자 // 10000 을 사용하여 구합니다.\n",
    "# 두 개의 컬럼이 추가된 bread의 첫 2개 행을 확인합니다.\n",
    "\n",
    "# 정수형인경우\n",
    "year = bread['인허가일자'] // 10000\n",
    "meonth = bread['인허가일자'] // 100 % 100\n",
    "day = bread['인허가일자'] %100\n",
    "\n",
    "# datetime 으로 바꿔서\n",
    "temp = pd.to_datetime(bread['인허가일자'], foramt='%Y,%m%d')\n",
    "temp.dt.year\n",
    "temp.dt.moth\n",
    "temp.dt.day\n",
    "\n",
    "# 문자열인경우 (슬라이싱으로)\n",
    "\n",
    "\n",
    "# 이번에는 정수형으로 함\n",
    "bread['설립년도'] = bread['인허가일자']//10000\n",
    "bread['폐업년도'] = bread['폐업일자']//10000\n",
    "bread.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-21] bread에 '영업기간' 컬럼을 추가합니다\n",
    "# '영업기간'은 '상태코드'가 1(=영업)인 경우 2021 - 설립년도 +1\n",
    "# '상태코드'가 2(=폐업)인 경우 폐업년도 - 설립년도 + 1로 계산합니다.\n",
    "\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "today.year\n",
    "today.month\n",
    "today.day\n",
    "\n",
    "nyear = today.year\n",
    "bread.loc[bread['상태코드']==1,'영업기간'] = nyear - bread['설립년도'] + 1\n",
    "bread.loc[bread['상태코드']==2, '영업기간'] = bread['폐업년도'] - bread['설립년도'] + 1\n",
    "bread.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-22] bread의 '설립년도'별 데이터 수를 구해 년도별로 정렬하고,\n",
    "# DataFrame으로 변경하여 전치행렬을 구해 temp1 이름을 부여해 출력합니다.\n",
    "temp1 = bread['설립년도'].value_counts().sort_index(ascending=True)\n",
    "temp1 = pd.DataFrame(temp1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-28] 설립년도가 2000년 이후이면서 영업 중인 곳의 영업기간 정보를 구합니다.\n",
    "# paris, tous에 대해 각각 구해서 temp1, temp2로 이름 붙입니다.\n",
    "temp1 = paris.loc[(paris['설립년도']>=2000) & (paris['상태코드']==1), '영업기간']\n",
    "temp2 = tous.loc[(tous['설립년도']>=2000) & (tous['상태코드']==1), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 이름을 comp로 하는 DataFrame으로 만듭니다.\n",
    "# index => ['파리바게트', '뚜레쥬르'], columns => ['영업']\n",
    "# 선생님방법\n",
    "s = pd.Series([temp1.mean(), temp2.mean()], index=['파리바게트', '뚜레쥬르'])\n",
    "comp = pd.DataFrame(s, columns=['영업'])\n",
    "comp\n",
    "\n",
    "# [4-29] 설립년도 2000년 이후이면서 폐업한 곳의 영업기간 정보를 구합니다.\n",
    "# paris, tous에 대해 각각 구해서 temp1, temp2로 이름 붙입니다.\n",
    "temp1 = paris.loc[(paris['설립년도']>=2000) & (paris['상태코드']==2), '영업기간']\n",
    "temp2 = tous.loc[(tous['설립년도']>=2000) & (tous['상태코드']==2), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 comp에 '폐업' 컬럼으로 추가합니다.\n",
    "comp['폐업']= [temp1.mean(), temp2.mean()]\n",
    "comp\n",
    "\n",
    "# [4-31] other의 2000년 이후 설립된 곳의 영업, 폐업 사업장을 구한 뒤\n",
    "# temp1, temp2 이름을 붙입니다.\n",
    "temp1 = other.loc[(other['설립년도']>=2000) & (other['상태코드']==1), '영업기간']\n",
    "temp2 = other.loc[(other['설립년도']>=2000) & (other['상태코드']==2), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 comp 에 '나머지' 행으로 추가합니다.\n",
    "temp = pd.DataFrame([[temp1.mean(), temp2.mean()]], index=['나머지'], columns=['영업', '폐업'])\n",
    "comp = comp.append(temp)\n",
    "comp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 데이터 표본 추출(샘플링)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- df.sample(n=개수, frac=비율, random_state=None, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 무작위 추출\n",
    "df.sample(frac=0.02, random_state=777)\n",
    "df.sample(n=5, random_state=777)\n",
    "\n",
    "# 계통 추출 : 시작 위치부터 일정한 간격으로 추출하는 것 \n",
    "df.iloc[::50,:]   # 인덱스 50단위로 계통 추출\n",
    "\n",
    "# 층화 추출 : 독립변수의 unique값이 실제 분포하는 비율에 맞춰서 데이터 추출\n",
    "# [5] 층화 추출 - 성별을 기준으로 성별 비율에 맞춰 10개 데이터 추출\n",
    "# [5-1] 성별 비율 구하기(rate), 추출할 sample 개수 구하기(sample_n)\n",
    "sample_n = 20   # 전체적으로 사용할 샘플의 개수\n",
    "temp = tips['sex'].value_counts(normalize=True).to_frame()\n",
    "temp.columns=['rate']\n",
    "temp['sample_n'] = round(sample_n * temp['rate'], 0).astype('int')\n",
    "# [5-2] Female, Male로 데이터 분리 및 sample개수 만큼의 임의 표본 추출\n",
    "Female = tips.loc[tips['sex']=='Female']\n",
    "Male = tips.loc[tips['sex']=='Male']\n",
    "df1 = Female.sample(n=temp.loc['Female', 'sample_n'], random_state=1)\n",
    "df2 = Male.sample(n=temp.loc['Male', 'sample_n'], random_state=1)\n",
    "df = pd.concat([df1, df2])\n",
    "df.sample(frac=1, random_state=1)   # 섞기 위해서 frac=1 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 도수분포표"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관측 값을 몇 개 범주로 나눈 다음 그 범주에 속하는 관측 값의 개수(도수)와 그 도수를 전체 관측 값의 개수로 나눈 값(상대도수)에 대한 표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 데이터 생성하기\n",
    "data = ['A+', 'A', 'B+', 'B', 'C+', 'C', 'D+', 'D', 'F']\n",
    "cnt  = [3, 6, 12, 16, 10, 8, 4, 1, 2]\n",
    "mylist = []\n",
    "for s, c in zip(data, cnt):\n",
    "    mylist.extend([s] * c)\n",
    "\n",
    "df = pd.DataFrame(mylist, columns=['data'])\n",
    "df = df.sample(frac=1)\n",
    "print(df['data'].to_list())\n",
    "\n",
    "# [2] mylist를 사용하여 돗수분포표를 만들어 본다\n",
    "s = pd.Categorical(df['data'], categories=data, ordered=True)\n",
    "s = s.value_counts().sort_index()\n",
    "print(s)\n",
    "\n",
    "# [3] s를 사용하여 돗수분포표의 비율, 누적인원, 누적비율을 완성해 본다\n",
    "df = pd.DataFrame(s, columns=['인원'])\n",
    "df.index.name = '학점'\n",
    "df['비율'] = (df['인원']/ df['인원'].sum()).round(2)\n",
    "df['누적인원'] = df['인원'].cumsum()\n",
    "df['누적비율'] = df['비율'].cumsum()\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 상관계수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] 데이터 불러오기\n",
    "# sns.load_dataset(데이터셋의 이름)\n",
    "# iris(붓꽃) 데이터를 불러옴\n",
    "# sepal : 꽃받침, petal : 꽃잎\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()\n",
    "\n",
    "# [1] species를 LabelEncoding하여 'species_LE' 컬럼 추가하기\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "ans = LabelEncoder().fit_transform(iris['species'])\n",
    "\n",
    "# [2] iris의 상관계수 구하기\n",
    "iris['species'] = LabelEncoder().fit_transform(iris['species'])\n",
    "\n",
    "iris.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형2 : 데이터 모델링 1문제 * 40 [40점]\n",
    "- pandas 및 sklearn을 활용한 머신러닝 작업에 대한 능력 확인\n",
    "- 주어진 데이터를 활용해 전처리, 모형 구축, 평가 작업을 한 뒤\n",
    "- 예측 결과를 csv파일로 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝 과정\n",
    "- 데이터 로드\n",
    "- 데이터 전처리 (X와 X_sub에 동일한 전처리를 위해 두 데이터 결합해서 dfX로 명명)  \n",
    "dfX = pd.concat([X, Xsub], axis=0, ignore_index=True)\n",
    "    - 결측치 없도록 한다 (결측치 제거 / 결측치 대체(평균, 중앙값))\n",
    "    - dtype : int, float만 가능 (범주형은 인코딩, 수치형은 스케일링)  \n",
    "    (문제지와 실제 데이터 타입 다르면 문제지에 맞게 바꿔줘야한다)\n",
    "    - 이상치 관리 (이상치 대체(최소값, 최대값 대체))\n",
    "    - 상관관계가 0.9이상 -0.9이하의 특성 값들은 하나 제거\n",
    "- 학습 모델 생성 및 학습\n",
    "    - 부차적인 것(피쳐 엔지니어링, 하이퍼파라미터 수정, 모델 선택 등 )\n",
    "- 성능 평가\n",
    "- 제출파일 작성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 스케일링(수치형 데이터 스케일링)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min-max normalization : 값의 범위를 [0, 1]로 변환\n",
    "* (xi - x.min()) / (x.max() - x.min())   \n",
    "\n",
    "standardiztion : 특성의 값이 표준 정규분포를 갖도록 변환 (평균 0, 표준편차 1)\n",
    "- (xi - x.mean()) / x.std()\n",
    "- from sklearn.preprocessing import StandardScaler  \n",
    "res = StandardScaler.fit_transform(X_train)\n",
    "\n",
    "#### 스케일러는 하나 만들어서 그거로 X_train 데이터 스케일링 하고, X_test 데이터에도 적용\n",
    "Sc = StandartScaler.fit(Xtain)  \n",
    "X = Sc.transform(Xtrain)  \n",
    "Xsub = Sc.transform(Xtest)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Scaling을 위한 함수 구현\n",
    "def minmaxScale(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "def standardScale(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "# [2] minmaxScale 함수를 사용한 'tip' 컬럼의 스케일링\n",
    "x = minmaxScale(tips['tip'])\n",
    "\n",
    "# [3] standardScale 함수를 사용한 'tip' 컬럼의 스케일링\n",
    "x = standardScale(tips['tip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] sklearn 라이브러리의 스케일러(MinMaxScaler)를 사용한 스케일링\n",
    "# 스케일러의 fit_transform() 사용시 2차원의 데이터를 전달해야 함\n",
    "# (DataFrame도 2차원), 결과는 ndarray로 반환 됨\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "x = MinMaxScaler().fit_transform(tips[['tip']])     # 이차원으로된 데이터를 넣어줘야하고, fit_transform하는 것을 까먹지 않으면 제일 편한 방법\n",
    "print(x.min(), x.max())\n",
    "\n",
    "# [5] sklearn 라이브러리의 스케일러(StandardScaler)를 사용한 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = StandardScaler().fit_transform(tips[['tip']])\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6] scipy.stats의 zscore 함수를 사용한 스케일링\n",
    "from scipy.stats import zscore\n",
    "x = zscore(tips['tip'])\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 인코딩(범주형 데이터 라벨링) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 위해 범주형 데이터를 수치형으로 바꿔준다.  \n",
    "\n",
    "  \n",
    "label Encoding : 값의 일련번호로 표기함 : 순서가 있는 경우 유리, 값의 개수가 적은 경우 유리\n",
    "- sri = sri.astype('category').cat.codes\n",
    "- sri = sri.replace([변경 전 목록], [변경 후 목록])사용  \n",
    "- from sklearn.preprocessing import LabelEncoder  \n",
    "sri = LabelEncoder().fit_transform(sri)\n",
    "\n",
    "one hot encoding : 범주의 개수 만큼 feature를 만들어냄 : 순서가 없는 경우 유리, 값의 개수가 많은 경우 유리\n",
    "- pd.get_dummies(sri)\n",
    "- pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'A':['월', '화', '수', '화', '수', '금', '월'],\n",
    "                   'B':['여자', '남자', '여자', '남자', '아이', '남자', '아이']})\n",
    "\n",
    "# [1] 요일에 대해서 Label Encoding 합니다.  (cat.code 사용)\n",
    "weekdays = '월 화 수 목 금 토 일'.split()\n",
    "print(weekdays)\n",
    "df['A_LE'] = pd.Categorical(df['A'], weekdays, ordered=True)\n",
    "df['A_LE'] = df['A_LE'].cat.codes\n",
    "\n",
    "# [2] '남자', '여자', '아이'에 대해서 Label Encoding 합니다 (replace 사용)\n",
    "v = df['B'].unique()\n",
    "df['B'].replace(v, range(len(v)))\n",
    "\n",
    "# [3] df의 'A', 'B' 컬럼을 One Hot Encoding 합니다.\n",
    "a = pd.get_dummies(df['A'])\n",
    "b = pd.get_dummies(df['B'])\n",
    "df2 = pd.concat([a,b], axis=1)\n",
    "\n",
    "# [6] df의 모든 범주형 변수를 OneHotEncoding 합니다\n",
    "df3 = pd.get_dummies(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binning (연속형 변수를 구간을 이용해 범주화)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연속형 변수를 구간을 이용해 범주화하여 정보를 압축시키고 단순하게 만듬 (정확도는 떨어짐)  \n",
    "이상치 해결방법 중 한가지로 사용하거나, 오버피팅 방지 기법으로 사용되기도 함\n",
    "- (3,6] : 3초과 6이하, right=True\n",
    "- [3, 6) : 3이상, 6미만, right=False\n",
    "- pd.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] 데이터 준비\n",
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "df = pd.DataFrame(data, columns=['data'])\n",
    "df['data'].to_list()\n",
    "\n",
    "# [1] data를 사용하여 0~3, 4~6, 7~10 에 대한 binning을 하여 보도록 한다\n",
    "# 범주의 레이블은 ['A', 'B', 'C']를 사용한다\n",
    "\n",
    "# (Min-1, 3], (3, 6], (6, Max] 로 범주를 만들어 result_A 컬럼으로 추가합니다.\n",
    "df['result_A'] = pd.cut(df['data'], [df['data'].min()-1,3,6,10], labels=['A', 'B', 'C'], right=True)\n",
    "# [Min, 3), [3, 6), [6, Max+1) 로 범주를 만들어 result_B 컬럼으로 추가합니다.\n",
    "df['result_B'] = pd.cut(df['data'], [df['data'].min(),3,6,df['data'].max()+1], labels=[0, 1, 2], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] result_A에 대해 LabelEncoding을 하여 result_C를 컬럼으로 추가합니다.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['result_C'] = LabelEncoder().fit_transform(df['result_A'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비율을 사용하여 구간 나누기\n",
    "- pd.qcut(데이터, 구간)\n",
    "- 구간은 0~1 사이의 숫자 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 비율을 사용하여 구간나누기\n",
    "df = pd.DataFrame([0, 1, 2, 3, 10, 11, 12, 13, 20, 30, 40, 50], columns=['data'])\n",
    "pd.qcut(df['data'], [0, 0.25, 0.5, 0.75 ,1])  # 네 구간으로 나눈 것\n",
    "\n",
    "# [4] 다음 데이터를 binning 하여보세요 (bins 컬럼 추가)\n",
    "# 사용구간 : [10, 40), [40, 50), [50, 60), [60, 70), [70, 80), [80, 101)\n",
    "# 사용레이블 : '10-40미만', '40-50미만', '50-60미만', '60-70미만', '70-80미만', '80-100이하'\n",
    "data = [55.6, 83.3, 43.4, 58.1, 31.6, 55.6, 60.7, 64.6,\n",
    "        73.3, 55.6, 64.3, 52.8, 22.7, 46.3, 71.4, 53.8,\n",
    "        64.5, 67.9, 71.4, 80.0, 59.5, 40.5, 77.1, 58.6,\n",
    "        65.4, 52.4, 66.7, 91.3, 41.3, 72.1, 61.9, 78.4,\n",
    "        63.6, 41.0, 65.2, 81.3, 54.8, 19.6, 50.0, 53.1,\n",
    "        41.2, 56.5]\n",
    "df = pd.DataFrame(data, columns=['data'])\n",
    "df['bins'] = pd.cut(df['data'], [10, 40, 50, 60, 70, 80, 101],\n",
    "       labels = ['10-40미만', '40-50미만', '50-60미만', '60-70미만', '70-80미만', '80-100이하'], \n",
    "       right=False)\n",
    "# [5] bins 컬럼에 대해 LabelEncoding을 하여 bin_n 컬럼을 추가합니다.\n",
    "df['bins'] = df['bins'].cat.codes\n",
    "\n",
    "# [6] 다음 나이 데이터를 binning 하여보세요\n",
    "bin_labels = 'Baby Child Teenager Student Young_Adult Adult Elderly'.split()   # 7개 label\n",
    "data = [42, 11, 40, 16, 35, 58, 1, 13, 22, 7, 62, 11, 52, 67, 42, 33, 15, 60, 36, 36]\n",
    "df = pd.DataFrame(data, columns=['age'])\n",
    "\n",
    "# (-1, 5], (5, 12], (12, 18], (18, 25], (25, 35], (35, 60], (60, df['age'].max()] 로\n",
    "# 범주를 만들어 age_cat 컬럼으로 추가합니다.\n",
    "df['age_cat'] = pd.cut(df['age'], [-1, 5, 12, 18, 25, 35, 60, df['age'].max()], \n",
    "                       labels = bin_labels,\n",
    "                       right = True)\n",
    "df['age_LE'] = df['age_cat'].cat.codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델링\n",
    "- train_test_split() : 홀드 아웃\n",
    "- fit() : 학습\n",
    "- score() : 성능 측정 (오버피팅, 언더피팅 확인)\n",
    "    - 성능이 낮으면 : 하이퍼파라미터 조정, 모델 변경, 데이터 전처리 변경 시도\n",
    "- predict() : 예측  /  predict_proba() : 예측 확률 값\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 분류\n",
    "라벨 데이터가 범주형이고, 모든 범주가 1개 이상의 데이터를 갖고 있을 때 층화추출(stratify=y) 해야한다.   \n",
    "y가 가지고 있는 데이터 비율에 맞춰서 샘플 나눈다. (회귀에서는 층화 추출 하지 않는다.)  \n",
    "균형데이터로 만들기 위해 층화추출한다.  \n",
    "- train_test_split(X, y, stratify=y)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 이항 분류 : 라벨 데이터의 범주가 2개\n",
    "- RandomForest  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "rf = RandomForestClassifier(n_estimators=n, max_depth=m)\n",
    "    - n_estimators : 기본 100 (500으로 늘리면 좋아지기도 함)\n",
    "    - max_depth : \n",
    "\n",
    "* Xgboost  \n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "xg = XGBClassifier(eval_metric='logloss', use_label_encoder=False, n_estimators=n, max_depth=m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)다항 분류 : 라벨 데이터의 범주가 3개 이상"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 분류 평가방법\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정확도(accuracy)  \n",
    "from sklearn.metrics import confusion_matrix  \n",
    "label = ['라벨1','라벨2']  \n",
    "예측Y = model.predict(X)  \n",
    "cm = confusion_matrix(실제Y, 예측Y)  \n",
    "cm = pd.DataFrame(cm, columns=label, index=label)  \n",
    "print(cm) -> 컬럼이 예측값 / 행이 실제값\n",
    "\n",
    "- 확률값(proba)  \n",
    "proba = model.predict_proba(X_test)  \n",
    "print(proba)\n",
    "\n",
    "- ROC   \n",
    "from sklearn.metrics import roc_auc_score   \n",
    "y_pred = model.predict_proba(X_test)[:,확률확인할컬럼인덱스번호]  \n",
    "roc_auc_score = roc_auc_score(y_test, y_pred)   \n",
    "- AUC  \n",
    "from sklearn.metrics import roc_auc_score   \n",
    "y_pred = model.predict_proba(X_test)[:,확률확인할컬럼인덱스번호]  \n",
    "roc_auc_score = roc_auc_score(y_test, y_pred)  \n",
    "\n",
    "- F1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 회귀\n",
    "- RandomForest  \n",
    "from sklearn.ensemble import RandomForestRegressor  \n",
    "rf = RandomForestRegressor(n_estimators=n, max_depth=m)\n",
    "    - n_estimators :\n",
    "    - max_depth : \n",
    "\n",
    "* Xgboost  \n",
    "from xgboost.sklearn import XGBReggresor  \n",
    "xg = XGBRegressor(eval_metric='logloss', use_label_encoder=False, n_estimators=n, max_depth=m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 회귀 평가방법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SSE(Sum Squared Error) : \n",
    "- MSE(Mean Squared Error) : \n",
    "- RMSE(Root Mean Squared Error) :\n",
    "- MAE(Mean Absolute Error) : \n",
    "- 결정계수(R2) :\n",
    "- Adjusted R2 : \n",
    "- MSPE(Mean Squared Percentage Error) : \n",
    "- MAPE(Mean Absolute Percentage Error) :\n",
    "- RMSLE(Root Mean Squared Logarithmic Error) : \n",
    "- AIC(Akaike Information Criterion) : \n",
    "- BIC(Bayes Information Criteria) : \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 예측값 저장 (예측값 형태나 프레임은 문제에서 정해준다.)\n",
    "sub = pd.DataFrame()  \n",
    "sub['id'] = pd.RangeIndex(1, len(X)+1)  \n",
    "sub['proba'] = model.predict_proba(X_test)[:,1]  \n",
    "submission = pd.DataFrame({'id':sub['id], 'proba':sub['proba']})\n",
    "submission.to_csv('submission.csv', index=False)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형3 : 통계 검증 2문제 * 15 [30점]\n",
    "- scipy 모듈을 활용한 통계적 유의성 검정에 대한 능력 확인\n",
    "- 주어진 데이터를 활용해 가설검정 진행 후, 통계량, p-value, 채택/기각 등의 결과 출력\n",
    "- 소수점 아래 자릿수가 정해진 실수로 답을 출력"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 확률 분포"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률 현상 : 나오는 결과의 범위는 알지만, 가능한 결과들 중 정확히 어떤 결과가 나올지 모르는 현상\n",
    "확률 변수 : 확률 현상에 기인해 결과 값이 확률적으로 정해지는 변수    \n",
    "확률 분포 : 확률 변수가 특정한 값을 가질 확률을 나타내는 분포\n",
    "\n",
    "확률 질량 함수 (PMF) : 이산 확률 변수에서 특정 값에 대한 확를을 계산하기 위한 함수 (이산형에서 사용)  \n",
    "확률 밀도 함수 (PDF) : 연속 확률 변수에서 특정 구간에 속할 확률을 계산하기 위한 함수 (연속형에서 사용)   \n",
    "누적 분포 함수 (CDF) : 어떤 확률 분포에 대해 확률 변수가 특정 값보다 작거나 같을 확률을 계산하기 위한 함수 (이산형과 연속형에서 사용)  \n",
    "(PPF) : CDF 반대, 확률 값을 주면 그 때의 x값을 줌\n",
    "\n",
    "### 1) 이산형 확률분포 : 확률변수가 몇 개의 한정된 가능한 값을 가지는 분포 (각 사건은 서로 독립이어야 함)  \n",
    "#### (1)베르누이 분포 : 확률 변수의 가능한 값이 두 개뿐인 이산 확률 분포, 이러한 실험 1회 시행이 베르누이 시행이라 함\n",
    "#### (2) 이항 분포 : 연속된 n번의 독립적 시행에서 각 시행이 확률 p를 가질 때의 이산 확률 분포 (n이 1일 때 이항 분포는 베르누이 분포와 같다)\n",
    "#### (3) 기하 분포 : 성공/실패 로 구성된 시행을 연달아 수행시 처음 성공할 때 까지 시도한 회수 k에 대한 \n",
    "#### (4) 초기하 분포 : \n",
    "#### (5) 포아송 분포 : \n",
    "\n",
    "### 2) 연속형 확률분포 : 확률변수의 가능한 값이 무한 개 이며 사실상 셀 수 없는 경우의 분포\n",
    "#### (1) 정규 분포(가우스 분포) : \n",
    "#### (2) 표준 정규 분포 (Z-분포)\n",
    "#### (3) T-분포 : \n",
    "#### (4) 카이제곱 분포(x제곱-분포) :\n",
    "#### (5) F-분포 \n",
    "#### (6) 지수 분포 : \n",
    "#### (7) 감마 분포 : \n",
    "\n",
    "\n",
    "### 3) 신뢰구간\n",
    "#### (1) 신뢰수준 : 추정값이 존재하는 구간에 모수가 포함되어 있을 가능성의 크기 또는 정확도\n",
    "#### (2) 신뢰구간 : 신뢰수준을 기준으로 추정된 통계적으로 유의미한 모수의 범위  \n",
    "- 표본평균 - (z * SE) ~ 표본평균 + (z * SE)  \n",
    "    - SE ; 표준오차"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (중요!) 2. 가설 검정\n",
    "귀무 가설(H0) : 현재까지 주장되어 온 것이나 기존과 비교하여 차이가 없음을 나타내는 가설  \n",
    "대립 가설(H1) : 표본을 통해 확실한 근거를 가지고 입증하고자 하는 가설(연구 가설)  \n",
    "  \n",
    "p-value : 귀무 가설이 참이라는 가정하에 관찰된 결과가 일어날 확률 값   \n",
    "- p-값 0.05 이상 : 귀무가설 채택, 대립가설 기각\n",
    "- p-값 0.05 미만 : 귀무가설 기각, 대립가설 채택\n",
    "  \n",
    "가설 검정 오류  \n",
    "- 제 1종 오류 : 실제(귀무가설 사실) , 판단(귀무가설 기각)\n",
    "- 제 2종 오류 : 실제(귀무가설 거짓) , 판단(귀무가설 채택)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (중요!) 3. 모수 검정 -> 그룹 나누는 법을 알아야 한다\n",
    "표집분포의 모수를 알고 있다고 가정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 정규성 검정 : 정규분포인지 검증\n",
    "- 귀무가설 : 데이터셋이 정규분포를 따른다\n",
    "- 대립가설 : 데이터셋이 정규분포르 따르지 않는다.\n",
    "- 유의 수준 0.05인 경우, p-value >= 0.05 정규성이 보장된다고 할 수 있다.\n",
    "- 검정통계량 < 임계값 인 경우, 정규성이 보장된다고 할 수 있다. (검정 통계량이 임계값 이하? 미만? 인 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   stime20s  20 non-null     int64\n",
      " 1   stime40s  20 non-null     int64\n",
      " 2   ID        20 non-null     int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 608.0 bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/sleepage.csv')\n",
    "df.info()\n",
    "\n",
    "# 그룹을 나누어 저장함\n",
    "gA = df['stime20s']\n",
    "gB = df['stime40s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statisticA:0.9777, pvalueA0.4595\n",
      "statisticB:0.9778, pvalueB0.4647\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 1 shapiro wilks test # 표본 5000개 미만 \n",
    "from scipy.stats import shapiro  \n",
    "statisticA, pvalueA = shapiro(gA)\n",
    "statisticB, pvalueB = shapiro(gB)\n",
    "\n",
    "# f-string으로 자리수 맞춰서 출력하기\n",
    "print(f'statisticA:{statisticA:.4f}, pvalueA{pvalueA:.4f}') # 정규성 만족\n",
    "print(f'statisticB:{statisticB:.4f}, pvalueB{pvalueB:.4f}') # 정규성 만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.9986501019683699, pvalue=8.073266280952718e-58)\n",
      "KstestResult(statistic=0.9999997133484281, pvalue=2.8061866176047734e-131)\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 2 kstest(kolmogorov-smirnov test) # 표본수 2000 초과 // 정규성 검정에만 쓰는 건 아님\n",
    "from scipy.stats import kstest # 두 개의 데이터가 같은 분포인지 아닌지 검정 \n",
    "# 귀무가설 : 두집단의 분포가 같다\n",
    "# 대립가설 : 두집단의 분포가 다르다\n",
    "\n",
    "print(kstest(gA, \"norm\")) # 비교대상으로 \"norm\"으로 정규분포 데이터를 준다 / 정규성 불만족\n",
    "print(kstest(gB, \"norm\")) # 정규성 불만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormaltestResult(statistic=1.173158148726697, pvalue=0.5562268444276741)\n",
      "NormaltestResult(statistic=2.045029526680344, pvalue=0.3596892684891455)\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 3 normaltest  # 표본 20개 이상\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "print(normaltest(gA))  # 정규성 만족\n",
    "print(normaltest(gB))  # 정규성 만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그룹A: 0.4079859754958619 [0.538 0.613 0.736 0.858 1.021] [15.  10.   5.   2.5  1. ]\n",
      "그룹B: 0.360841172628966 [0.538 0.613 0.736 0.858 1.021] [15.  10.   5.   2.5  1. ]\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 4 anderson darling test # 검정통계량, 임계값 출력 (검정통계량 < 임계값이면 귀무가설 채택) // 정규성 검정에만 쓰는 건 아님\n",
    "# 검정통계량 < 임계값, 정규성이 보장된다고 할 수 있다\n",
    "from scipy.stats import anderson\n",
    "rA = anderson(gA, dist=\"norm\") # static(검정통계량), critical_values(임계값), significance_level(유의수준)\n",
    "rB = anderson(gB, dist=\"norm\")\n",
    "\n",
    "print('그룹A:', *rA)\n",
    "print('그룹B:', *rB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 등분산성 검정 : 분산이 동일한지 검증\n",
    "* 귀무가설 : 데이터셋이 등분산성을 충족한다.\n",
    "* 대립가설 : 데이터셋이 등분산성을 충족하지 않는다.\n",
    "\n",
    "- burtlett : 정규성 충족하는, 데이터셋의 크기가 서로 다른 2개 이상의 집단 사용 가능\n",
    "- levene, fligner : 정규성을 충족하지 않는, 비모수 데이터에 대해서도 사용가능 (중앙을 median으로 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bartlett, levene, fligner\n",
    "bartlett()\n",
    "levene() # center={'median'(정규분포가아닌경우사용), 'mean'(좌우대칭일때사용), 'trimmed'(양 끝단에 데이터가 헤비하게 있을 때 사용)}, proportiontocut=0.05 center='trimmed'일 경우 사용\n",
    "fligner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "# 파일 읽어와 내용 확인\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "print(iris.head(2))\n",
    "\n",
    "# [1] target='sepal_length', 품종별 그룹을 나누어 저장함\n",
    "import pandas as pd\n",
    "target = 'sepal_length'\n",
    "iris['species'].unique()\n",
    "gA = iris.loc[iris['species']=='setosa', target]\n",
    "gB = iris.loc[iris['species']=='versicolor', target]\n",
    "gC = iris.loc[iris['species']=='virginica', target]\n",
    "\n",
    "# [2] burtlett 검정\n",
    "from scipy.stats import bartlett, shapiro\n",
    "# 정규성검정\n",
    "_, pvalueA = shapiro(gA)\n",
    "_, pvalueB = shapiro(gB)\n",
    "_, pvalueC = shapiro(gC)\n",
    "print(pvalueA, pvalueB, pvalueC)    # p-value가 0.05 이상 귀무가설 채택 - 정규성 만족\n",
    "# 등분산성 검정\n",
    "static, pvalue = bartlett(gA, gB, gC)\n",
    "print(static, pvalue)   # p-value가 0.05 미만 - 귀무가설 기각 - 등분산성 불만족\n",
    "\n",
    "# [3] levene 의 center는 'mean'으로 지정\n",
    "from scipy.stats import levene\n",
    "print(levene(gA, gB, gC, center='median')) # p-value가 0.05 미만 - 귀무가설 기각 - 등분산성 불만족\n",
    "\n",
    "# [4] fligner의 center는 'trimmed', proportiontocut=5% 지정\n",
    "from scipy.stats import fligner\n",
    "print(fligner(gA, gB, gC, center='trimmed'))    # p-value가 0.05 미만 - 귀무가설 기각 - 등분산성 불만족"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) T-검정 (정규성 및 등분산성이 만족 되어야 함)\n",
    "표본 1개 : 검정통계량이 귀무가설 하에서 t-분포를 따르는 통계적 가설 검정  \n",
    "표본 2개 : 표본을 사용한 모평균 검정 및 두 데이터 세트(집단)의 모평균이 서로 유의하게 다른지 여부 판별  \n",
    "\n",
    "- (1) One sample T test : 표본을 사용한 모평균 검정 방법\n",
    "    - 1표본 평균 검정 : ttest_1samp(a, popmean, alternative='') \n",
    "    - alternative는 대립가설 쪽 이다아니다 = 'two-sided', 작다='less', 크다='greater'\n",
    "    - one sample은 등분산 검정 필요 없음\n",
    "- (2) 신뢰구간 구하기\n",
    "    target = siri\n",
    "    - lower, upper = df(df=len(target)-1).interval(0.95, loc=target.mean(), scale=sem(target))\n",
    "\n",
    "* 검정 통계량이 양수인 경우 > 앞에 있는 gA가 popmean보다 더 크다는 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3)\n",
      "0.9239 0.118\n",
      "채택\n",
      "6.15 6.1500\n",
      "0.4592 0.6513\n",
      "채택\n",
      "0.4592 0.6743\n",
      "채택\n",
      "0.4592 0.3257\n",
      "채택\n",
      "5.4663 ~ 6.8337\n"
     ]
    }
   ],
   "source": [
    "# [0] 수면 시간 정보가 포함된 파일 불러오기\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/sleepage.csv')\n",
    "print(df2.shape)\n",
    "df.T\n",
    "\n",
    "# [1] 20대의 수면시간에 대해 정규성 검정\n",
    "# one sample이므로 정규성 검정만필요, 등분산 검정 필요 없음\n",
    "from scipy.stats import shapiro\n",
    "statistic, pvalue = shapiro(df['stime20s'])\n",
    "print(round(statistic,4), round(pvalue,4))\n",
    "print('기각' if pvalue < 0.05 else '채택')  # 정규성 만족\n",
    "\n",
    "# [2] 20대의 수면시간에 대해 평균 구하기\n",
    "sleep = df['stime20s'].mean()\n",
    "print(round(sleep,4), f'{sleep:.04f}')\n",
    "\n",
    "# [3] One Sample t-test 수행 \n",
    "# 가설 1.alternative='two-sided'\n",
    "# 귀무가설 : 20대 수면시간의 평균은 6시간이다.\n",
    "# 대립가설 : 20대 수면시간의 평균은 6시간이 아니다.\n",
    "from scipy.stats import ttest_1samp\n",
    "statistic, pvalue = ttest_1samp(df['stime20s'], 6, alternative='two-sided')\n",
    "print(round(statistic,4), round(pvalue,4))  # 0.6513\n",
    "\n",
    "# [4] 가설 결과\n",
    "# p-value가 0.05보다 크다. -> 귀무가설을 채택해야한다.\n",
    "# 귀무가설 : 20대 수면시간의 평균은 6시간이다. \n",
    "print('기각' if pvalue < 0.05 else '채택') # 채택\n",
    "\n",
    "# [5] 가설-2. alternative='less'\n",
    "# 귀무가설 : 20대 수면시간의 평균은 6시간 보다 크거나 같다.\n",
    "# 대립가설 : 20대 수면시간의 평균은 6시간 보다 작다\n",
    "statistic, pvalue = ttest_1samp(df['stime20s'], 6, alternative='less')\n",
    "print(round(statistic, 4), round(pvalue, 4))\n",
    "\n",
    "# [6] 가설 결과\n",
    "print('기각' if pvalue < 0.05 else '채택') # 같은게 포함이 되어서 채택됨\n",
    "\n",
    "# [7] 가설-3. alternative='greater'\n",
    "# 귀무가설 : 20대 수면시간의 평균은 6시간 보다 작거나 같다.\n",
    "# 대립가설 : 20대 수면시간의 평균은 6시간 보다 크다\n",
    "from scipy.stats import ttest_1samp\n",
    "statistic, pvalue = ttest_1samp(df['stime20s'], 6, alternative='greater')\n",
    "print(round(statistic,4), round(pvalue,4))\n",
    "\n",
    "# [8] 가설 결과\n",
    "print('기각' if pvalue < 0.05 else '채택')  # 같은게 포함이 되어서 채택됨\n",
    "\n",
    "# [9] 95% 신뢰구간 구하기\n",
    "from scipy.stats import t, sem\n",
    "target = df['stime20s']\n",
    "lower, upper = t(df=len(target)-1).interval(0.95, \n",
    "                                            loc=target.mean(), \n",
    "                                            scale=sem(target))\n",
    "print(f'{lower:.4f} ~ {upper:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3) Two sample T test : 서로 다른 두 그룹의 표본 평균을 비교하여 두 모집단의 평균차이 검정\n",
    "    - 독립 표본 표본 T 검정 : ttest_ind(a, b, alternative='two-sided', equal_var=True/False)     \n",
    "    - 등분산아닌 경우 equal_var=False\n",
    "\n",
    "* 검정 통계량이 양수인 경우 > 앞에 있는 gA가 gB보다 더 크다는 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 그룹 분할하기\n",
    "gA = df['stime20s']\n",
    "gB = df['stime40s']\n",
    "\n",
    "# [2] 그룹별 평균 구하기\n",
    "print(gA.mean(), gB.mean())\n",
    "\n",
    "# [3] 등분산 검정 - 3가지 방법으로 실행 후, pvalue 확인\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import levene, fligner, bartlett\n",
    "# 정규성 검정\n",
    "statistic1, pvalue1 = shapiro(gA) # 귀무가설 채택(정규성 만족)\n",
    "statistic2, pvalue2 = shapiro(gB) # 귀무가설 기각(정규성 불만족)\n",
    "print(pvalue1, pvalue2)\n",
    "# 등분산성 검정\n",
    "# 20은 정규성 만족 했으니까 bartlett\n",
    "statistic3, pvalue3 = bartlett(gA, gB) # 귀무가설 채택 (등분산 만족)\n",
    "# 40은 정규성 불만족 했으니까 levene\n",
    "statistic4, pvalue4 = levene(gA, gB, center='median') # 귀무가설 채택 (등분산 만족)\n",
    "print(pvalue3, pvalue4) # 등분산 만족하니까 일반적인 ttest_ind사용 가능\n",
    "\n",
    "# [4] two-sample t-test 수행\n",
    "# 귀무가설 : groupA, groupB의 평균은 동일하다\n",
    "# 대립가설 : groupA, groupB의 평균은 동일하지 않다\n",
    "from scipy.stats import ttest_ind\n",
    "statistic, pvalue = ttest_ind(gA, gB, alternative='two-sided')\n",
    "print(pvalue)   # 0.62 귀무가설 채택, 평균이 동일하다.\n",
    "# [5] 가설 결과\n",
    "print('기각' if pvalue < 0.05 else '채택')\n",
    "\n",
    "# [6] \n",
    "# 귀무가설 : groupA의 평균이 groupB의 평균보다 크거나 같다\n",
    "# 대립가설 : groupA의 평균이 groupB의 평균보다 작다\n",
    "from scipy.stats import ttest_ind\n",
    "statistic, pvalue = ttest_ind(gA, gB, alternative='less')\n",
    "print(pvalue)\n",
    "print('기각' if pvalue < 0.05 else '채택') # 채택\n",
    "\n",
    "# [7] \n",
    "# 귀무가설 : groupA의 평균이 groupB의 평균보다 작거나 같다\n",
    "# 대립가설 : groupA의 평균이 groupB의 평균보다 크다\n",
    "from scipy.stats import ttest_ind\n",
    "statistics, pvalue = ttest_ind(gA, gB, alternative='greater', equal_var=True)\n",
    "print(pvalue)\n",
    "print('기각' if pvalue < 0.05 else '채택') # 채택"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (4) Paried T test\n",
    "    - 동일 개체에 어떤 처리를 하기 전, 후의 자료 얻을 때 차이 값에 대한 평균 검정 \n",
    "    - 두 집단 A, B의 평균 차이가 유의미한지 확인하는 용도\n",
    "    - 쌍체(대응) 표본 t-검정 : ttest_rel(a, b, alternative='two-sided)\n",
    "    - alternative=는 첫번째 집단 기준으로 큰지 작은지 나타냄\n",
    "    - 등분산성 검정은 하지 않아도 되는 특징 가지고 있음\n",
    "    \n",
    "* 검정 통계량이 양수인 경우 > 앞에 있는 gA가 gB보다 더 크다는 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 데이터 가져오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df2 = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/sleep.csv')\n",
    "print(df2.shape)\n",
    "\n",
    "# [2] 그룹 나누기\n",
    "gA = df2.loc[df2['group']==1.0, 'extra']\n",
    "gB = df2.loc[df2['group']==2.0, 'extra']\n",
    "gA.shape, gB.shape\n",
    "\n",
    "# [3] 정규성 검정(shapiro)\n",
    "from scipy.stats import shapiro\n",
    "_, pv1 = shapiro(gA)    # 0.40 귀무가설 채택 (정규성 만족)\n",
    "_, pv2 = shapiro(gB)    # 0.35 귀무가설 채택 (정규성 만족)\n",
    "print(pv1, pv2)\n",
    "\n",
    "# [5] gA와 gB의 평균 구하기\n",
    "print(gA.mean(), gB.mean())\n",
    "\n",
    "# [6] Paired t-test\n",
    "# 가설1. alternative='two-sided'\n",
    "# 귀무가설 : gA의 평균 - gB의 평균이 0과 같다\n",
    "# 대립가설 : gA의 평균 - gB의 평균이 0과 같지 않다\n",
    "# 귀무가설 : gA와 gB의 평균은 같다\n",
    "# 대립가설 : gA와 gB의 평균은 같지 않다\n",
    "from scipy.stats import ttest_rel\n",
    "statistics, pvalue = ttest_rel(gA, gB, alternative='two-sided')\n",
    "print(statistic, pvalue)    # p-value가 0.002, 귀무가설 기각, 평균이 다르다.\n",
    "# [7] 결과해석\n",
    "print('기각' if pvalue < 0.05 else '채택')\n",
    "\n",
    "# [8] Paired t-test\n",
    "# 가설2. alternative='less'\n",
    "# 귀무가설 : gA의 평균 - gB의 평균이 0보다 크거나 같다\n",
    "# 대립가설 : gA의 평균 - gB의 평균이 0보다 작다\n",
    "# 귀무가설 : gA의 평균이 gB의 평균보다 크거나 같다\n",
    "# 대립가설 : gA의 평균이 gB의 평균보다 작다\n",
    "from scipy.stats import ttest_rel\n",
    "statistics, pvalue = ttest_rel(gA, gB, alternative='less')\n",
    "print(pvalue)   # p-value : 0.004 귀무가설 기각 \n",
    "# [9] 결과해석\n",
    "print('기각' if pvalue < 0.05 else '채택') \n",
    "\n",
    "# [10] Paired t-test\n",
    "# 가설3. alternative='greater'\n",
    "# - 귀무가설 : gA의 평균 - gB의 평균이 0보다 작거나 같다\n",
    "# - 대립가설 : gA의 평균 - gB의 평균이 0보다 크다\n",
    "# - 귀무가설 : gA의 평균이 gB의 평균보다 작거나 같다\n",
    "# - 대립가설 : gA의 평균이 gB의 평균보다 크다\n",
    "from scipy.stats import ttest_rel\n",
    "statistics, pvalue = ttest_rel(gA, gB, alternative='greater')\n",
    "print(f'{pvalue:.4f}')  # p-value : 0.99, 귀무가설 채택\n",
    "# [11] 결과해석\n",
    "print('기각' if pvalue < 0.05 else '채택')  # 채택"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (5) 분류 모델에서 활용\n",
    "    - label data 에 따라서 유의미한 차이가 있는지 비교해 보는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Warehouse_block</th>\n",
       "      <th>Mode_of_Shipment</th>\n",
       "      <th>Customer_care_calls</th>\n",
       "      <th>Customer_rating</th>\n",
       "      <th>Cost_of_the_Product</th>\n",
       "      <th>Prior_purchases</th>\n",
       "      <th>Product_importance</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Discount_offered</th>\n",
       "      <th>Weight_in_gms</th>\n",
       "      <th>Reached.on.Time_Y.N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Flight</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>low</td>\n",
       "      <td>F</td>\n",
       "      <td>44</td>\n",
       "      <td>1233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>Flight</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "      <td>low</td>\n",
       "      <td>M</td>\n",
       "      <td>59</td>\n",
       "      <td>3088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Warehouse_block Mode_of_Shipment  Customer_care_calls  Customer_rating  \\\n",
       "0   1               D           Flight                    4                2   \n",
       "1   2               F           Flight                    4                5   \n",
       "\n",
       "   Cost_of_the_Product  Prior_purchases Product_importance Gender  \\\n",
       "0                  177                3                low      F   \n",
       "1                  216                2                low      M   \n",
       "\n",
       "   Discount_offered  Weight_in_gms  Reached.on.Time_Y.N  \n",
       "0                44           1233                    1  \n",
       "1                59           3088                    1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Soyoung-Yoon/bigdata/main/1st_Train.csv')\n",
    "data.head(2)\n",
    "\n",
    "# t-test\n",
    "# 연속형 변수 : 'Cost_of_the_Product', 'Weight_in_gms', 'Discount_offered'\n",
    "# 범주형 변수 : 'Reached.on.Time_Y.N'\n",
    "\n",
    "# 데이터 5000개 미만이면 정규성 테스트 shapiro,  kstest\n",
    "from scipy.stats import shapiro, kstest, bartlett, levene\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# [1] 그룹 나누기\n",
    "feature = 'Cost_of_the_Product'\n",
    "gA = data.loc[data['Reached.on.Time_Y.N']==1, feature]\n",
    "gB = data.loc[data['Reached.on.Time_Y.N']==0, feature]\n",
    "print(feature, gA.shape, gB.shape)\n",
    "\n",
    "# [2] 정규성, 등분산성 검정\n",
    "# sta, pva = shapiro(gA)    # 데이터가 5000개 이상이면 사피로 비추\n",
    "# stb, pvb = shapiro(gB)    # 데이터 많으면 kstest 추천\n",
    "_, pva = kstest(gA, \"norm\")\n",
    "_, pvb = kstest(gA, \"norm\")\n",
    "print(pva, pvb) # 0.0, 0.0 -> 둘 다 귀무가설 기각 -> 둘 다 정규성 불만족\n",
    "\n",
    "# 정규성 갖지 못하니까 levene사용\n",
    "_, pv = levene(gA, gB, center='median')\n",
    "print(pv)   # 0.06 -> 귀무가설 채택 -> 등분산성 만족\n",
    "\n",
    "# [3] ttest_ind\n",
    "from scipy.stats import ttest_ind\n",
    "statistics, pvalue = ttest_ind(gA, gB, alternative='two-sided', equal_var=True)\n",
    "print(f'{pvalue:.4f}')  # 0.000 -> 귀무가설 기각 -> 라벨 데이터 별 유의미한 차이 있다.\n",
    "\n",
    "# [4] 결론\n",
    "print('기각' if pvalue < 0.05 else '채택')  # 라벨 데이터 별 유의미한 차이 있다.\n",
    "\n",
    "# 위 코드는 피쳐 하나만 했는데 제시된 피쳐 하나씩 다해보면 피쳐마다 유의미한지 아닌지 알 수 있음\n",
    "# 함수나 반복문을 사용해서 돌려도 됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) ANOVA\n",
    "t-test는 1개 또는 2개 집단에 대한 평균 검정  ,\n",
    "ANOVA는 3개 이상의 집단 평균 검정 : 집단간분산/집단내분산 기반의 F분포 사용해  \n",
    "정규성, 등분산성, 독립성 가정하는 분산 분석\n",
    "- (1) 일원분산분석(One-Way ANOVA)범주형 독립변수가 한 개인 경우 사용 : f_oneway(*data)\n",
    "    - 귀무가설 : 모든 집단의 평균이 같다\n",
    "    - 대립가설 : 하나 이상의 집단 평균이 다르다\n",
    "    - from scipy.stats import f_oneway  \n",
    "    fstatistic, pavlue = f_oneway(*sample) -> F-statistic과 p-value 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.160040089612075\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 예시 데이터 설명\n",
    "# iris의 target이 0 : 'setosa', 1 : 'versicolor', 2: 'virginica' 품종\n",
    "# 독립변수 :'sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'\n",
    "# 품종별 sepal, petal의 length, width가 차이를 보일까?에 anova 사용\n",
    "\n",
    "# [1] 데이터 가져오기\n",
    "import pandas as pd\n",
    "\n",
    "iris = pd.read_csv('./eduatoz/04. 통계적 검정/bigdata/iris_data.csv')\n",
    "iris.columns = ['sepal_length', 'sepal_width',\n",
    "                'petal_length', 'petal_width', 'target']\n",
    "\n",
    "# [2-1] 품종별 각 변수의 평균 확인\n",
    "# print(iris.groupby('target').mean())\n",
    "\n",
    "# [2-2] 특정 변수에 대한 품종별 평균 확인\n",
    "featrue = 'sepal_width'\n",
    "# print(iris.groupby('target')[featrue].mean())\n",
    "\n",
    "# group 0, 1, 2의 평균의 차이가 있습니다 -> 그룹이 0,1,2 세개이기 때문에 ANOVA사용\n",
    "# 평균값의 차이가 실제로 의미가 있는 차이인지 알고 싶다면,\n",
    "# 분산 분석을 통해 통계적 유의성을 알아 볼 수 있습니다.\n",
    "\n",
    "# 그룹 나누기\n",
    "gA = iris.loc[iris['target']==0, featrue]\n",
    "gB = iris.loc[iris['target']==1, featrue]\n",
    "gC = iris.loc[iris['target']==2, featrue]\n",
    "\n",
    "\n",
    "# [4] 정규성 확인\n",
    "# p-value가 0.05 보다 큰 값일 때 정규성을 갖음\n",
    "from scipy.stats import shapiro\n",
    "_, p0 = shapiro(gA)\n",
    "_, p1 = shapiro(gB)\n",
    "_, p2 = shapiro(gC)\n",
    "# print(p0, p1, p2)   # 모두 귀무가설 채택, 정규성 만족\n",
    "\n",
    "# [5] 등분산성 확인\n",
    "# p-value가 0.05 보다 큰 값일 때 등분산성을 갖음\n",
    "from scipy.stats import bartlett    # 정규성 충족하는 경우 등분산성 확인\n",
    "statistics, pvalue = bartlett(gA, gB, gC)\n",
    "# print(pvalue)   # 귀무가설 채택, 등분산성 만족\n",
    "\n",
    "# [6] 일원분산분석 - 1\n",
    "from scipy.stats import f_oneway\n",
    "fstatistics, pvalue = f_oneway(gA, gB, gC)\n",
    "print(fstatistics)\n",
    "print(round(pvalue,4))  # 귀무가설 기각\n",
    "\n",
    "# 귀무가설 : 세 집단간 평균이 같다.   <- 기각\n",
    "# 대립가설 : 세 집단간 평균이 다르다.  <- 채택, 세 집단간 유의미한 차이 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2) 이원분산분석(Two-Way ANOVA)주형 독립변수가 두 개인 경우 사용 : f_twoway(*data)  \n",
    "(K-Way ANOVA; 범주형 변수가 K개인 경우 사용)\n",
    "    - 귀무가설1 : 1변수 그룹들의 평균이 같다.\n",
    "    - 집단가설1 : 1변수 그룹들의 평균이 다르다.\n",
    "    - 귀무가설2 : 2변수 그룹들의 평균이 같다.\n",
    "    - 집단가설2 : 2변수 그룹들의 평균이 다르다.\n",
    "    - 귀무가설3 : 두 변수 사이의 상호작용 효과가 없다.\n",
    "    - 집단가설3 : 두 변수 사이의 상호작용 효과가 있다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5) 사후검정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA 분석을 통해 집단간 차이가 있는 것을 알 순 있지만 어떤 집단 간 차이인지 알 수 없음  \n",
    "이때, 사후 검정을 통해 어떤 것에 차이가 있는지 찾을 수 있음 (post hoc 의미 \"after this\")\n",
    "민감도 : Scheffe(엄격) < Tukey < Duncan/Fisher(널널)\n",
    "\n",
    "#### 1) 정규분포, 등분산, 동일표본O\n",
    "- Tukey : \n",
    "- Duncan : \n",
    "#### 2) 정규분포, 등분산, 동일표본X\n",
    "- Scheff : \n",
    "- Fisher : \n",
    "#### 3) 정규분포, 이분산, 동일표본X\n",
    "- Games Howell : \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (중요!) 4. 비모수 검정 -> 교차표 만드는 법 알아야함\n",
    "표집분포의 모수를 알지 못한다고 가정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Chi-Square Test : 적합성 (모집단 1개일 때)\n",
    "2) Chi-Square Test : 동질성/독립성 (모집단 2개 이상일 때) \n",
    "3) Wilcoxon Signed Rank TEst : 1표본, paired // 모수 검정의 ttest_1samp, ttest_rel\n",
    "4) Wilcoxon Rank Sum Test / Mann-Whitney : 2표본, 독립 // 모수 검정의 ttest_ind 비슷\n",
    "5) Kruskal-Wallis H Test : k표본, 독립 // 모수 검정의 ANOVA 비슷"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 카이검정\n",
    "O: 관찰값, E:기대값\n",
    "- (1) 적합도 검정 : (한 개 범주형 변수, 알려진 사실)\n",
    "    - 귀무가설 : 변수의 분포가 기대 분포와 같다. / 모두 동일한 비율이다.  \n",
    "    (p-vlaue가 유의수준 보다 높은 경우 관찰도수와 기대도수의 차이가 작고 적합도가 높다고 할 수 있음)\n",
    "    - 대립가설 : 변수의 분포가 기대 분포와 다르다. / 적어도 하나는 기대 비율과 다르다.\n",
    "  \n",
    "모집단의 어떤 확률 분포를 가설로 설정한 후, 표본 자료의 분포를 분석해 모집단 가설에 대한 타당성 검증  \n",
    "\n",
    "* from scipy.stats import chisqure  \n",
    "chisquare(관찰값(O), 기대값=None(E), ddof=0, axis=0) -> chisq, p-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blood_type\n",
      "A             16\n",
      "B             16\n",
      "AB             4\n",
      "O              4\n",
      "dtype: int64\n",
      "[ 8.  8. 12. 12.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 1-1. 특정비율\n",
    "# A학급 40명의 혈액형 비율을 A, B, O, AB 각각 20%, 20%, 30%, 30%로 예상하였다. \n",
    "# 실제 측정 결과 16, 16, 4, 4 명인 경우의 적합도 검정을 수행하여 보자\n",
    "# 𝑯_𝟎 : 변수의 분포가 기대 분포와 같다\n",
    "# 𝑯_𝟏 : 변수의 분포가 기대 분포와 같지 않다\n",
    "# [파일로 주어진 경우 직접 개수 계산]\n",
    "import pandas as pd\n",
    "data = {'blood_type': ['A']*16 + ['B']*16 + ['O']*4 + ['AB']*4}\n",
    "data = pd.DataFrame(data)\n",
    "print(data.value_counts()) \n",
    "## 파일로 불러온 경우 .value_counts() 해서 값들의 계수를 세어주면 된다.\n",
    "## 이때 변수의 순서에 주의한다. 정렬되면서 순서가 바뀌니까.\n",
    "\n",
    "# 그룹 나누기\n",
    "observed = [16, 16, 4, 4] # 관찰값 (총 40명)\n",
    "expected = [40*0.2, 40*0.2, 40*0.3, 40*0.3] # 기대값 [8, 8, 12, 12]\n",
    "import numpy as np\n",
    "expected = sum(observed) * np.array([0.2, 0.2, 0.3, 0.3])\n",
    "print(expected)\n",
    "\n",
    "# 카이제곱 검정 적합도 검사\n",
    "from scipy.stats import chisquare\n",
    "chi, pvalue = chisquare(observed, expected)\n",
    "print(round(pvalue,4))  # pvalue : 0 -> 귀무가설 기각 -> 관찰도수의 분포가 기대도수의 분포와 다르다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50. 50. 50. 50.]\n",
      "0.2001374153373317\n"
     ]
    }
   ],
   "source": [
    "# 1-2. 동일 비율\n",
    "# 4개의 범주에 대해 동일한 비율이라고 가정하고, 실제측정 한 표본 분포가 다음과 같을 때,   \n",
    "# 카이제곱 적합도 검정을 수행하여 보자  \n",
    "# 𝑯_𝟎 : 변수의 분포에 비율 차이가 없다.\n",
    "# 𝑯_𝟏 : 변수의 분포에 비율 차이가 있다.\n",
    "\n",
    "# 그룹 나누기\n",
    "import numpy as np\n",
    "observed = [54, 46, 60, 40] # 관찰값\n",
    "expected = sum(observed) * np.array([1/len(observed)]*len(observed)) # 기대값 동일한 비율이라고 했으니까 : (1/len(observed)) 가 len(observed) 개\n",
    "print(expected)\n",
    "\n",
    "# 카이제곱 검정\n",
    "from scipy.stats import chisquare\n",
    "chi, pvalue = chisquare(observed, expected)\n",
    "print(pvalue)   # pvalue : 0.2 -> 귀무가설 채택 -> 변수의 분포 비율 차이가 없다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 범주형 자료 간의 차이 및 연관성을 분석하는 방법  \n",
    "관찰 빈도(실제값), 기대빈도(기대값) 유의미한 차이가 있는지 검증\n",
    "\n",
    "- (2) 동질성 검정 : (부모집단, 범주형 변수)\n",
    "    - 부모집단에 대해서 범주형 변수의 분포가 동일한지 검정,  \n",
    "    예) 성별(부모)에 따라 음료(자식)의 선호가 동일한지 검정\n",
    "    - 귀무가설 : 집단간 변수의 분포가 같다. / 차이가 없다.\n",
    "    - 대립가설 : 집단간 변수의 분포가 다르다. / 차이가 크다.\n",
    "\n",
    "* (3) 독립성 검정 : (두 개 범주형 변수)  \n",
    "    * 성별과 음료의 연관성 검정\n",
    "    * 귀무가설 : 두 변수는 독립적이다. / 연관성 없다.\n",
    "    * 대립가설 : 두 변수는 독립적이지 않다. / 연관성 있다.\n",
    "\n",
    "from scipy.stats import chi2_contingenc    \n",
    "(observed는 교차표 형태로 줘야함 pd.corsstab(index, columns)로 만듬)  \n",
    "chi2_contingenc(observed, correction=True, lambda=None)  \n",
    "-> chisq, p-value, df(자유도), expected(기대값 우리가 입력하는게 아니라 받는 것)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model_A  model_B  model_C\n",
      "M       10       40       50\n",
      "F       30       60       10\n",
      "40.6667 0.0 2 [[20. 50. 30.]\n",
      " [20. 50. 30.]]\n"
     ]
    }
   ],
   "source": [
    "# 2-1.카이제곱 동질성 검정\n",
    "# H0 : 성별별 핸드폰 모델 선호도 분포는 같다\n",
    "# H1 : 성별별 핸드폰 모델 선호도 분포가 같지 않다\n",
    "\n",
    "# 크로스 테이블 생성\n",
    "import pandas as pd\n",
    "crs_table = pd.DataFrame([[10, 40, 50], [30, 60, 10]],\n",
    "                         index=['M', 'F'],\n",
    "                columns=['model_A', 'model_B', 'model_C'])\n",
    "print(crs_table)\n",
    "\n",
    "# 동질성 검정\n",
    "from scipy.stats import chi2_contingency\n",
    "chi, pvalue, df, expected = chi2_contingency(crs_table)\n",
    "print(round(chi,4), round(pvalue,4), df, expected)  # pvalue : 0 -> 귀무가설 기각 -> 성별별 핸드폰 모델 선호도 분포 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function chi2_contingency in module scipy.stats.contingency:\n",
      "\n",
      "chi2_contingency(observed, correction=True, lambda_=None)\n",
      "    Chi-square test of independence of variables in a contingency table.\n",
      "    \n",
      "    This function computes the chi-square statistic and p-value for the\n",
      "    hypothesis test of independence of the observed frequencies in the\n",
      "    contingency table [1]_ `observed`.  The expected frequencies are computed\n",
      "    based on the marginal sums under the assumption of independence; see\n",
      "    `scipy.stats.contingency.expected_freq`.  The number of degrees of\n",
      "    freedom is (expressed using numpy functions and attributes)::\n",
      "    \n",
      "        dof = observed.size - sum(observed.shape) + observed.ndim - 1\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    observed : array_like\n",
      "        The contingency table. The table contains the observed frequencies\n",
      "        (i.e. number of occurrences) in each category.  In the two-dimensional\n",
      "        case, the table is often described as an \"R x C table\".\n",
      "    correction : bool, optional\n",
      "        If True, *and* the degrees of freedom is 1, apply Yates' correction\n",
      "        for continuity.  The effect of the correction is to adjust each\n",
      "        observed value by 0.5 towards the corresponding expected value.\n",
      "    lambda_ : float or str, optional\n",
      "        By default, the statistic computed in this test is Pearson's\n",
      "        chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n",
      "        Cressie-Read power divergence family [3]_ to be used instead.  See\n",
      "        `scipy.stats.power_divergence` for details.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    chi2 : float\n",
      "        The test statistic.\n",
      "    p : float\n",
      "        The p-value of the test\n",
      "    dof : int\n",
      "        Degrees of freedom\n",
      "    expected : ndarray, same shape as `observed`\n",
      "        The expected frequencies, based on the marginal sums of the table.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    scipy.stats.contingency.expected_freq\n",
      "    scipy.stats.fisher_exact\n",
      "    scipy.stats.chisquare\n",
      "    scipy.stats.power_divergence\n",
      "    scipy.stats.barnard_exact\n",
      "    scipy.stats.boschloo_exact\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    An often quoted guideline for the validity of this calculation is that\n",
      "    the test should be used only if the observed and expected frequencies\n",
      "    in each cell are at least 5.\n",
      "    \n",
      "    This is a test for the independence of different categories of a\n",
      "    population. The test is only meaningful when the dimension of\n",
      "    `observed` is two or more.  Applying the test to a one-dimensional\n",
      "    table will always result in `expected` equal to `observed` and a\n",
      "    chi-square statistic equal to 0.\n",
      "    \n",
      "    This function does not handle masked arrays, because the calculation\n",
      "    does not make sense with missing values.\n",
      "    \n",
      "    Like stats.chisquare, this function computes a chi-square statistic;\n",
      "    the convenience this function provides is to figure out the expected\n",
      "    frequencies and degrees of freedom from the given contingency table.\n",
      "    If these were already known, and if the Yates' correction was not\n",
      "    required, one could use stats.chisquare.  That is, if one calls::\n",
      "    \n",
      "        chi2, p, dof, ex = chi2_contingency(obs, correction=False)\n",
      "    \n",
      "    then the following is true::\n",
      "    \n",
      "        (chi2, p) == stats.chisquare(obs.ravel(), f_exp=ex.ravel(),\n",
      "                                     ddof=obs.size - 1 - dof)\n",
      "    \n",
      "    The `lambda_` argument was added in version 0.13.0 of scipy.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] \"Contingency table\",\n",
      "           https://en.wikipedia.org/wiki/Contingency_table\n",
      "    .. [2] \"Pearson's chi-squared test\",\n",
      "           https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n",
      "    .. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
      "           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
      "           pp. 440-464.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    A two-way example (2 x 3):\n",
      "    \n",
      "    >>> from scipy.stats import chi2_contingency\n",
      "    >>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n",
      "    >>> chi2_contingency(obs)\n",
      "    (2.7777777777777777,\n",
      "     0.24935220877729619,\n",
      "     2,\n",
      "     array([[ 12.,  12.,  16.],\n",
      "            [ 18.,  18.,  24.]]))\n",
      "    \n",
      "    Perform the test using the log-likelihood ratio (i.e. the \"G-test\")\n",
      "    instead of Pearson's chi-squared statistic.\n",
      "    \n",
      "    >>> g, p, dof, expctd = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
      "    >>> g, p\n",
      "    (2.7688587616781319, 0.25046668010954165)\n",
      "    \n",
      "    A four-way example (2 x 2 x 2 x 2):\n",
      "    \n",
      "    >>> obs = np.array(\n",
      "    ...     [[[[12, 17],\n",
      "    ...        [11, 16]],\n",
      "    ...       [[11, 12],\n",
      "    ...        [15, 16]]],\n",
      "    ...      [[[23, 15],\n",
      "    ...        [30, 22]],\n",
      "    ...       [[14, 17],\n",
      "    ...        [15, 16]]]])\n",
      "    >>> chi2_contingency(obs)\n",
      "    (8.7584514426741897,\n",
      "     0.64417725029295503,\n",
      "     11,\n",
      "     array([[[[ 14.15462386,  14.15462386],\n",
      "              [ 16.49423111,  16.49423111]],\n",
      "             [[ 11.2461395 ,  11.2461395 ],\n",
      "              [ 13.10500554,  13.10500554]]],\n",
      "            [[[ 19.5591166 ,  19.5591166 ],\n",
      "              [ 22.79202844,  22.79202844]],\n",
      "             [[ 15.54012004,  15.54012004],\n",
      "              [ 18.10873492,  18.10873492]]]]))\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# help 기능\n",
    "from scipy.stats import chi2_contingency\n",
    "print(help(chi2_contingency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비만 여부   N   Y\n",
      "당뇨 여부        \n",
      "N      62  22\n",
      "Y      12   4\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 2-2. 카이제곱 독립성 검정\n",
    "# 귀무가설 : 당뇨와 비만 사이에 관계는 독립이다.  \n",
    "# 대립가설 : 당뇨와 비만 사이에 관계는 독립이 아니다.\n",
    "\n",
    "# [1] 데이터 가져오기\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"./eduatoz/04. 통계적 검정/data_02/data_chi.csv\")\n",
    "# print(data)\n",
    "\n",
    "# 크로스 테이블 생성\n",
    "crs_tab = pd.crosstab(data['당뇨 여부'], data['비만 여부'])\n",
    "print(crs_tab)\n",
    "\n",
    "# 동질성 검정\n",
    "from scipy.stats import chi2_contingency\n",
    "chi, pvalue, df, expected = chi2_contingency(crs_tab)\n",
    "print(pvalue)   # pvalue : 0.1 -> 귀무가설 채택 -> 당뇨와 비만 사이 관계는 독립이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (4) 피셔 정확 검정 : 2x2 매트릭스 분석에 적합\n",
    "    - from scipy import fisher_exact  \n",
    "    odd, pvalue = fisher_exact(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# [1] 분할표 생성\n",
    "data = pd.DataFrame([[1, 6], [5, 2]])\n",
    "data.columns = ['가짜 약','진짜 약']\n",
    "data.index = ['효과있음', '효과없음']\n",
    "print(data)\n",
    "\n",
    "# [2] 피셔 검정\n",
    "from scipy.stats import fisher_exact\n",
    "odd, pvalue = fisher_exact(data)\n",
    "print(pvalue)   # pvalue : 0.1 -> 귀무가설 채택 -> 약과 효과의 관계는 독립니다. (연관 없다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wife</th>\n",
       "      <th>Alternating</th>\n",
       "      <th>Husband</th>\n",
       "      <th>Jointly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Laundry</th>\n",
       "      <td>156</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Main_meal</th>\n",
       "      <td>124</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinner</th>\n",
       "      <td>77</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breakfeast</th>\n",
       "      <td>82</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tidying</th>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dishes</th>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shopping</th>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Official</th>\n",
       "      <td>12</td>\n",
       "      <td>46</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Driving</th>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finances</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insurance</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repairs</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holidays</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Wife  Alternating  Husband  Jointly\n",
       "Laundry      156           14        2        4\n",
       "Main_meal    124           20        5        4\n",
       "Dinner        77           11        7       13\n",
       "Breakfeast    82           36       15        7\n",
       "Tidying       53           11        1       57\n",
       "Dishes        32           24        4       53\n",
       "Shopping      33           23        9       55\n",
       "Official      12           46       23       15\n",
       "Driving       10           51       75        3\n",
       "Finances      13           13       21       66\n",
       "Insurance      8            1       53       77\n",
       "Repairs        0            3      160        2\n",
       "Holidays       0            1        6      153"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [1] crosstable 형태의 데이터 읽기\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/housetasks.csv') # 0 칼럼이 인덱스로 가면 교차표 형태가 된다\n",
    "data = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/housetasks.csv', index_col=0)\n",
    "\n",
    "# print(data.shape)\n",
    "# print(data.info())\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (5) 분류 모델에서 활용 : 2개 범주형 변수의 관계에 대해 검정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 파일 불러오기\n",
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Soyoung-Yoon/bigdata/main/1st_Train.csv')\n",
    "X = data.drop(columns=['Reached.on.Time_Y.N'])\n",
    "Y = data[['ID', 'Reached.on.Time_Y.N']]\n",
    "#[2] X,Y가 분리되어 있는 경우 병합하기\n",
    "XY = pd.merge(X, Y, on='ID')\n",
    "#[3] 컬럼별 값의 가짓수 확인하기 (종류의 가지 수 확인이니까 당연히 nunique)\n",
    "print(XY.nunique())\n",
    "#[4] 범주형 변수 확인하기\n",
    "# (설명을 보고 확인하는 것이 정확함)\n",
    "XY.info()\n",
    "\n",
    "# (독립)범주형 변수 : 'Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender', 'Customer_rating', 'Customer_care_calls', 'Prior_purchases'\n",
    "# (종속)범주형 변수 : 'Reached.on.Time_Y.N'\n",
    "# 귀무가설 : 'Reached.on.Time_Y.N'과 feature는 독립이다. (연관성이 없다.)\n",
    "# 대립가설 : 'Reached.on.Time_Y.N'과 feature는 독립이 아니다. (연관성이 있다.)\n",
    "\n",
    "# 크로스탭 만들기\n",
    "df = pd.merge(X[['ID', 'Warehouse_block']],Y,on='ID')\n",
    "crs_tab = pd.crosstab(df['Warehouse_block'], df['Reached.on.Time_Y.N'])\n",
    "# print(crs_tab)\n",
    "\n",
    "# [5] 카이제곱 독립성 검정\n",
    "from scipy.stats import chi2_contingency\n",
    "chi, pv, df, exp = chi2_contingency(crs_tab)\n",
    "print(round(pv, 4)) # pvalue 0.6 -> 귀무가설 채택 -> 독립성 있다. (연관성 없다.)\n",
    "\n",
    "# [6] 결론\n",
    "print('기각' if pv < 0.05 else '채택') # 채택\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 순위 검정\n",
    "- (1) 윌콕슨 순위합 검정(Wilcoxon Signed Rank Test) : 1표본/대응표본 T검정에서 정규성 가정이 만족되지 않을 때 사용하는 비모수 검정법  \n",
    "1표본/대응표본 T검정과는 달리 중앙값에 관한 결과를 얻을 수 있음  \n",
    "가정 : 표본은 동일한 모집단에서 추출되어야한다, 표본읜 임의, 독립적으로 추출되어야한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3-1.일표본(One Sample)\n",
    "   - alternative='two-sided'\n",
    "      - H0: 모집단의 중앙값은 A 이다.\n",
    "      - H1: 모집단의 중앙값은 A가 아니다.\n",
    "      - H0 : [관찰값 - A(중앙값이라 기대하는 값)] \"값 차이는 0이다.\" \n",
    "      - H1 : [관찰값 - A(중앙값이라 기대하는 값)] \"값 차이는 0이 아니다.\" \n",
    "   - alternative='less'\n",
    "      - H0: 모집단의 중앙값은 A보다 크거나 같다. (관찰값 - A의 값차이는 0보다 크거나 같다)\n",
    "      - H1: 모집단의 중앙값은 A보다 작다. (값 차이는 0보다 작다)\n",
    "   - alternative='greater'\n",
    "      - H0: 모집단의 중앙값은 A보다 작거나 같다.\n",
    "      - H1: 모집단의 중앙값은 A보다 크다.\n",
    "\n",
    "* from scipy.stats import wilcoxon  \n",
    "statstics, pvalue = wilcoxon(관측값 - 중앙값기대값, alternative='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1640625\n"
     ]
    }
   ],
   "source": [
    "# 중량이 100g 으로 표기된 닭가슴살 제품이 100g 이라고 할 수 있는가? \n",
    "# 동일한 회사 제품을 임의로 9개 표본 추출하였음 <- 샘플의 수가 너무 작아서 모수기법 사용 못함 / 비모수 기법 (비모수기법은 중앙값이 기준이됨 (모수기법은 평균))\n",
    "# 통계적 유의수준은 0.05로 사용함\n",
    "# H0: 닭 가슴살 중량의 중앙값은 100g이다\n",
    "# H1: 닭 가슴살 중량의 중앙값은 100g이 아니다\n",
    "\n",
    "# [1] 데이터 가져오기\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/chicken_breast.csv')\n",
    "# print(df)\n",
    "\n",
    "# [2] 중앙값 구하기\n",
    "median = df.weight.median()\n",
    "# print(median)\n",
    "\n",
    "# [3] 윌콕슨 검정 실행\n",
    "from scipy.stats import wilcoxon\n",
    "# print(scipy.stats.__all__)     # 목록 확인\n",
    "# help(wilcoxon)\n",
    "statistics, pvalue = wilcoxon(df['weight'] - 100, alternative='two-sided')\n",
    "print(pvalue)   # pvalue 0.16 -> 귀무가설 채택 -> 중앙값은 100g 맞음 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2) 대응표본 검정 :\n",
    "    - alternative='two-sided'\n",
    "        - H0: 값 차이(처리후 - 처리전)의 중앙값이 0이다(=값의 차이가 없다)\n",
    "        - H1: 값 차이(처리후 - 처리전)의 중앙값이 0이 아니다(=값의 차이가 있다)\n",
    "    - alternative='less'\n",
    "        - H0: 값 차이(처리후 - 처리전)의 중앙값이 0보다 크거나 같다\n",
    "        - H1: 값 차이(처리후 - 처리전)의 중앙값이 0보다 작다\n",
    "    - alternative='greater'\n",
    "        - H0: 값 차이(처리후 - 처리전)의 중앙값이 0보다 작거나 같다\n",
    "        - H1: 값 차이(처리후 - 처리전)의 중앙값이 0보다 크다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0078125\n"
     ]
    }
   ],
   "source": [
    "# 다이어트 후, 체중이 줄었다고 할 수 있는가? 몸무게 : 비율척도\n",
    "# 동일한 회사 제품을 임의로 8개 표본 추출하였음 <- 비모수 검정\n",
    "# 통계적 유의수준은 0.05로 사용함 \n",
    "\n",
    "# H0: 다이어트 후 - 다이어트 전 몸무게 중앙값은 0보다 크거나 같다\n",
    "# H1: 다이어트 후 - 다이어트 전 몸무게 중앙값은 0보다 작다.   <- alternative='less' \n",
    "\n",
    "# [1] 파일 불러오기\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/diet_result.csv')\n",
    "# print(df)\n",
    "# [2] wilcoxon sign rank sum - (paired t-test의 비모수)\n",
    "from scipy.stats import wilcoxon\n",
    "stat, p = wilcoxon(df['after'] - df['before'], alternative='less')\n",
    "print(p)    # p 0.007 -> 귀무가설 기각 -> 다이어트 효과 있다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 독립 2표본 \n",
    "- 독립 2표본 T 검정에서 정규성이 만족이 안될 때 사용하는 비모수 검정법  \n",
    "가정 : 두 그룹은 독립적이다, 측정값은 최소 순서형 변수이다(대소비교 가능해야함) \n",
    "    - alternative='two-sided'\n",
    "        - H0: gA의 중앙값과 gB의 중앙값이 같다(차이가 없다)\n",
    "        - H1: gA의 중앙값과 gB의 중앙값이 다르다(차이가 있다)\n",
    "    - alternative='less'\n",
    "        - H0: gA의 중앙값이 gB의 중앙값보다 크거나 같다\n",
    "        - H1: gA의 중앙값이 gB의 중앙값보다 작다\n",
    "    - alternative='greater'\n",
    "        - H0: gA의 중앙값이 gB의 중앙값보다 작거나 같다\n",
    "        - H1: gA의 중앙값이 gB의 중앙값보다 크다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) 윌콕슨 랜크 섬\n",
    "  \n",
    "from scipy.stats import ranksums  \n",
    "stat, p = ranksums(gA, gB, alternative='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ranksums\n",
    "# print(scipy.stats.__all__)\n",
    "# print(help(ranksums))\n",
    "\n",
    "# A사 닭가슴살 제품의 중량과 B사 닭가슴살 제품의 중량이 차이가 있는지 확인\n",
    "# A사 닭가슴살은 40개의 표본, B사 닭가슴살은 20개의 표본이 있으며\n",
    "# 각각 독립적이고 임의로 추출했다\n",
    "# 통계적 유의수준은 0.05 사용\n",
    "\n",
    "# H0: A사 닭가슴살 중량과 B사 닭가슴살 중량의 차이가 없다\n",
    "# H1: A사 닭가슴살 중량과 B사 닭가슴살 중량의 차이가 있다\n",
    "\n",
    "# [1] 파일 불러오기\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/chicken_weight_AB.csv')\n",
    "\n",
    "\n",
    "# [2] 그룹 나누기\n",
    "# print(df['company'].unique())\n",
    "gA = df.loc[df.company=='A', 'weight']\n",
    "gB = df.loc[df.company=='B', 'weight']\n",
    "\n",
    "# [3] wilcoxon rank sum test (two sample)\n",
    "from scipy.stats import ranksums\n",
    "# print(scipy.stats.__all__)\n",
    "# print(help(ranksums))\n",
    "stat, p = ranksums(gA, gB, alternative='two-sided')\n",
    "print(round(p,4))    # pvalue 0 -> 귀무가설 기각 -> 중량 차이 있다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2) 맨휘트니유 검정 \n",
    "  \n",
    "from scipy.stats import mannwhitneyu  \n",
    "stat, p = mannwhitneyu(gA, gB, alternative='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# H0: A사 닭가슴살 중량과 B사 닭가슴살 중량의 차이가 없다\n",
    "# H1: A사 닭가슴살 중량과 B사 닭가슴살 중량의 차이가 있다\n",
    "\n",
    "# 그룹나누기 : 위에서 이미 함\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "# print(scipy.stats.__all__)\n",
    "# help(mannwhitneyu)\n",
    "\n",
    "stat, p = mannwhitneyu(gA, gB, alternative='two-sided')\n",
    "print(round(p,4))   # pvalue 0 -> 귀무가설 기각 -> 중량 차이 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 다변량 추론 검정\n",
    "분산 분석(ANOVA)에서 정규성 가정이 만족되지 않을 때 사용하는 비모수 검정법  \n",
    "ANOVA와는 달리 중앙값에 관한 결과를 얻을 수 있음  \n",
    "가정 : 표본은 독립적이다, 측정값은 최소 순서형 변수이다. (대소 비교가 가능해야 합니다.)\n",
    "- 가설\n",
    "  - H0: 모든 그룹의 중앙값은 서로 같다\n",
    "  - H1: 모든 그룹의 중앙값이 전부 같은 것은 아니다\n",
    "  - alternative 옵션 주지 않음\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크루스칼 - 왈리스 검정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 5-1.K개 Sample\n",
    "# A, B, C 고등학교 학생들의 하루 공부 시간을 조사했을 때,고등학교 간에 공부 시간이 차이가 있는지 확인\n",
    "# 통계적 유의수준은 0.05\n",
    "\n",
    "# H0: A,B,C 고등학교 학생들의 하루 공부시간에 차이가 없다\n",
    "# H1: A,B,C 고등학교 학생들의 하루 공부시간에 차이가 있다\n",
    "\n",
    "# [1] 파일 불러오기\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/high_school.csv')\n",
    "# print(df.head(2))\n",
    "\n",
    "# [2] 그룹나누기\n",
    "# print(df['group'].unique())\n",
    "gA = df.loc[df.group=='A', 'time']\n",
    "gB = df.loc[df.group=='B', 'time']\n",
    "gC = df.loc[df.group=='C', 'time']\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "# print(scipy.stats.__all__)\n",
    "# help(kruskal)\n",
    "stat, p = kruskal(gA, gB, gC)\n",
    "print(round(p,4))   # pvalue 0 -> 귀무가설 기각 -> 고등학교 학생들의 공부 시간에 차이가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function kruskal in module scipy.stats._stats_py:\n",
      "\n",
      "kruskal(*samples, nan_policy='propagate', axis=0, keepdims=False)\n",
      "    Compute the Kruskal-Wallis H-test for independent samples.\n",
      "    \n",
      "    The Kruskal-Wallis H-test tests the null hypothesis that the population\n",
      "    median of all of the groups are equal.  It is a non-parametric version of\n",
      "    ANOVA.  The test works on 2 or more independent samples, which may have\n",
      "    different sizes.  Note that rejecting the null hypothesis does not\n",
      "    indicate which of the groups differs.  Post hoc comparisons between\n",
      "    groups are required to determine which groups are different.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    sample1, sample2, ... : array_like\n",
      "        Two or more arrays with the sample measurements can be given as\n",
      "        arguments. Samples must be one-dimensional.\n",
      "    nan_policy : {'propagate', 'omit', 'raise'}\n",
      "        Defines how to handle input NaNs.\n",
      "        \n",
      "        - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
      "          which the  statistic is computed, the corresponding entry of the output\n",
      "          will be NaN.\n",
      "        - ``omit``: NaNs will be omitted when performing the calculation.\n",
      "          If insufficient data remains in the axis slice along which the\n",
      "          statistic is computed, the corresponding entry of the output will be\n",
      "          NaN.\n",
      "        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
      "    axis : int or None, default: 0\n",
      "        If an int, the axis of the input along which to compute the statistic.\n",
      "        The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
      "        corresponding element of the output.\n",
      "        If ``None``, the input will be raveled before computing the statistic.\n",
      "    keepdims : bool, default: False\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    statistic : float\n",
      "        The Kruskal-Wallis H statistic, corrected for ties.\n",
      "    pvalue : float\n",
      "        The p-value for the test using the assumption that H has a chi\n",
      "        square distribution. The p-value returned is the survival function of\n",
      "        the chi square distribution evaluated at H.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    \n",
      "    :func:`f_oneway`\n",
      "        1-way ANOVA.\n",
      "    :func:`mannwhitneyu`\n",
      "        Mann-Whitney rank test on two samples.\n",
      "    :func:`friedmanchisquare`\n",
      "        Friedman test for repeated measurements.\n",
      "    \n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Due to the assumption that H has a chi square distribution, the number\n",
      "    of samples in each group must not be too small.  A typical rule is\n",
      "    that each sample must have at least 5 measurements.\n",
      "    \n",
      "    Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
      "    code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
      "    this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
      "    rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
      "    arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
      "    masked array with ``mask=False``.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n",
      "       One-Criterion Variance Analysis\", Journal of the American Statistical\n",
      "       Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n",
      "    .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy import stats\n",
      "    >>> x = [1, 3, 5, 7, 9]\n",
      "    >>> y = [2, 4, 6, 8, 10]\n",
      "    >>> stats.kruskal(x, y)\n",
      "    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n",
      "    \n",
      "    >>> x = [1, 1, 1]\n",
      "    >>> y = [2, 2, 2]\n",
      "    >>> z = [2, 2]\n",
      "    >>> stats.kruskal(x, y, z)\n",
      "    KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "help(kruskal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류모델에서 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation\n",
      "1    40.0\n",
      "2    46.0\n",
      "3    49.0\n",
      "4    29.0\n",
      "Name: Age, dtype: float64\n",
      "기각 \n"
     ]
    }
   ],
   "source": [
    "# - 1개 연속형 변수, 1개 범주형 변수 (4개 범주)\n",
    "# - 범주별로 연속형 변수의 평균 차이가 있는지 검정한다 -> 변수의 평균 차이 평균은 모수 검정 -> 차이가 있는지? 2개 비교하는 것 ttest_rel (근데 평균은 좀 틀린 말인듯)\n",
    "# 귀무가설 : Segmentation별 Age의 차이가 없다\n",
    "# 대립가설 : Segmentation별 Age의 차이가 있다   -> alternative='two-sided'\n",
    "\n",
    "# [1] 데이터 읽어오기 (4회기출)\n",
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Soyoung-Yoon/bigdata/main/train_04.csv')\n",
    "# print(data.head())\n",
    "\n",
    "# [2] 그룹 나누기 - 종속변수 Age, 독립변수 Segmentation\n",
    "# print(data['Age'].unique())\n",
    "# print(data['Segmentation'].unique())\n",
    "# print(data.info()) # 세그먼트 정수형\n",
    "gA = data.loc[data.Segmentation==1, 'Age']\n",
    "gB = data.loc[data.Segmentation==2, 'Age']\n",
    "gC = data.loc[data.Segmentation==3, 'Age']\n",
    "gD = data.loc[data.Segmentation==4, 'Age']\n",
    "\n",
    "# [3] 정규성 검정 \n",
    "from scipy.stats import shapiro\n",
    "stat1, p1 = shapiro(gA)\n",
    "stat2, p2 = shapiro(gB)\n",
    "stat2, p3 = shapiro(gC)\n",
    "stat2, p4 = shapiro(gD)\n",
    "# print(f'{p1:.4f}, {p2:.4f}, {p3:.4f}, {p4:.4f}')    # pvalue 0 -> 귀무가설 기각 -> 정규성 불만족 -> 비모수 검정\n",
    "\n",
    "# [4] 'Segmentation' 그룹별 'Age'의 중앙값 구하기\n",
    "print(data.groupby('Segmentation')['Age'].median()) # 40, 46, 49, 29 \n",
    "\n",
    "# [5] 크루스칼-왈리스 검정 수행\n",
    "from scipy.stats import kruskal\n",
    "stat, p = kruskal(gA, gB, gC, gD)\n",
    "# print(p)    # pvalue 0 -> 귀무가설 기각 -> 세그멘테이션별 나이 차이 있다. \n",
    "\n",
    "# # [6] 결과\n",
    "print('기각 ' if p < 0.05 else '채택')\n",
    "\n",
    "## mean() 과 median() 헛갈리지 말자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
