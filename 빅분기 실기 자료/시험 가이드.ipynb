{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형1 : 데이터 전처리 작업 3문제 * 10 [30점]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소수점 아래 2째자리까지 표시되도록 설정하기\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일 불러오기 \n",
    "df = pd.read_csv('파일경로.csv')\n",
    "## 불러오면서 인덱스 컬럼 설정\n",
    "df_csv1 = pd.read_csv('파일경로.csv', index_col='인덱스로 삼을 칼럼명')\n",
    "## 불러오면서 구분자 지정\n",
    "df_csv2 = pd.read_csv('파일경로.csv', sep='\\t')\n",
    "## 불러올 때 언어 설정\n",
    "df_csv3 = pd.read_csv('파일경로.csv', encoding='euc-kr')  # 한글지원 : euc-kr, cp949\n",
    "## 불러올 때 결측치 필터 사용 끄기\n",
    "df_csv2 = pd.read_csv('파일경로.csv', na_filter=False)\n",
    "\n",
    "\n",
    "# excel 파일 불러오기 (엑셀은 엔진 설정해주기!!)\n",
    "df_excel = pd.read_excel('파일경로.xlsx', engine='openpyxl')\n",
    "\n",
    "# csv 파일 저장하기\n",
    "df.to_csv('경로.csv', index=False)\n",
    "# excel 파일 저장하기\n",
    "df.to_excel('경로.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 구조 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이타가 많은 경우 모두 출력 안되고 ... 으로 생략해서 출력됩니다.\n",
    "# 생략되지 않는 행, 열의 개수를 설정하여 생략되지 않고 출력되도록 합니다.\n",
    "pd.set_option('display.max_rows', 800)    #출력할 max row를 지정\n",
    "pd.set_option('display.max_columns', 100)  #출력할 max columns를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 n개 행의 데이터 확인\n",
    "df.head()\n",
    "# 끝부터 n개 행의 데이터 확인\n",
    "df.tail()\n",
    "\n",
    "# 데이터프레임 row개수, colum개수, Not null,dtype 등 정보 확인\n",
    "df.info(memory_usage='deep')\n",
    "# 데이터프레임 통계 정보 확인\n",
    "df.describe()\n",
    "\n",
    "# 데이터프레임의 행, 열의 수를 (행, 열)형태의 튜블로 반환\n",
    "df.shape\n",
    "# 데이터프레임의 (행 x 열)의 전체 데이터 수를 반환\n",
    "df.size\n",
    "# 데이터프레임 데이터 타입 확인\n",
    "df.dtypes\n",
    "\n",
    "# 시리즈\n",
    "sri = df['특정 컬럼']\n",
    "# 시리즈 데이터 타입 확인\n",
    "sri.dtypes\n",
    "\n",
    "#데이터프레임의 인덱스 확인 (보통 0으로 시작해서 몇으로 끝나고 스텝이 몇인지 등을 보여줌)\n",
    "df.index\n",
    "# 데이터프레임의 컬럼 확인\n",
    "df.columns          # 타입 index\n",
    "df.columns.values   # 타입 array\n",
    "# 데이터프레임의 구성요소 2차원으로 보기\n",
    "df.values           # 타입 array\n",
    "\n",
    "# 시리즈의 구성요소\n",
    "sri.values          # 타입 array 해당 시리즈 값 배열로 쭉 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 중 컬럼에 존재하는 값 종류만 확인(결측치 포함, ndrarray로 반환)\n",
    "sri.unique()       \n",
    "# 시리즈 중 컬럼에 존재하는 값 종류의 개수만 확인\n",
    "sri.nunique()\n",
    "# 시리즈 중 컬럼에 대해 값 별 개수 확인(결측치 미포함, Series 반환)\n",
    "sri.value_counts() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 index, columns 및 Series의 index 는 대입연산을 사용하여 변경 가능 다만, 개수가 동일해야 함  \n",
    "  \n",
    "value는 인덱스를 이용해서 변경 가능  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 개수 확인 -> 컬럼 이름 리스트를 대입 연산자로 넣기(단, 개수 동일하게)\n",
    "df.columns = ['컬럼1', '컬럼2', '컬럼3']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 타입 변경\n",
    "데이터 타입 변경 전 데이터 조작이 필요할 수 있음 (예) 불필요 문자/콤마/공백 제거, 단위 변환 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 데이터 타입 변경 ('int', 'int32', 'int64', 'float', 'str', 'category' 등)\n",
    "## 넘파이 타입으로 하려면 넘파이 임포트 하여야함 (np.int16, np.float32, np.datetime64 등)\n",
    "sri.astype('타입')\n",
    "\n",
    "# 원하는 자료형으로 바꾸지 못하는 장애요소 제거하고 변환\n",
    "## replace 사용 시 regex=True 옵션을 사용하면 일부 내용만 변경대상으로 지정할 수 있음\n",
    "## 여기서 regex는 '정규표현식'을 의미\n",
    "sri.replace('변경전', '변경후', regex=True)\n",
    "# 변경할 내용이 두가지 이상일 때 list나 dic을 사용하여 변환\n",
    "list1 = ['변경전1', '변경전2']\n",
    "list2 = ['변경후1', '변경후2']\n",
    "sri.replace(list1, list2, regex=True)\n",
    "dic = {'변경전1':'변경후1', '변경전2':'변경후2'}\n",
    "sri.replace(dic, regex=True).astype('int64')\n",
    "\n",
    "## ,콤마는 메타 문자이므로 제거하고자 하는 경우 \\, 처럼 역슬레시 사용\n",
    "## 메타문자 종류 , . + ? * ^ $ 등\n",
    "sri.replace(['\\메타문자', '변견전2'], '', regex=True)  # 변경후 모습이 같다면 하나만 적어도 됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series는 Accessor라는 것을 가지고 있다. Accessor를 사용하기 위해서라도 데이터 타입 변경 필요\n",
    "- dt : Datetime, Timedelta, Period\n",
    "- str : String\n",
    "- cat : Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessor 사용 .str accessor 사용하면 각 행에 문자열 처럼 접근 \n",
    "sri.str[1:-1].astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 np.datetime64 나 category 타입 변경은 astype으로 변경 가능, 특수한 경우는 아래 방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime\n",
    "import numpy as np\n",
    "sri.astype(np.datetime64)  # 일/월/년 순으로 보기도 함 이런 경우 to_datatime을 사용\n",
    "# 내가 원하는 방식으로 날짜를 읽어오고 싶을 때\n",
    "## %Y: 4글자 년도, %y : 2글자 년도, %m : 2글자 월, %d : 2글자 일\n",
    "pd.to_datetime(sri, format='%y-%m-%d')\n",
    "\n",
    "# category\n",
    "sri.astype('category')  # 이렇게 변환한 자료에 sort_values 적용하면 가나다 순 정렬됨 \n",
    "# 카테고리 순서를 직접 지정하고 싶을 때\n",
    "temp = pd.Categorical(sri, categories=[\"범주1\", \"범주2\"], ordered=True)  # 이건 지금 시리즈는 아님\n",
    "sri = temp  # 이건 시리즈\n",
    "sri.sort_values() # 원하는 순서대로 정렬 할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 데이터 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_index() : 인덱스 정렬\n",
    "# [3-24] '측정일시'를 index로 설정하고,\n",
    "# index 기준으로 오름차순 정렬해서 df1으로 이름 붙입니다.\n",
    "# 그래프에서 y축으로 사용하려고 합니다.\n",
    "df1 = df.set_index('측정일시').sort_index()\n",
    "\n",
    "# sort_values() : 밸류 정렬, 디폴트(내림차순)\n",
    "df.sort_values('정렬기준칼럼', ascending=False)\n",
    "# 오름차순인 경우\n",
    "df.sort_values('정렬기준칼럼', ascending=True)\n",
    "# 정렬 기준이 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[False, False])  \n",
    "# 정렬 기준이 여러개, 내림차순 오름차순도 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[True, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 데이터 통계 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 통계 함수\n",
    "- axis=0 : 기본 값으로 행을 이동하면서, 행과 행의 연산을 수행한다.(수직으로 연산)\n",
    "- axis=1 : 컬럼을 이동하며 컬럼과 컬럼의 연산을 수행한다.(수평으로 연산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 통계량 확인\n",
    "df.describe()\n",
    "\n",
    "# 개수 세기\n",
    "df.count()\n",
    "# 합계\n",
    "df.sum()\n",
    "# 평균\n",
    "df.mean()\n",
    "# 표준편차 \n",
    "df.std()\n",
    "# 분산\n",
    "df.var()\n",
    "# 중앙값\n",
    "df.median()\n",
    "# 최빈값\n",
    "df.mode()  # 시리즈로 나옴\n",
    "# 최빈값 1개\n",
    "df.mode()[0]\n",
    "# 최대값\n",
    "df.max()   # 최대값, 최소값은 문자열에 대해서도 반응하는데 이는 ord(문자) 코드 숫자 기반\n",
    "# 최소값\n",
    "df.min()\n",
    "\n",
    "# 분위수\n",
    "df.quantile([0.25, 0.5, 0.75])    # 1사분위 0.25, 2사분위 0.5, 3사분위 0.75, \n",
    "# IQR\n",
    "df.quantile(0.75) - sri.quantile(0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통계 함수 여러개 적용하고 싶을 때 1\n",
    "df.apply([\"min\", \"max\", \"mean\"])    # apply 사용하면 어떤 함수든 적용 가능\n",
    "# 통계 함수 여러개 적용하고 싶을 때 2\n",
    "df.agg([\"min\", \"max\", \"mean\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 그룹별 통계\n",
    "- df.groupby(그룹명).통계함수() : 적용가능한 모든 단위\n",
    "- df.groupby(그룹명)[칼럼].통계함수 : Series 단위\n",
    "- df.groupby(그룹명)[[[칼럼1, 칼럼2 ... ]].통계함수 : 특정 컬럼 단위  \n",
    "- 그룹별로 통계치 구할 땐 agg(['var', 'std', 'mode'])로 여러개 가능\n",
    "\n",
    "* 통계 함수는 numerical 값에 적용되기 때문에 object는 형변환이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-54] 대륙별 주류 소비량 중앙값을 계산해 봅니다.\n",
    "df.groupby('대륙').median()\n",
    "\n",
    "# [2-55] 대륙별 맥주 소비량 평균은?\n",
    "df.groupby('대륙')['맥주'].mean()\n",
    "\n",
    "# [2-56] 전세계 맥주 소비량 평균보다 많은 맥주를 소비하는 대륙은?\n",
    "temp = df.groupby('대륙')[['맥주']].mean()\n",
    "temp[temp['맥주']> df['맥주'].mean()]\n",
    "\n",
    "# 그룹바이 기준이 두개 이상인 경우 멀티 인덱스로 작업\n",
    "# [3-34] df_dust에서 '년', '월'별 '미세먼지(㎍/㎥)' 데이터의 평균을 구해\n",
    "# DataFrame으로 만들어 meandf 라는 이름을 지정합니다.ㅣ\n",
    "meandf = df_dust.groupby(['년', '월'])[['미세먼지(㎍/㎥)']].mean()\n",
    "meandf.head()\n",
    "\n",
    "# [3-35] meandf에서 2017년 6월까지의 데이터만 출력합니다.  \n",
    "meandf.loc[:(2017,6),:] # 멀티인덱스 인경우 튜플로 표시한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피벗 테이블 : 행, 열 모두에 그룹으 지정하여 통계값 구하기\n",
    "- df.pivo_table(index=행방향칼럼, columns=열방향칼럼, values=집계대상칼럼, aggfunc=통계함수)\n",
    "- index, columns는 범주형, values는 연속형 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-58] pivot_table을 사용하여 대륙별(index), '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.pivot_table(index='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-59] pivot_table을 사용하여 대륙별(columns), '맥주'와 '와인'의 mean, median값을 구합니다.\n",
    "df.pivot_table(columns='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-60] groupby를 사용하여 대륙별, '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.groupby('대륙')['맥주', '와인'].agg(['mean', 'median', 'max'])\n",
    "# [2-58] 과 비슷하지만 피벗테이블을 쓰느냐 그룹바이를 쓰느냐에 따라 데이터프레임 구조가 미묘하게 다르다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Index, Columns 상호변경"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Columns to Index : df.set_index(['인덱스로 이동시킬 컬럼 명'..])\n",
    "- Index to Columns : df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-32] 국가별 주류 소비량 합계(맥주, 증류주, 와인의 합)를 구해 봅니다.\n",
    "df.set_index('국가')[['맥주', '증류주', '와인']].sum(axis=1)\n",
    "\n",
    "# [2-33] df를 ['대륙', '국가']를 index로 지정하고, 대륙별, 국가명으로  정렬하여 df로 저장합니다.\n",
    "df = df.set_index(['대륙','국가']).sort_index()\n",
    "\n",
    "# [2-34] df의 index를 모두 columns로 이동합니다.\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 데이터 정제하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 결측치\n",
    "- nan : 넘파이 배열에서 결측치 나타내는 경우\n",
    "- NaN : 판다스에서 시리즈나 데이터프레임에서 결측치 나타내는 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 찾기\n",
    "df.isna()\n",
    "df.isnull()\n",
    "# 결측치 합계 구하기\n",
    "df.isna().sum()\n",
    "\n",
    "# 결측치가 아닌 것 찾기\n",
    "df.notna()\n",
    "df.notnull()\n",
    "\n",
    "# 불리언 인덱싱으로 결측치만 찾기\n",
    "df[df['컬럼'].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 범주형 데이터 : 다른 범주로 만들어 채우기\n",
    "- 연속형 데이터 : 0으로 채우기, 평균값으로 채우기, 범주별 평균값으로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임의 결측치 채우기 (모두 같은 값으로)\n",
    "df = df.fillna('대체값')\n",
    "\n",
    "# 특정한 하나의 컬럼 결측치 채우기\n",
    "df['칼럼'] = df['칼럼'].fillna('대체값')\n",
    "df.loc[df['칼럼'].isna(), '대륙'] = '대체값'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "- 결측치 제거에 사용되는 메서드\n",
    "- how='any' : 결측치가 하나라도 포함된 행 삭제\n",
    "- how='all' : 모든 데이터가 결측치인 행 삭제\n",
    "- axis=1 : 컬럼에 대해 동작\n",
    "- thresh=숫자 : 숫자 이상의 데이터를 가진 행은 삭제 안함\n",
    "- subset=[컬럼이름1, ...] : subset으로 지정된 컬럼만 사용하여 삭제 대상 검색"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "df.dropna()\n",
    "\n",
    "# [3-30] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 모든 데이터가 결측치인 행을 제거하여 결과를 temp1으로 저장합니다\n",
    "temp1 = df_dust.dropna(how='all', axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])\n",
    "\n",
    "# [3-32] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 2개 이상의 데이터를 가진 행은 제거하지 않은 결과를 temp3로 저장합니다.\n",
    "# (= 3개의 정보 중 1개의 데이터만 가진 행을 제거함)\n",
    "temp3 = df_dust.dropna(thresh=2, axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측치 채우기\n",
    "Series.mask(조건, 조건이 참일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 True인 것에 대해 다른 값을 변경합니다.\n",
    "- sri.isna() : NA값에 대해 True, NA아닌 것은 False\n",
    "\n",
    "Series.where(조건, 조건이 거짓일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 False인 것에 대해서 다른 값으로 변경합니다.\n",
    "- sri.notna() : NA값에 대해 False, NA아닌 것은 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp의 'A' 열에 대해서 결측치인 경우 'B'의 값으로 대체합니다.\n",
    "temp['A'].mask(temp['A'].isna(), temp['B'])\n",
    "# temp의 'A' 열에 대해서 결측치인 경우 'C'의 값으로 대체합니다.\n",
    "temp['A'].where(temp['A'].notna(), temp['C'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 이상치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 파생 변수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 포함된 목록 확인\n",
    "- sri.isin() : \n",
    "- '찾을내용' in str : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].isin(['TV/방송', '게임'])]  # 또는 조건 isin으로도 해결 가능\n",
    "\n",
    "# isin([]) 목록이 한개여도 리스트 타입으로 넣어줘야 함\n",
    "df.loc[df['category'].isin(['음악/댄스/가수'])].sort_values('subscriber', ascending=False).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) str Accessor 사용\n",
    "- str.contains('문자열') : 특정 문자열을 포함하는지 아닌지를 True/False로 반환 (Boolean Indexing 조건으로 사용 가능)\n",
    "- str.upper() : 영문자 소문자를 대문자로 변경  /  str.lower() : 영문자 대문자를 소문자로 변경  \n",
    "- 세부 내용 : https://pandas.pydata.org/docs/reference/series.html#string-handling\n",
    "\n",
    "Series의 데이터를 list 및 ndarray로 반환 (데이터프레임 말고 시리즈에서 변환하는 것)\n",
    "- Series.to_list() : Series의 values를 list로 반환\n",
    "- Series.to_numpy() : Series의 values를 ndarray로 반환 (Series.values 와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1-44] title에 'KBS'가 포함된 채널 명 목록을 만들어 봅니다.\n",
    "df[df['title'].str.contains('KBS')]\n",
    "# 대소문자 구분 없이 검색 하려면? !str 두번 쓰면 됨\n",
    "df[df['title'].str.lower().str.contains('kbs')]\n",
    "\n",
    "# Series 형태를 list나 array 형태로 변환하기\n",
    "df[df['title'].str.contains('KBS')]['title'].to_list()\n",
    "df[df['title'].str.contains('KBS')]['title'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime에서 년 정보만 가져오기\n",
    "df['측정일시2'].dt.year\n",
    "# datetime에서 월 정보만 가져오기\n",
    "df['측정일시2'].dt.month\n",
    "# datetime에서 일 정보만 가져오기\n",
    "df['측정일시2'].dt.day\n",
    "# 월요일이 0 일요일이 6\n",
    "df['측정일시2'].dt.dayofweek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 불리언 인덱싱\n",
    "- df.loc[조건] : 조건은 boolean dtype이어야함 (기호 : | 선언, & 연언, ~ 부정) !!참 거짓으로 나올 수 있어야함\n",
    "- df.iloc[행범위, 열범위] : 인덱스 번호로 인덱싱 혹은 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예시 1-36] 'category'가 '음악/댄스/가수'인 채널의 subscriber TOP5를 알아봅니다.\n",
    "df.loc[df['category']=='음악/댄스/가수'].sort_values('subscriber').head()\n",
    "\n",
    "# [2-23] 와인 소비량이 맥주 + 증류주 소비량보다 큰 나라를 검색해,'대륙'을 기준으로 정렬해 보자\n",
    "df.loc[(df['와인']) > (df['맥주']+df['증류주'])].sort_values('대륙')\n",
    "\n",
    "# [2-24] 맥주 소비량이 230 초과이면서, 와인 소비량이 230 초과인 나라를 검색해 보자\n",
    "df.loc[(df['맥주']>230) & (df['와인']>230)]['국가']\n",
    "\n",
    "# [2-25] 대륙이 'AS'인 국가들의 정보를 검색해 보자\n",
    "df.loc[df['대륙']=='AS'].info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참 거짓으로 나오지 않는 사칙연산 같은 건 loc 쓰지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-27] 주류소비량2 = ['맥주', '증류주', '와인']에 대해 DataFrame.sum(axis=1) 함수 사용\n",
    "df['주류소비량2'] = df[['맥주', '증류주', '와인']].sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 추가, 제거 및 병합\n",
    "- 행 추가 : df.append(추가할 데이터프레임)\n",
    "- 컬럼 추가1 : df['새로운칼럼명'] = \n",
    "- 특정 위치에 컬럼 추가2 : df.insert(위치, 컬럼, 값) 단, inplace 동작됨\n",
    "- 열 제거1 : df = df.drop('열이름')    \n",
    "- 열 제거2 : df = df.drop(rows=['열이름1', '열이름2' ...])\n",
    "- 컬럼 제거1 : df = df.drop('칼럼명', axis=1)\n",
    "- 컬럼 제거2 : df = df.drop(columns=[컬럼명...])\n",
    "- 컬럼 제거3 : del df['제거할칼럼명'] 단, inplce 동작됨\n",
    "- 데이터프레임 행/열 전환 : df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-40] 세계의 각 컬럼별 평균을 구하여 DataFrame으로 만들고,\n",
    "# worldwide라는 이름을 지정합니다\n",
    "# 세계의 각 컬럼별 평균은 DataFrame.mean()을 사용합니다.\n",
    "worldwide = pd.DataFrame(df.mean(axis=0))\n",
    "\n",
    "# [2-41] worldwide의 행과 열을 전환해 wwT로 저장합니다.\n",
    "wwT = worldwide.T\n",
    "wwT\n",
    "\n",
    "# [2-42] wwT의 맨 앞에 '국가' 컬럼을 'World Wide' 값으로 추가합니다.\n",
    "# 여러 번 추가하면 안됨\n",
    "wwT.insert(0,'국가','World Wide') # 값을 여러개 줄 땐 리스트로 묶어줘야 한다.\n",
    "wwT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행 병합 : concat([합칠행1, 합칠행2] ..., axis=0)\n",
    "- 열 병합 : concat([합칠열1, 합칠열2] ..., axis=1)\n",
    "* df.append()와 달리 여러 개의 df를 목록을 주어 한 번에 합칠 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-44] wwT와 korea 를 합쳐 하나의 DataFrame을 생성하여 df2로 저장합니다.\n",
    "df2 = pd.concat([wwT,korea], axis=0)\n",
    "df2\n",
    "\n",
    "# [3-5] df2016, df2017, df2018, df2019를 합쳐 한 개의 DataFrame으로 만들어 df라는 이름을 지정합니다.\n",
    "dfList = [df2016, df2017, df2018, df2019]\n",
    "df = pd.concat(dfList, axis=0)\n",
    "\n",
    "# concat을 하더라도 인덱스 번호는 유지되기 때문에 인덱스번호 다시 설정해줘야함\n",
    "df.index = pd.RangeIndex(len(df))\n",
    "# concat 할때부터 옵션으로 인덱스 잡아주기\n",
    "df = pd.concat(dfList, ignore_index=True, axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3-25] df1의 '이산화질소농도(ppm)':'일산화탄소농도(ppm)'의 결측치의 상태를 그래프로 확인해 봅니다\n",
    "# seaborn.heatmap()를 사용하며, df.isna()를 데이터로 지정해 확인할 수 있습니다.\n",
    "# 흐린색이 결측치를 의미합니다.\n",
    "import koreanize_matplotlib\n",
    "\n",
    "plt.figure(figsize=(14, 10))    # 그래프 크기\n",
    "ax = sns.heatmap(df1.loc[:, '이산화질소농도(ppm)':'일산화탄소농도(ppm)'].isna(), cbar=False)    # 히트맵으로\n",
    "ax.set_xticklabels(ax.get_xticklabels(),fontsize=13, rotation=0)    # 눈금\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. 범주형 데이터 라벨링\n",
    "        1) LabelEncoding\n",
    "        2) one-hot-encoding(getdummys()?)\n",
    "    5. 뉴메리컬 데이터 스케일링\n",
    "        1) StandardScaler\n",
    "        2) MinMaxScaler\n",
    "        \n",
    "    6. 모델링\n",
    "        1) 지도학습 분류 : RandomforestClassifier\n",
    "        2) 지도학습 회귀 : RandomforestRegressor\n",
    "        3) 비지도학습 군집화\n",
    "    7. 모델 적용\n",
    "        1) train_test_split(층화 추출 옵션stratify = y)\n",
    "        2) new - fit - predict\n",
    "    8. 모델 성능 확인 : 문제에서 요구하는 평가지표에 맞게 수행\n",
    "        1) 지도 학습\n",
    "            - 정확도(accuracy)\n",
    "            - 정밀도(precision)\n",
    "            - 재현율(recall)\n",
    "            - 특이도(Specificity)\n",
    "            - f1_score : \n",
    "            - ROC(Receiver Operating Characteristic) 곡선: \n",
    "            - AUC(Are Under Curve) : \n",
    "        2) 회귀 학습\n",
    "            - SSE(Sum Squared Error) : \n",
    "            - MSE(Mean Squared Error) : \n",
    "            - RMSE(Root Mean Squared Error) :\n",
    "            - MAE(Mean Absolute Error) : \n",
    "            - 결정계수(R2) :\n",
    "            - Adjusted R2 : \n",
    "            - MSPE(Mean Squared Percentage Error) : \n",
    "            - MAPE(Mean Absolute Percentage Error) :\n",
    "            - RMSLE(Root Mean Squared Logarithmic Error) : \n",
    "            - AIC(Akaike Information Criterion) : \n",
    "            - BIC(Bayes Information Criteria) : \n",
    "        3) 비지도 학습\n",
    "            - 실루엣 계수(Silhouette) : \n",
    "            - Dunn lndex : \n",
    "            - 군집 분석\n",
    "            - 연관 분석\n",
    "    9. 예측 값 저장 \n",
    "        submission.columns([''])\n",
    "        to_csv('.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "유형3 : 데이터 검증 2문제(15*2)\n",
    "    1. 데이터 불러오기 : pd.read_csv('.csv', index_col=0)\n",
    "    2. 데이터 파악하기 : data.describe()\n",
    "    3. 데이터 검증하기\n",
    "    귀무가설(h0) 대신 대립가설(h1)을 선택할 시 오류를 범할 최대의 확률 값으로 0.05 이하이면 유의하여 대립가설 채택, 일반적으로 채택/기각은 귀무가설 기준\n",
    "        01) t-검정\n",
    "            (1)\n",
    "            (2)\n",
    "            (3)\n",
    "        02) 정규성 검정\n",
    "            (1)\n",
    "            (2)\n",
    "        03) One-way ANOVA\n",
    "        04) Two-way ANOVA\n",
    "        05) 카이제곱 검정\n",
    "            (1) 독립성 검정\n",
    "            (2) 동질성 검정\n",
    "            (3) 적합성 검정\n",
    "        06) 순위 검정\n",
    "            (1) 윌콕슨 순위합 검정\n",
    "            (2) 크루스칼 - 왈리스 검정\n",
    "        07) 부호검정\n",
    "            (1) 부호검정\n",
    "            (2) 맥네마 검정\n",
    "        08) 다변량 추론\n",
    "            (1) 다중 회귀 분석\n",
    "\n",
    "    4. 결과값 입력하기\n",
    "\n",
    "\n",
    "\n",
    "t-검정 이해하기\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
