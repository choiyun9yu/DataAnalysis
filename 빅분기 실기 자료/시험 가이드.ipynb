{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형1 : 데이터 전처리 3문제 * 10 [30점]\n",
    "- pandas를 활용한 데이터 탐색 작업에 대한 능력 확인\n",
    "- 주어진 데이터를 활용해 조건에 맞는 데이터를 추출하고 해당 데이터에 대한 통계량 구하거나 개수 세는 작업 요구\n",
    "- 1개의 정수로 답 print()  <- 언제든지 바뀔 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소수점 아래 2째자리까지 표시되도록 설정하기\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일 불러오기 \n",
    "df = pd.read_csv('파일경로.csv')\n",
    "## 불러오면서 인덱스 컬럼 설정\n",
    "df_csv1 = pd.read_csv('파일경로.csv', index_col='인덱스로 삼을 칼럼명')\n",
    "## 불러오면서 구분자 지정\n",
    "df_csv2 = pd.read_csv('파일경로.csv', sep='\\t')\n",
    "## 불러올 때 언어 설정\n",
    "df_csv3 = pd.read_csv('파일경로.csv', encoding='euc-kr')  # 한글지원 : euc-kr, cp949\n",
    "## 불러올 때 결측치 필터 사용 끄기\n",
    "df_csv2 = pd.read_csv('파일경로.csv', na_filter=False)\n",
    "\n",
    "\n",
    "# excel 파일 불러오기 (엑셀은 엔진 설정해주기!!)\n",
    "df_excel = pd.read_excel('파일경로.xlsx', engine='openpyxl')\n",
    "\n",
    "# csv 파일 저장하기\n",
    "df.to_csv('경로.csv', index=False)\n",
    "# excel 파일 저장하기\n",
    "df.to_excel('경로.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 구조 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이타가 많은 경우 모두 출력 안되고 ... 으로 생략해서 출력됩니다.\n",
    "# 생략되지 않는 행, 열의 개수를 설정하여 생략되지 않고 출력되도록 합니다.\n",
    "pd.set_option('display.max_rows', 800)    #출력할 max row를 지정\n",
    "pd.set_option('display.max_columns', 100)  #출력할 max columns를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 n개 행의 데이터 확인\n",
    "df.head()\n",
    "# 끝부터 n개 행의 데이터 확인\n",
    "df.tail()\n",
    "\n",
    "# 데이터프레임 row개수, colum개수, Not null,dtype 등 정보 확인\n",
    "df.info(memory_usage='deep')\n",
    "# 데이터프레임 통계 정보 확인\n",
    "df.describe()\n",
    "\n",
    "# 데이터프레임의 행, 열의 수를 (행, 열)형태의 튜블로 반환\n",
    "df.shape\n",
    "# 데이터프레임의 (행 x 열)의 전체 데이터 수를 반환\n",
    "df.size\n",
    "# 데이터프레임 데이터 타입 확인\n",
    "df.dtypes\n",
    "\n",
    "# 시리즈\n",
    "sri = df['특정 컬럼']\n",
    "# 시리즈 데이터 타입 확인\n",
    "sri.dtypes\n",
    "\n",
    "#데이터프레임의 인덱스 확인 (보통 0으로 시작해서 몇으로 끝나고 스텝이 몇인지 등을 보여줌)\n",
    "df.index\n",
    "# 데이터프레임의 컬럼 확인\n",
    "df.columns          # 타입 index\n",
    "df.columns.values   # 타입 array\n",
    "# 데이터프레임의 구성요소 2차원으로 보기\n",
    "df.values           # 타입 array\n",
    "\n",
    "# 시리즈의 구성요소\n",
    "sri.values          # 타입 array 해당 시리즈 값 배열로 쭉 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 중 컬럼에 존재하는 값 종류만 확인(결측치 포함, ndrarray로 반환)\n",
    "sri.unique()       \n",
    "# 시리즈 중 컬럼에 존재하는 값 종류의 개수만 확인\n",
    "sri.nunique()\n",
    "# 시리즈 중 컬럼에 대해 값 별 개수 확인(결측치 미포함, Series 반환)\n",
    "sri.value_counts() \n",
    "\n",
    "# 해당 칼럼의 결측치 개수 까지 같이 보고 싶은 경우\n",
    "sri.value_counts(dropna=False)\n",
    "# 해당 값들의 비율을 보고싶은 경우(결측치 비율까지 같이 나옴)\n",
    "sri.value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 타입 변경\n",
    "데이터 타입 변경 전 데이터 조작이 필요할 수 있음 (예) 불필요 문자/콤마/공백 제거, 단위 변환 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 데이터 타입 변경 ('int', 'int32', 'int64', 'float', 'str', 'category' 등)\n",
    "## 넘파이 타입으로 하려면 넘파이 임포트 하여야함 (np.int16, np.float32, np.datetime64 등)\n",
    "sri.astype('타입')\n",
    "\n",
    "# 원하는 자료형으로 바꾸지 못하는 장애요소 제거하고 변환\n",
    "## replace 사용 시 regex=True 옵션을 사용하면 일부 내용만 변경대상으로 지정할 수 있음\n",
    "## 여기서 regex는 '정규표현식'을 의미\n",
    "sri.replace('변경전', '변경후', regex=True)\n",
    "# 변경할 내용이 두가지 이상일 때 list나 dic을 사용하여 변환\n",
    "list1 = ['변경전1', '변경전2']\n",
    "list2 = ['변경후1', '변경후2']\n",
    "sri.replace(list1, list2, regex=True)\n",
    "dic = {'변경전1':'변경후1', '변경전2':'변경후2'}\n",
    "sri.replace(dic, regex=True).astype('int64')\n",
    "\n",
    "## ,콤마는 메타 문자이므로 제거하고자 하는 경우 \\, 처럼 역슬레시 사용\n",
    "## 메타문자 종류 , . + ? * ^ $ 등\n",
    "sri.replace(['\\메타문자', '변견전2'], '', regex=True)  # 변경후 모습이 같다면 하나만 적어도 됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series는 Accessor라는 것을 가지고 있다. Accessor를 사용하기 위해서라도 데이터 타입 변경 필요\n",
    "- dt : Datetime, Timedelta, Period\n",
    "- str : String\n",
    "- cat : Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessor 사용 .str accessor 사용하면 각 행에 문자열 처럼 접근 \n",
    "sri.str[1:-1].astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 np.datetime64 나 category 타입 변경은 astype으로 변경 가능, 특수한 경우는 아래 방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime\n",
    "import numpy as np\n",
    "sri.astype(np.datetime64)  # 일/월/년 순으로 보기도 함 이런 경우 to_datatime을 사용\n",
    "# 내가 원하는 방식으로 날짜를 읽어오고 싶을 때\n",
    "## %Y: 4글자 년도, %y : 2글자 년도, %m : 2글자 월, %d : 2글자 일\n",
    "pd.to_datetime(sri, format='%y-%m-%d')\n",
    "\n",
    "# category\n",
    "sri.astype('category')  # 이렇게 변환한 자료에 sort_values 적용하면 가나다 순 정렬됨 \n",
    "# 카테고리 순서를 직접 지정하고 싶을 때\n",
    "temp = pd.Categorical(sri, categories=[\"범주1\", \"범주2\"], ordered=True)  # 이건 지금 시리즈는 아님\n",
    "sri = temp  # 이건 시리즈\n",
    "sri.sort_values() # 원하는 순서대로 정렬 할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_index() : 인덱스 정렬\n",
    "# [3-24] '측정일시'를 index로 설정하고,\n",
    "# index 기준으로 오름차순 정렬해서 df1으로 이름 붙입니다.\n",
    "# 그래프에서 y축으로 사용하려고 합니다.\n",
    "df1 = df.set_index('측정일시').sort_index()\n",
    "\n",
    "# sort_values() : 밸류 정렬, 디폴트(내림차순)\n",
    "df.sort_values('정렬기준칼럼', ascending=False)\n",
    "# 오름차순인 경우\n",
    "df.sort_values('정렬기준칼럼', ascending=True)\n",
    "# 정렬 기준이 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[False, False])  \n",
    "# 정렬 기준이 여러개, 내림차순 오름차순도 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[True, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 index, columns 및 Series의 index 는 대입연산을 사용하여 변경 가능 다만, 개수가 동일해야 함  \n",
    "  \n",
    "value는 인덱스를 이용해서 변경 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 개수 확인 -> 컬럼 이름 리스트를 대입 연산자로 넣기(단, 개수 동일하게)\n",
    "df.columns = ['컬럼1', '컬럼2', '컬럼3']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼이름 변경하기\n",
    "- DataFrame.rename(columns={'변경전이름':'변경후이름', ...})\n",
    "- DataFrame.rename({'변경전이름':'변경후이름', ...}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-17] bread의 '상세영업상태코드'라는 컬럼명을 '상태코드'로 변경한 뒤,\n",
    "# 첫 2개의 행을 확인합니다.\n",
    "bread = bread.rename(columns={'상세영업상태코드':'상태코드'})\n",
    "bread.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Series.argmax() : 가장 값이 큰 것의 integer index 구하기\n",
    "- Series.argmin() : 가장 값이 작은 것의 integer index 구하기\n",
    "- Series[Series.argmax()] : 가장 큰 값 구하기\n",
    "- Series[Series.argmin()] : 가장 작은 값 구하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인덱싱 할 때 레이블을 쓰면 ['전':'후'] 에서 후 까지 포함  \n",
    "레이블이 아닌 숫자를 쓰면 [1:9] 뒤에 9 미포함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 통계 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 통계 함수\n",
    "- axis=0 : 기본 값으로 행을 이동하면서, 행과 행의 연산을 수행한다.(수직으로 연산)\n",
    "- axis=1 : 컬럼을 이동하며 컬럼과 컬럼의 연산을 수행한다.(수평으로 연산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 통계량 확인\n",
    "df.describe()\n",
    "\n",
    "# 개수 세기\n",
    "df.count()\n",
    "# 합계\n",
    "df.sum()\n",
    "# 노적\n",
    "df.cumsum()\n",
    "# 평균\n",
    "df.mean()\n",
    "# 표준편차 \n",
    "df.std()\n",
    "# 분산\n",
    "df.var()\n",
    "# 중앙값\n",
    "df.median()\n",
    "# 최빈값\n",
    "df.mode()  # 시리즈로 나옴\n",
    "# 최빈값 1개\n",
    "df.mode()[0]\n",
    "# 최대값\n",
    "df.max()   # 최대값, 최소값은 문자열에 대해서도 반응하는데 이는 ord(문자) 코드 숫자 기반\n",
    "# 최소값\n",
    "df.min()\n",
    "\n",
    "# 분위수\n",
    "df.quantile([0.25, 0.5, 0.75])    # 1사분위 0.25, 2사분위 0.5, 3사분위 0.75, \n",
    "# IQR\n",
    "df.quantile(0.75) - sri.quantile(0.25)\n",
    "# 상위20% 이상, 하위 20%도 이하 모두 이상이하로 계산한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통계 함수 여러개 적용하고 싶을 때 1\n",
    "df.apply([\"min\", \"max\", \"mean\"])    # apply 사용하면 어떤 함수든 적용 가능\n",
    "# 통계 함수 여러개 적용하고 싶을 때 2\n",
    "df.agg([\"min\", \"max\", \"mean\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 그룹별 통계\n",
    "- df.groupby(그룹명).통계함수() : 적용가능한 모든 단위\n",
    "- df.groupby(그룹명)[칼럼].통계함수 : Series 단위\n",
    "- df.groupby(그룹명)[[[칼럼1, 칼럼2 ... ]].통계함수 : 특정 컬럼 단위  \n",
    "- 그룹별로 통계치 구할 땐 agg(['var', 'std', 'mode'])로 여러개 가능\n",
    "\n",
    "* 통계 함수는 numerical 값에 적용되기 때문에 object는 형변환이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-54] 대륙별 주류 소비량 중앙값을 계산해 봅니다.\n",
    "df.groupby('대륙').median()\n",
    "\n",
    "# [2-55] 대륙별 맥주 소비량 평균은?\n",
    "df.groupby('대륙')['맥주'].mean()\n",
    "\n",
    "# [2-56] 전세계 맥주 소비량 평균보다 많은 맥주를 소비하는 대륙은?\n",
    "temp = df.groupby('대륙')[['맥주']].mean()\n",
    "temp[temp['맥주']> df['맥주'].mean()]\n",
    "\n",
    "# 그룹바이 기준이 두개 이상인 경우 멀티 인덱스로 작업\n",
    "# [3-34] df_dust에서 '년', '월'별 '미세먼지(㎍/㎥)' 데이터의 평균을 구해\n",
    "# DataFrame으로 만들어 meandf 라는 이름을 지정합니다.ㅣ\n",
    "meandf = df_dust.groupby(['년', '월'])[['미세먼지(㎍/㎥)']].mean()\n",
    "meandf.head()\n",
    "\n",
    "# [3-35] meandf에서 2017년 6월까지의 데이터만 출력합니다.  \n",
    "meandf.loc[:(2017,6),:] # 멀티인덱스 인경우 튜플로 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3-39] df_dust의 일자(년, 월, 일)별 '미세먼지(㎍/㎥)'의 평균을 구합니다.\n",
    "# 인덱스가 유지되지 않음\n",
    "df_dust.groupby(['측정일시'])['미세먼지(㎍/㎥)'].mean()  # <- 동일한 값을 갖음.  MultiIndex 아님\n",
    "# 인덱스가 유지되면서 그룹별 함수 적용\n",
    "df_dust.groupby(['측정일시'])['미세먼지(㎍/㎥)'].transform('mean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 피벗 테이블 : 행, 열 모두에 그룹으 지정하여 통계값 구하기\n",
    "- df.pivo_table(index=행방향칼럼, columns=열방향칼럼, values=집계대상칼럼, aggfunc=통계함수)\n",
    "- index, columns는 범주형, values는 연속형 사용\n",
    "\n",
    "* df.pivot_table(index=행방향그룹열이름, columns=열방향그룹열이름, values=집계대상열이름, aggfunc=통계함수)\n",
    "* index, columns는 범주형, values는 연속형 사용\n",
    "* values, aggfunc의 경우 단독의 경우 출력에 표시되지 않으나, 목록은 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-58] pivot_table을 사용하여 대륙별(index), '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.pivot_table(index='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-59] pivot_table을 사용하여 대륙별(columns), '맥주'와 '와인'의 mean, median값을 구합니다.\n",
    "df.pivot_table(columns='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-60] groupby를 사용하여 대륙별, '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.groupby('대륙')['맥주', '와인'].agg(['mean', 'median', 'max'])\n",
    "# 이건 [2-58] 과 비슷하지만 피벗테이블을 쓰느냐 그룹바이를 쓰느냐에 따라 데이터프레임 구조가 미묘하게 다르다.\n",
    "\n",
    "# [3-48] df_dust의 월/년 별 미세먼지의 'mean', 'min', 'max' 구하기\n",
    "# pivot_table 사용, values의 경우 목록으로 지정시와 단독 지정시가 다르게 표시됨\n",
    "df_dust.pivot_table(index='월', columns='년', values='미세먼지(㎍/㎥)', aggfunc=['mean', 'min', 'max'])\n",
    "\n",
    "# [3-49] df_dust에서 '측정소명'이 '강남구'인 데이터의\n",
    "# 월별(index), 년별(columns), 미세먼지 농도 평균을 조회하여 temp로 저장합니다\n",
    "temp = df_dust.loc[df['측정소명']=='강남구'].pivot_table(index='월', columns='년', values=['미세먼지(㎍/㎥)'], aggfunc=['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3-50] 2016년 ~ 2020년도 미세먼지 농도가 가장 높은 월의 위치\n",
    "temp = df_dust.pivot_table(index='월', columns='년', values='미세먼지(㎍/㎥)', aggfunc='mean')\n",
    "for year in temp.columns:\n",
    "    idx=temp[year].argmax()\n",
    "    print(temp.index[idx])\n",
    "\n",
    "# [3-52] 2016년 ~ 2019년 월별 미세먼지 평균을 구해 temp (DataFrame)로 저장합니다.\n",
    "temp = df_dust.loc[df_dust['년']<=2019].groupby('월')[['미세먼지(㎍/㎥)']].mean() \n",
    "\n",
    "# [3-52] 2016년 ~ 2019년 월별 미세먼지 평균을 구해 temp (DataFrame)로 저장합니다.\n",
    "temp = df_dust.loc[df_dust['년']<=2019].groupby('월')[['미세먼지(㎍/㎥)']].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Index, Columns 상호변경"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Columns to Index : df.set_index(['인덱스로 이동시킬 컬럼 명'..])\n",
    "- Index to Columns : df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-32] 국가별 주류 소비량 합계(맥주, 증류주, 와인의 합)를 구해 봅니다.\n",
    "df.set_index('국가')[['맥주', '증류주', '와인']].sum(axis=1)\n",
    "\n",
    "# [2-33] df를 ['대륙', '국가']를 index로 지정하고, 대륙별, 국가명으로  정렬하여 df로 저장합니다.\n",
    "df = df.set_index(['대륙','국가']).sort_index()\n",
    "\n",
    "# [2-34] df의 index를 모두 columns로 이동합니다.\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 정제하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 결측치\n",
    "- nan : 넘파이 배열에서 결측치 나타내는 경우\n",
    "- NaN : 판다스에서 시리즈나 데이터프레임에서 결측치 나타내는 경우 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 찾기\n",
    "df.isna()\n",
    "df.isnull()\n",
    "# 결측치 합계 구하기\n",
    "df.isna().sum()\n",
    "\n",
    "# 결측치가 아닌 것 찾기\n",
    "df.notna()\n",
    "df.notnull()\n",
    "\n",
    "# 불리언 인덱싱으로 결측치만 찾기\n",
    "df[df['컬럼'].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 범주형 데이터 : 다른 범주로 만들어 채우기\n",
    "- 연속형 데이터 : 0으로 채우기, 평균값으로 채우기, 범주별 평균값으로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임의 결측치 채우기 (모두 같은 값으로)\n",
    "df = df.fillna('대체값')\n",
    "\n",
    "# 특정한 하나의 컬럼 결측치 채우기\n",
    "df['칼럼'] = df['칼럼'].fillna('대체값')\n",
    "df.loc[df['칼럼'].isna(), '대륙'] = '대체값'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "- 결측치 제거에 사용되는 메서드\n",
    "- how='any' : 결측치가 하나라도 포함된 행 삭제\n",
    "- how='all' : 모든 데이터가 결측치인 행 삭제\n",
    "- axis=1 : 컬럼에 대해 동작\n",
    "- thresh=숫자 : 숫자 이상의 데이터를 가진 행은 삭제 안함\n",
    "- subset=[컬럼이름1, ...] : subset으로 지정된 컬럼만 사용하여 삭제 대상 검색"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "df.dropna()\n",
    "\n",
    "# [3-30] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 모든 데이터가 결측치인 행을 제거하여 결과를 temp1으로 저장합니다\n",
    "temp1 = df_dust.dropna(how='all', axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])\n",
    "\n",
    "# [3-32] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 2개 이상의 데이터를 가진 행은 제거하지 않은 결과를 temp3로 저장합니다.\n",
    "# (= 3개의 정보 중 1개의 데이터만 가진 행을 제거함)\n",
    "temp3 = df_dust.dropna(thresh=2, axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 결측치 대체\n",
    "Series.mask(조건, 조건이 참일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 True인 것에 대해 다른 값을 변경합니다.\n",
    "- sri.isna() : NA값에 대해 True, NA아닌 것은 False\n",
    "\n",
    "Series.where(조건, 조건이 거짓일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 False인 것에 대해서 다른 값으로 변경합니다.\n",
    "- sri.notna() : NA값에 대해 False, NA아닌 것은 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp의 'A' 열에 대해서 결측치인 경우 'B'의 값으로 대체합니다.\n",
    "temp['A'].mask(temp['A'].isna(), temp['B'])\n",
    "# temp의 'A' 열에 대해서 결측치인 경우 'C'의 값으로 대체합니다.\n",
    "temp['A'].where(temp['A'].notna(), temp['C'])\n",
    "\n",
    "# [3-43] meandf에 '결측치대체' 및 '차이2'라는 컬럼을 추가합니다.\n",
    "# '결측치대체' 컬럼은 df_dust에서 '년', '월'별 '미세먼지(㎍/㎥)' 데이터의 평균을 사용합니다.\n",
    "# '차이2' 컬럼은 '미세먼지(㎍/㎥)' - '결측치대체'를 사용합니다.\n",
    "meandf['결측치대체'] = df_dust.groupby(['년','월'])['미세먼지(㎍/㎥)'].mean()\n",
    "\n",
    "# [3-45] df_dust의 '오존농도(ppm)', '초미세먼지(㎍/㎥)' 컬럼에 대해서도\n",
    "# '미세먼지(㎍/㎥)'와 같이 동일한 '년', '월', '일'의 평균 값으로 채우기 합니다.\n",
    "fine_dust = df_dust.groupby('측정일시')['오존농도(ppm)'].transform('mean')\n",
    "s = df_dust['오존농도(ppm)']\n",
    "df_dust['오존농도(ppm)'] = s.mask(s.isna(), fine_dust)\n",
    "\n",
    "fine_dust = df_dust.groupby('측정일시')['초미세먼지(㎍/㎥)'].transform('mean')\n",
    "s = df_dust['초미세먼지(㎍/㎥)']\n",
    "df_dust['초미세먼지(㎍/㎥)'] = s.mask(s.isna(), fine_dust)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 이상치"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 이상치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 시리즈의 이상치를 확인하는 박스플롯 그리기\n",
    "sri.plot.box(figsize=(3,4))\n",
    "plt.show()\n",
    "\n",
    "# 데이터프레임의 연속형 데이터에 대해 모두 박스플롯 그리기\n",
    "df.plot(kind='box', subplots=True)\n",
    "plt.show()\n",
    "\n",
    "# [5] ESD(Extream Studentized Diviate)를 이용한 방법\n",
    "# 평균으로 부터 3 표준편차 떨어진 값을 이상치로 판단\n",
    "# tip에 대한 이상치 구하기, 소수점 아래 2째자리까지 표기\n",
    "s = tips['tip']\n",
    "s_mean, s_std = s.mean(), s.std()\n",
    "e_lower =  round(s_mean - 3 * s_std, 2) # round(숫자, 소수이하표시자리)\n",
    "e_upper =  round(s_mean + 3 * s_std, 2)\n",
    "print(f'Lower: {e_lower}, Upper: {e_upper}')\n",
    "# 소수점 아래 2째자리 까지 표기는 출력시에만 그렇지 않으면 이상치 대체할 때 박스플롯 밖에 점 생성\n",
    "\n",
    "# [6] 사분위수를 이용한 방법\n",
    "# Q1 - 1.5*IQR 미만, Q3 + 1.5*IQR 초과 를 이상치로 판단 (IQR = Q3 - Q1)\n",
    "# tip에 대한 이상치 구하기, 소수점 아래 2째자리까지 표기\n",
    "s = tips['tip']\n",
    "Q1, Q3 = s.quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "q_lower = Q1 - 1.5 * IQR\n",
    "q_upper = Q3 + 1.5 * IQR\n",
    "print(f'Lower: {round(q_lower,2)}, Upper: {round(q_upper,2)}')\n",
    "# 소수점 아래 2째자리 까지 표기는 출력시에만 그렇지 않으면 이상치 대체할 때 박스플롯 밖에 점 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정상범주에 있는 데이터를 indexing 하는 방법\n",
    "# [7] 이상치 제거 - 정상범주에 있는 데이터를 indexing 하는 방법으로 처리 (q_lower, q_upper 사이값이 정상)\n",
    "condition1 = tips['tip'] >= q_lower\n",
    "condition2 = tips['tip'] <= q_upper\n",
    "tips2 = tips.loc[condition1 & condition2, 'tip']\n",
    "print(tips.shape, tips2.shape)\n",
    "\n",
    "# [8] 이상치 데이터 모음\n",
    "condition1 = tips['tip'] <= q_lower\n",
    "condition2 = tips['tip'] >= q_upper\n",
    "tips_outlier = tips.loc[condition1 | condition2]\n",
    "tips_outlier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 이상치 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9] 이상값 대체\n",
    "# q_upper 보다 큰 데이터는 q_upper, q_lower 보다 작은 데이터는 q_lower로 변경\n",
    "# tips3 데이터프레임에서 'tip' 값이 q_upper 보다 큰 것의 'tip'값을 q_upper 값으로 변경\n",
    "# tips3 데이터프레임에서 'tip' 값이 q_lower 보다 작은 것의 'tip'값을 q_lower 값으로 변경\n",
    "tips3 = tips.copy()\n",
    "tips3.loc[tips3['tip'] > q_upper, 'tip'] = q_upper\n",
    "tips3.loc[tips3['tip'] < q_lower, 'tip'] = q_lower\n",
    "\n",
    "tips3['tip'].plot.box()\n",
    "plt.show()\n",
    "# 소수점 아래 2째자리 까지 표기는 출력시에만 그렇지 않으면 이상치 대체할 때 박스플롯 밖에 점 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 데이터 변환\n",
    "이상치를 완화하거나, 정규분포가 되도록 하기위해 사용, numpy의 log1p, sqrt, expm1, power 등의 함수 사용\n",
    "- log1p, sqrt는 큰 값을 작게 만들어주며, 오른쪽 꼬리가 긴 근포를 정규분포로 변환하는데 사용, 큰 이상치를 작게 만들 수 있음\n",
    "- expm1(exp마이너스원), power는 작은 값을 크게 만들어 주며, 왼쪽 꼬리가 긴 분포를 정규분포로 변환하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] tips에서 tip의 분포 확인  (어느쪽 꼬리가 긴지 확인하기 위함)\n",
    "s = tips['tip']\n",
    "s.plot.hist(bins=20)    # bin은 몇개 구간으로 나눌지 나타냄\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] y = log(x) 이해 -> np.power(np.e, y) = x  \n",
    "# np.log(100) : log만 덩그러니 적혀있으면 자연로그e(np.e)로 해석,\n",
    "# np.log10(100) : 10을 몇번 거듭제곱해야 100이라는 숫자가 되는지 구함 \n",
    "# np.power(10, 2) : 앞의 수를 뒤의 수로 거듭제곱하면 나오는 수 구함\n",
    "print(np.log10(100), np.power(10,2))\n",
    "print(np.e, np.log(100), np.power(np.e, 4.605170185988092))\n",
    "\n",
    "# [3] y = exp(x) 이해 -> y = power(np.e, x)\n",
    "# exp(x)는 np.e에 x만큼 거듭제곱하라는 뜻\n",
    "print(np.exp(4.605170185988092), np.power(np.e, 4.605170185988092))\n",
    "\n",
    "# [4] log(0) -> x가 0인 경우 -inf 이기 때문에 x에 +1을 해서 동작하는 np.log1p를 사용함  // -inf는 무한대라는 뜻\n",
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] tip의 분포를 오른쪽 꼬리가 긴 것에 대해서 짧게 만들기 (로그)\n",
    "s = np.log1p(sri)   # 시리즈 데이터 상ㅂ입\n",
    "s.plot.hist(bins=20)\n",
    "plt.show()\n",
    "\n",
    "# [6] tip의 분포를 오른쪽 꼬리가 긴 것에 대해서 짧게 만들기  (제곱근)\n",
    "s = np.sqrt(tips['tip'])\n",
    "s.plot.hist(bins=20)\n",
    "plt.show()\n",
    "\n",
    "# [7] tips['tip'] 원본 -> log1p -> expm1 = 원본 (로그 취했던 것)\n",
    "a = tips['tip']\n",
    "b = np.log1p(a)  # 변환\n",
    "c = np.expm1(b)\n",
    "print(a[:3], b[:3], c[:3], sep='\\n\\n')\n",
    "\n",
    "# [8] tips['tip'] 원본 - sqrt - power = 원본 (제곱근 취했던 것)\n",
    "a = tips['tip']\n",
    "b = np.sqrt(a)\n",
    "c = np.power(b, 2)\n",
    "print(a[:3], b[:3], c[:3], sep='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 파생 변수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 포함된 목록 확인\n",
    "- sri.isin() : \n",
    "- '찾을내용' in str : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].isin(['TV/방송', '게임'])]  # 또는 조건 isin으로도 해결 가능\n",
    "\n",
    "# isin([]) 목록이 한개여도 리스트 타입으로 넣어줘야 함\n",
    "df.loc[df['category'].isin(['음악/댄스/가수'])].sort_values('subscriber', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-26] bread에서 '사업장명' 컬럼을 사용하여\n",
    "# '파리바게트', '파리바게뜨' 이름인 곳을 뽑아 paris로 이름 붙입니다.\n",
    "bread[bread['사업장명'].str.contains('파리바게트', '파리바게뜨')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Accessor 사용\n",
    "Series 타입에 .str을 붙여서 문자열 메소드 쓰는 것\n",
    "- str.contains('문자열') : 특정 문자열을 포함하는지 아닌지를 True/False로 반환 (Boolean Indexing 조건으로 사용 가능)\n",
    "- str.upper() : 영문자 소문자를 대문자로 변경  /  str.lower() : 영문자 대문자를 소문자로 변경  \n",
    "- 세부 내용 : https://pandas.pydata.org/docs/reference/series.html#string-handling\n",
    "\n",
    "Series의 데이터를 list 및 ndarray로 반환 (데이터프레임 말고 시리즈에서 변환하는 것)\n",
    "- Series.to_list() : Series의 values를 list로 반환\n",
    "- Series.to_numpy() : Series의 values를 ndarray로 반환 (Series.values 와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1-44] title에 'KBS'가 포함된 채널 명 목록을 만들어 봅니다.\n",
    "df[df['title'].str.contains('KBS')]\n",
    "# 대소문자 구분 없이 검색 하려면? !str 두번 쓰면 됨\n",
    "df[df['title'].str.lower().str.contains('kbs')]\n",
    "\n",
    "# Series 형태를 list나 array 형태로 변환하기\n",
    "df[df['title'].str.contains('KBS')]['title'].to_list()\n",
    "df[df['title'].str.contains('KBS')]['title'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime에서 년 정보만 가져오기\n",
    "df['측정일시2'].dt.year\n",
    "# datetime에서 월 정보만 가져오기\n",
    "df['측정일시2'].dt.month\n",
    "# datetime에서 일 정보만 가져오기\n",
    "df['측정일시2'].dt.day\n",
    "# 월요일이 0 일요일이 6\n",
    "df['측정일시2'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-8] bread 의 '소재지전체주소' 중 시/도에 대한 정보(목록)를 추출합니다.\n",
    "bread['소재지전체주소'].str.split(' ').str[0]   # str 여러번 붙여 써도 괜찮음\n",
    "\n",
    "# [4-9] bread에서 소재지전체주소의 처음이 '서울특별시'이면서,\n",
    "# '업태구분명'이 '제과점영업'인 것만 추출합니다.\n",
    "bread[(bread['업태구분명']=='제과점영업') & (bread['소재지전체주소'].str.split(' ').str[0]=='서울특별시')]\n",
    "# 이렇게 정리해도 괜찮은 듯\n",
    "condition1 = bread['업태구분명']=='제과점영업'\n",
    "condition2 = bread['소재지전체주소'].str.split(' ').str[0]=='서울특별시'\n",
    "bread = bread[condition1 & condition2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- str.strip('제거할 문자들') : 문자열의 앞/뒤에 불필요한 것을 제거함\n",
    "   - 제거할 문자들을 지정하지 않을 경우 whitespace를 제거함\n",
    "- str.split('구분자')\n",
    "   - 구분자를 지정하지 않을 경우 whitespace를 기준으로 분리함\n",
    "   - 각 구분된 내용은 str[0], str[1], .. 등으로 접근\n",
    "- str.join('구분자')\n",
    "   - 구분자 지정을 생략할 수 없음\n",
    "   - 분리된 문자열을 구분자를 사이에 넣어 하나의 문자열로 만듦\n",
    "- str.replace(전, 후)\n",
    "   - 문자열의 일부 내용을 변경 가능함\n",
    "   - 변경전 내용을 찾아 변경후 내용으로 바꿈   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'A': ['    김   수민 ', '  이  나라     ', '  황   소  라  '],\n",
    "        'B': ['  d2021-10-29.   ', '   \\n\\t\\r2021-10-30c    \\n', '2021-11-01c   '],\n",
    "        'C': ['*7', '6', '*7']}\n",
    "temp = pd.DataFrame(data)\n",
    "print(temp)\n",
    "\n",
    "# [1] 'A' 컬럼의 데이터를 빈칸 없는 이름으로 만들어 temp의 'A'컬럼 뒤에 'A-01'컬럼으로 추가해 보세요.\n",
    "temp.insert(1, column=' A-01', \n",
    "            value=temp['A'].str.split().str.join(''))        \n",
    "\n",
    "# [2] 'B' 컬럼의 데이티를 2021-10-29 처럼 앞/뒤에 공백이나 다른 문자('.dc')가 없도록 만들어\n",
    "# temp에 'B-01' 컬럼으로 추가해 보세요.\n",
    "temp.insert(3, column='B-01', \n",
    "            value=temp['B'].str.strip().str.strip('d.c'))\n",
    "\n",
    "# [3] 'B-01' 컬럼의 데이터에서 '-'를 '/'로 수정해 temp에 'B-02' 컬럼으로 추가해 보세요.\n",
    "temp.insert(4, column='B-02',\n",
    "            value=temp['B-01'].str.replace('-', '/'))\n",
    "\n",
    "# [4] 'C' 컬럼에서 *을 제거하고 숫자로 변경해 'C-01'컬럼으로 추가해 보세요.\n",
    "temp['C-01'] = temp['C'].str.replace('*','').astype(int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 불리언 인덱싱\n",
    "- df.loc[조건] : 조건은 boolean dtype이어야함 (기호 : | 선언, & 연언, ~ 부정) !!참 거짓으로 나올 수 있어야함\n",
    "- df.iloc[행범위, 열범위] : 인덱스 번호로 인덱싱 혹은 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예시 1-36] 'category'가 '음악/댄스/가수'인 채널의 subscriber TOP5를 알아봅니다.\n",
    "df.loc[df['category']=='음악/댄스/가수'].sort_values('subscriber').head()\n",
    "\n",
    "# [2-23] 와인 소비량이 맥주 + 증류주 소비량보다 큰 나라를 검색해,'대륙'을 기준으로 정렬해 보자\n",
    "df.loc[(df['와인']) > (df['맥주']+df['증류주'])].sort_values('대륙')\n",
    "\n",
    "# [2-24] 맥주 소비량이 230 초과이면서, 와인 소비량이 230 초과인 나라를 검색해 보자\n",
    "df.loc[(df['맥주']>230) & (df['와인']>230)]['국가']\n",
    "\n",
    "# [2-25] 대륙이 'AS'인 국가들의 정보를 검색해 보자\n",
    "df.loc[df['대륙']=='AS'].info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참 거짓으로 나오지 않는 사칙연산 같은 건 loc 쓰지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-27] 주류소비량2 = ['맥주', '증류주', '와인']에 대해 DataFrame.sum(axis=1) 함수 사용\n",
    "df['주류소비량2'] = df[['맥주', '증류주', '와인']].sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 추가, 제거 및 병합\n",
    "- 행 추가 : df.append(추가할 데이터프레임)\n",
    "- 컬럼 추가1 : df['새로운칼럼명'] = \n",
    "- 특정 위치에 컬럼 추가2 : df.insert(위치, 컬럼, 값) 단, inplace 동작됨\n",
    "- 열 제거1 : df = df.drop('열이름')    \n",
    "- 열 제거2 : df = df.drop(rows=['열이름1', '열이름2' ...])\n",
    "- 컬럼 제거1 : df = df.drop('칼럼명', axis=1)\n",
    "- 컬럼 제거2 : df = df.drop(columns=[컬럼명...])\n",
    "- 컬럼 제거3 : del df['제거할칼럼명'] 단, inplce 동작됨\n",
    "- 데이터프레임 행/열 전환 : df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-40] 세계의 각 컬럼별 평균을 구하여 DataFrame으로 만들고,\n",
    "# worldwide라는 이름을 지정합니다\n",
    "# 세계의 각 컬럼별 평균은 DataFrame.mean()을 사용합니다.\n",
    "worldwide = pd.DataFrame(df.mean(axis=0))\n",
    "\n",
    "# [2-41] worldwide의 행과 열을 전환해 wwT로 저장합니다.\n",
    "wwT = worldwide.T\n",
    "wwT\n",
    "\n",
    "# [2-42] wwT의 맨 앞에 '국가' 컬럼을 'World Wide' 값으로 추가합니다.\n",
    "# 여러 번 추가하면 안됨\n",
    "wwT.insert(0,'국가','World Wide') # 값을 여러개 줄 땐 리스트로 묶어줘야 한다.\n",
    "wwT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행 병합 : concat([합칠행1, 합칠행2] ..., axis=0)\n",
    "- 열 병합 : concat([합칠열1, 합칠열2] ..., axis=1)\n",
    "* df.append()와 달리 여러 개의 df를 목록을 주어 한 번에 합칠 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-44] wwT와 korea 를 합쳐 하나의 DataFrame을 생성하여 df2로 저장합니다.\n",
    "df2 = pd.concat([wwT,korea], axis=0)\n",
    "df2\n",
    "\n",
    "# [3-5] df2016, df2017, df2018, df2019를 합쳐 한 개의 DataFrame으로 만들어 df라는 이름을 지정합니다.\n",
    "dfList = [df2016, df2017, df2018, df2019]\n",
    "df = pd.concat(dfList, axis=0)\n",
    "\n",
    "# concat을 하더라도 인덱스 번호는 유지되기 때문에 인덱스번호 다시 설정해줘야함\n",
    "df.index = pd.RangeIndex(len(df))\n",
    "# concat 할때부터 옵션으로 인덱스 잡아주기\n",
    "df = pd.concat(dfList, ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-20] bread에 '설립년도' 및 '폐업년도' 컬럼을 추가합니다.\n",
    "# '인허가일자'//10000, '폐업일자 // 10000 을 사용하여 구합니다.\n",
    "# 두 개의 컬럼이 추가된 bread의 첫 2개 행을 확인합니다.\n",
    "\n",
    "# 정수형인경우\n",
    "year = bread['인허가일자'] // 10000\n",
    "meonth = bread['인허가일자'] // 100 % 100\n",
    "day = bread['인허가일자'] %100\n",
    "\n",
    "# datetime 으로 바꿔서\n",
    "temp = pd.to_datetime(bread['인허가일자'], foramt='%Y,%m%d')\n",
    "temp.dt.year\n",
    "temp.dt.moth\n",
    "temp.dt.day\n",
    "\n",
    "# 문자열인경우 (슬라이싱으로)\n",
    "\n",
    "\n",
    "# 이번에는 정수형으로 함\n",
    "bread['설립년도'] = bread['인허가일자']//10000\n",
    "bread['폐업년도'] = bread['폐업일자']//10000\n",
    "bread.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-21] bread에 '영업기간' 컬럼을 추가합니다\n",
    "# '영업기간'은 '상태코드'가 1(=영업)인 경우 2021 - 설립년도 +1\n",
    "# '상태코드'가 2(=폐업)인 경우 폐업년도 - 설립년도 + 1로 계산합니다.\n",
    "\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "today.year\n",
    "today.month\n",
    "today.day\n",
    "\n",
    "nyear = today.year\n",
    "bread.loc[bread['상태코드']==1,'영업기간'] = nyear - bread['설립년도'] + 1\n",
    "bread.loc[bread['상태코드']==2, '영업기간'] = bread['폐업년도'] - bread['설립년도'] + 1\n",
    "bread.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-22] bread의 '설립년도'별 데이터 수를 구해 년도별로 정렬하고,\n",
    "# DataFrame으로 변경하여 전치행렬을 구해 temp1 이름을 부여해 출력합니다.\n",
    "temp1 = bread['설립년도'].value_counts().sort_index(ascending=True)\n",
    "temp1 = pd.DataFrame(temp1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-28] 설립년도가 2000년 이후이면서 영업 중인 곳의 영업기간 정보를 구합니다.\n",
    "# paris, tous에 대해 각각 구해서 temp1, temp2로 이름 붙입니다.\n",
    "temp1 = paris.loc[(paris['설립년도']>=2000) & (paris['상태코드']==1), '영업기간']\n",
    "temp2 = tous.loc[(tous['설립년도']>=2000) & (tous['상태코드']==1), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 이름을 comp로 하는 DataFrame으로 만듭니다.\n",
    "# index => ['파리바게트', '뚜레쥬르'], columns => ['영업']\n",
    "# 선생님방법\n",
    "s = pd.Series([temp1.mean(), temp2.mean()], index=['파리바게트', '뚜레쥬르'])\n",
    "comp = pd.DataFrame(s, columns=['영업'])\n",
    "comp\n",
    "\n",
    "# [4-29] 설립년도 2000년 이후이면서 폐업한 곳의 영업기간 정보를 구합니다.\n",
    "# paris, tous에 대해 각각 구해서 temp1, temp2로 이름 붙입니다.\n",
    "temp1 = paris.loc[(paris['설립년도']>=2000) & (paris['상태코드']==2), '영업기간']\n",
    "temp2 = tous.loc[(tous['설립년도']>=2000) & (tous['상태코드']==2), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 comp에 '폐업' 컬럼으로 추가합니다.\n",
    "comp['폐업']= [temp1.mean(), temp2.mean()]\n",
    "comp\n",
    "\n",
    "# [4-31] other의 2000년 이후 설립된 곳의 영업, 폐업 사업장을 구한 뒤\n",
    "# temp1, temp2 이름을 붙입니다.\n",
    "temp1 = other.loc[(other['설립년도']>=2000) & (other['상태코드']==1), '영업기간']\n",
    "temp2 = other.loc[(other['설립년도']>=2000) & (other['상태코드']==2), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 comp 에 '나머지' 행으로 추가합니다.\n",
    "temp = pd.DataFrame([[temp1.mean(), temp2.mean()]], index=['나머지'], columns=['영업', '폐업'])\n",
    "comp = comp.append(temp)\n",
    "comp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 데이터 표본 추출(샘플링)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- df.sample(n=개수, frac=비율, random_state=None, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 무작위 추출\n",
    "df.sample(frac=0.02, random_state=777)\n",
    "df.sample(n=5, random_state=777)\n",
    "\n",
    "# 계통 추출 : 시작 위치부터 일정한 간격으로 추출하는 것 \n",
    "df.iloc[::50,:]   # 인덱스 50단위로 계통 추출\n",
    "\n",
    "# 층화 추출 : 독립변수의 unique값이 실제 분포하는 비율에 맞춰서 데이터 추출\n",
    "# [5] 층화 추출 - 성별을 기준으로 성별 비율에 맞춰 10개 데이터 추출\n",
    "# [5-1] 성별 비율 구하기(rate), 추출할 sample 개수 구하기(sample_n)\n",
    "sample_n = 20   # 전체적으로 사용할 샘플의 개수\n",
    "temp = tips['sex'].value_counts(normalize=True).to_frame()\n",
    "temp.columns=['rate']\n",
    "temp['sample_n'] = round(sample_n * temp['rate'], 0).astype('int')\n",
    "# [5-2] Female, Male로 데이터 분리 및 sample개수 만큼의 임의 표본 추출\n",
    "Female = tips.loc[tips['sex']=='Female']\n",
    "Male = tips.loc[tips['sex']=='Male']\n",
    "df1 = Female.sample(n=temp.loc['Female', 'sample_n'], random_state=1)\n",
    "df2 = Male.sample(n=temp.loc['Male', 'sample_n'], random_state=1)\n",
    "df = pd.concat([df1, df2])\n",
    "df.sample(frac=1, random_state=1)   # 섞기 위해서 frac=1 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 도수분포표"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관측 값을 몇 개 범주로 나눈 다음 그 범주에 속하는 관측 값의 개수(도수)와 그 도수를 전체 관측 값의 개수로 나눈 값(상대도수)에 대한 표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 데이터 생성하기\n",
    "data = ['A+', 'A', 'B+', 'B', 'C+', 'C', 'D+', 'D', 'F']\n",
    "cnt  = [3, 6, 12, 16, 10, 8, 4, 1, 2]\n",
    "mylist = []\n",
    "for s, c in zip(data, cnt):\n",
    "    mylist.extend([s] * c)\n",
    "\n",
    "df = pd.DataFrame(mylist, columns=['data'])\n",
    "df = df.sample(frac=1)\n",
    "print(df['data'].to_list())\n",
    "\n",
    "# [2] mylist를 사용하여 돗수분포표를 만들어 본다\n",
    "s = pd.Categorical(df['data'], categories=data, ordered=True)\n",
    "s = s.value_counts().sort_index()\n",
    "print(s)\n",
    "\n",
    "# [3] s를 사용하여 돗수분포표의 비율, 누적인원, 누적비율을 완성해 본다\n",
    "df = pd.DataFrame(s, columns=['인원'])\n",
    "df.index.name = '학점'\n",
    "df['비율'] = (df['인원']/ df['인원'].sum()).round(2)\n",
    "df['누적인원'] = df['인원'].cumsum()\n",
    "df['누적비율'] = df['비율'].cumsum()\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 상관계수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] 데이터 불러오기\n",
    "# sns.load_dataset(데이터셋의 이름)\n",
    "# iris(붓꽃) 데이터를 불러옴\n",
    "# sepal : 꽃받침, petal : 꽃잎\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()\n",
    "\n",
    "# [1] species를 LabelEncoding하여 'species_LE' 컬럼 추가하기\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "ans = LabelEncoder().fit_transform(iris['species'])\n",
    "\n",
    "# [2] iris의 상관계수 구하기\n",
    "iris['species'] = LabelEncoder().fit_transform(iris['species'])\n",
    "\n",
    "iris.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형2 : 데이터 모델링 1문제 * 40 [40점]\n",
    "- pandas 및 sklearn을 활용한 머신러닝 작업에 대한 능력 확인\n",
    "- 주어진 데이터를 활용해 전처리, 모형 구축, 평가 작업을 한 뒤\n",
    "- 예측 결과를 csv파일로 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 스케일링(수치형 데이터 스케일링)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min-max normalization : 값의 범위를 [0, 1]로 변환\n",
    "- (xi - x.min()) / (x.max() - x.min())  \n",
    "  \n",
    "standardiztion : 특성의 값이 표준 정규분포를 갖도록 변환 (평균 0, 표준편차 1)\n",
    "- (xi - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Scaling을 위한 함수 구현\n",
    "def minmaxScale(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "def standardScale(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "# [2] minmaxScale 함수를 사용한 'tip' 컬럼의 스케일링\n",
    "x = minmaxScale(tips['tip'])\n",
    "\n",
    "# [3] standardScale 함수를 사용한 'tip' 컬럼의 스케일링\n",
    "x = standardScale(tips['tip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] sklearn 라이브러리의 스케일러(MinMaxScaler)를 사용한 스케일링\n",
    "# 스케일러의 fit_transform() 사용시 2차원의 데이터를 전달해야 함\n",
    "# (DataFrame도 2차원), 결과는 ndarray로 반환 됨\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "x = MinMaxScaler().fit_transform(tips[['tip']])     # 이차원으로된 데이터를 넣어줘야하고, fit_transform하는 것을 까먹지 않으면 제일 편한 방법\n",
    "print(x.min(), x.max())\n",
    "\n",
    "# [5] sklearn 라이브러리의 스케일러(StandardScaler)를 사용한 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = StandardScaler().fit_transform(tips[['tip']])\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6] scipy.stats의 zscore 함수를 사용한 스케일링\n",
    "from scipy.stats import zscore\n",
    "x = zscore(tips['tip'])\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 인코딩(범주형 데이터 라벨링) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 위해 범주형 데이터를 수치형으로 바꿔준다.  \n",
    "  \n",
    "label Encoding : 값의 일련번호로 표기함 : 순서가 있는 경우 유리, 값이 2개 밖에 없는 경우\n",
    "- sri.replace() 사용  \n",
    "\n",
    "one hot encoding : 범주의 개수 만큼 feature를 만들어냄 : 순서가 없는 경우 유리\n",
    "- pd.get_dummies(sri)\n",
    "- pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'A':['월', '화', '수', '화', '수', '금', '월'],\n",
    "                   'B':['여자', '남자', '여자', '남자', '아이', '남자', '아이']})\n",
    "\n",
    "# [1] 요일에 대해서 Label Encoding 합니다.  (cat.code 사용)\n",
    "weekdays = '월 화 수 목 금 토 일'.split()\n",
    "print(weekdays)\n",
    "df['A_LE'] = pd.Categorical(df['A'], weekdays, ordered=True)\n",
    "df['A_LE'] = df['A_LE'].cat.codes\n",
    "\n",
    "# [2] '남자', '여자', '아이'에 대해서 Label Encoding 합니다 (replace 사용)\n",
    "v = df['B'].unique()\n",
    "df['B'].replace(v, range(len(v)))\n",
    "\n",
    "# [3] df의 'A', 'B' 컬럼을 One Hot Encoding 합니다.\n",
    "a = pd.get_dummies(df['A'])\n",
    "b = pd.get_dummies(df['B'])\n",
    "df2 = pd.concat([a,b], axis=1)\n",
    "\n",
    "# [6] df의 모든 범주형 변수를 OneHotEncoding 합니다\n",
    "df3 = pd.get_dummies(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binning (연속형 변수를 구간을 이용해 범주화)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연속형 변수를 구간을 이용해 범주화하여 정보를 압축시키고 단순하게 만듬 (정확도는 떨어짐)  \n",
    "이상치 해결방법 중 한가지로 사용하거나, 오버피팅 방지 기법으로 사용되기도 함\n",
    "- (3,6] : 3초과 6이하, right=True\n",
    "- [3, 6) : 3이상, 6미만, right=False\n",
    "- pd.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] 데이터 준비\n",
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "df = pd.DataFrame(data, columns=['data'])\n",
    "df['data'].to_list()\n",
    "\n",
    "# [1] data를 사용하여 0~3, 4~6, 7~10 에 대한 binning을 하여 보도록 한다\n",
    "# 범주의 레이블은 ['A', 'B', 'C']를 사용한다\n",
    "\n",
    "# (Min-1, 3], (3, 6], (6, Max] 로 범주를 만들어 result_A 컬럼으로 추가합니다.\n",
    "df['result_A'] = pd.cut(df['data'], [df['data'].min()-1,3,6,10], labels=['A', 'B', 'C'], right=True)\n",
    "# [Min, 3), [3, 6), [6, Max+1) 로 범주를 만들어 result_B 컬럼으로 추가합니다.\n",
    "df['result_B'] = pd.cut(df['data'], [df['data'].min(),3,6,df['data'].max()+1], labels=[0, 1, 2], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] result_A에 대해 LabelEncoding을 하여 result_C를 컬럼으로 추가합니다.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['result_C'] = LabelEncoder().fit_transform(df['result_A'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비율을 사용하여 구간 나누기\n",
    "- pd.qcut(데이터, 구간)\n",
    "- 구간은 0~1 사이의 숫자 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 비율을 사용하여 구간나누기\n",
    "df = pd.DataFrame([0, 1, 2, 3, 10, 11, 12, 13, 20, 30, 40, 50], columns=['data'])\n",
    "pd.qcut(df['data'], [0, 0.25, 0.5, 0.75 ,1])  # 네 구간으로 나눈 것\n",
    "\n",
    "# [4] 다음 데이터를 binning 하여보세요 (bins 컬럼 추가)\n",
    "# 사용구간 : [10, 40), [40, 50), [50, 60), [60, 70), [70, 80), [80, 101)\n",
    "# 사용레이블 : '10-40미만', '40-50미만', '50-60미만', '60-70미만', '70-80미만', '80-100이하'\n",
    "data = [55.6, 83.3, 43.4, 58.1, 31.6, 55.6, 60.7, 64.6,\n",
    "        73.3, 55.6, 64.3, 52.8, 22.7, 46.3, 71.4, 53.8,\n",
    "        64.5, 67.9, 71.4, 80.0, 59.5, 40.5, 77.1, 58.6,\n",
    "        65.4, 52.4, 66.7, 91.3, 41.3, 72.1, 61.9, 78.4,\n",
    "        63.6, 41.0, 65.2, 81.3, 54.8, 19.6, 50.0, 53.1,\n",
    "        41.2, 56.5]\n",
    "df = pd.DataFrame(data, columns=['data'])\n",
    "df['bins'] = pd.cut(df['data'], [10, 40, 50, 60, 70, 80, 101],\n",
    "       labels = ['10-40미만', '40-50미만', '50-60미만', '60-70미만', '70-80미만', '80-100이하'], \n",
    "       right=False)\n",
    "# [5] bins 컬럼에 대해 LabelEncoding을 하여 bin_n 컬럼을 추가합니다.\n",
    "df['bins'] = df['bins'].cat.codes\n",
    "\n",
    "# [6] 다음 나이 데이터를 binning 하여보세요\n",
    "bin_labels = 'Baby Child Teenager Student Young_Adult Adult Elderly'.split()   # 7개 label\n",
    "data = [42, 11, 40, 16, 35, 58, 1, 13, 22, 7, 62, 11, 52, 67, 42, 33, 15, 60, 36, 36]\n",
    "df = pd.DataFrame(data, columns=['age'])\n",
    "\n",
    "# (-1, 5], (5, 12], (12, 18], (18, 25], (25, 35], (35, 60], (60, df['age'].max()] 로\n",
    "# 범주를 만들어 age_cat 컬럼으로 추가합니다.\n",
    "df['age_cat'] = pd.cut(df['age'], [-1, 5, 12, 18, 25, 35, 60, df['age'].max()], \n",
    "                       labels = bin_labels,\n",
    "                       right = True)\n",
    "df['age_LE'] = df['age_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델링"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 분류"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 분류 평가방법\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정확도(accuracy) : \n",
    "- 정밀도(precision) : \n",
    "- 재현율(recall) : \n",
    "- 특이도(Specificity) : \n",
    "- f1_score : \n",
    "- ROC(Receiver Operating Characteristic) 곡선 :  \n",
    "- AUC(Are Under Curve) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 회귀 평가방법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SSE(Sum Squared Error) : \n",
    "- MSE(Mean Squared Error) : \n",
    "- RMSE(Root Mean Squared Error) :\n",
    "- MAE(Mean Absolute Error) : \n",
    "- 결정계수(R2) :\n",
    "- Adjusted R2 : \n",
    "- MSPE(Mean Squared Percentage Error) : \n",
    "- MAPE(Mean Absolute Percentage Error) :\n",
    "- RMSLE(Root Mean Squared Logarithmic Error) : \n",
    "- AIC(Akaike Information Criterion) : \n",
    "- BIC(Bayes Information Criteria) : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형3 : 통계 검증 2문제 * 15 [30점]\n",
    "- scipy 모듈을 활용한 통계적 유의성 검정에 대한 능력 확인\n",
    "- 주어진 데이터를 활용해 가설검정 진행 후, 통계량, p-value, 채택/기각 등의 결과 출력\n",
    "- 소수점 아래 자릿수가 정해진 실수로 답을 출력"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 확률 분포"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률 현상 : 나오는 결과의 범위는 알지만, 가능한 결과들 중 정확히 어떤 결과가 나올지 모르는 현상\n",
    "확률 변수 : 확률 현상에 기인해 결과 값이 확률적으로 정해지는 변수    \n",
    "확률 분포 : 확률 변수가 특정한 값을 가질 확률을 나타내는 분포\n",
    "\n",
    "확률 질량 함수 (PMF) : 이산 확률 변수에서 특정 값에 대한 확를을 계산하기 위한 함수 (이산형에서 사용)  \n",
    "확률 밀도 함수 (PDF) : 연속 확률 변수에서 특정 구간에 속할 확률을 계산하기 위한 함수 (연속형에서 사용)   \n",
    "누적 분포 함수 (CDF) : 어떤 확률 분포에 대해 확률 변수가 특정 값보다 작거나 같을 확률을 계산하기 위한 함수 (이산형과 연속형에서 사용)  \n",
    "(PPF) : CDF 반대, 확률 값을 주면 그 때의 x값을 줌\n",
    "\n",
    "### 1) 이산형 확률분포 : 확률변수가 몇 개의 한정된 가능한 값을 가지는 분포 (각 사건은 서로 독립이어야 함)  \n",
    "#### (1)베르누이 분포 : 확률 변수의 가능한 값이 두 개뿐인 이산 확률 분포, 이러한 실험 1회 시행이 베르누이 시행이라 함\n",
    "#### (2) 이항 분포 : 연속된 n번의 독립적 시행에서 각 시행이 확률 p를 가질 때의 이산 확률 분포 (n이 1일 때 이항 분포는 베르누이 분포와 같다)\n",
    "#### (3) 기하 분포 : 성공/실패 로 구성된 시행을 연달아 수행시 처음 성공할 때 까지 시도한 회수 k에 대한 \n",
    "#### (4) 초기하 분포 : \n",
    "#### (5) 포아송 분포 : \n",
    "\n",
    "### 2) 연속형 확률분포 : 확률변수의 가능한 값이 무한 개 이며 사실상 셀 수 없는 경우의 분포\n",
    "#### (1) 정규 분포(가우스 분포) : \n",
    "#### (2) 표준 정규 분포 (Z-분포)\n",
    "#### (3) T-분포 : \n",
    "#### (4) 카이제곱 분포(x제곱-분포) :\n",
    "#### (5) F-분포 \n",
    "#### (6) 지수 분포 : \n",
    "#### (7) 감마 분포 : \n",
    "\n",
    "\n",
    "### 3) 신뢰구간\n",
    "#### (1) 신뢰수준 : 추정값이 존재하는 구간에 모수가 포함되어 있을 가능성의 크기 또는 정확도\n",
    "#### (2) 신뢰구간 : 신뢰수준을 기준으로 추정된 통계적으로 유의미한 모수의 범위  \n",
    "- 표본평균 - (z * SE) ~ 표본평균 + (z * SE)  \n",
    "    - SE ; 표준오차"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (중요!) 2. 가설 검정\n",
    "귀무 가설(H0) : 현재까지 주장되어 온 것이나 기존과 비교하여 차이가 없음을 나타내는 가설  \n",
    "대립 가설(H1) : 표본을 통해 확실한 근거를 가지고 입증하고자 하는 가설(연구 가설)  \n",
    "  \n",
    "p-value : 귀무 가설이 참이라는 가정하에 관찰된 결과가 일어날 확률 값   \n",
    "- p-값 0.05 이상 : 귀무가설 채택, 대립가설 기각\n",
    "- p-값 0.05 미만 : 귀무가설 기각, 대립가설 채택\n",
    "  \n",
    "가설 검정 오류  \n",
    "- 제 1종 오류 : 실제(귀무가설 사실) , 판단(귀무가설 기각)\n",
    "- 제 2종 오류 : 실제(귀무가설 거짓) , 판단(귀무가설 채택)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (중요!) 3. 모수 검정 -> 그룹 나누는 법을 알아야 한다\n",
    "표집분포의 모수를 알고 있다고 가정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 정규성 검정 : 정규분포인지 검증\n",
    "- 귀무가설 : 데이터셋이 정규분포를 따른다\n",
    "- 대립가설 : 데이터셋이 정규분포르 따르지 않는다.\n",
    "- 유의 수준 0.05인 경우, p-value >= 0.05 정규성이 보장된다고 할 수 있다.\n",
    "- 검정통계량 < 임계값 인 경우, 정규성이 보장된다고 할 수 있다. (검정 통계량이 임계값 이하? 미만? 인 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   stime20s  20 non-null     int64\n",
      " 1   stime40s  20 non-null     int64\n",
      " 2   ID        20 non-null     int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 608.0 bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./eduatoz/04. 통계적 검정/data_02/sleepage.csv')\n",
    "df.info()\n",
    "\n",
    "# 그룹을 나누어 저장함\n",
    "gA = df['stime20s']\n",
    "gB = df['stime40s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statisticA:0.9777, pvalueA0.4595\n",
      "statisticB:0.9778, pvalueB0.4647\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 1 shapiro wilks test # 표본 5000개 미만 \n",
    "from scipy.stats import shapiro  \n",
    "statisticA, pvalueA = shapiro(gA)\n",
    "statisticB, pvalueB = shapiro(gB)\n",
    "\n",
    "# f-string으로 자리수 맞춰서 출력하기\n",
    "print(f'statisticA:{statisticA:.4f}, pvalueA{pvalueA:.4f}') # 정규성 만족\n",
    "print(f'statisticB:{statisticB:.4f}, pvalueB{pvalueB:.4f}') # 정규성 만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.9986501019683699, pvalue=8.073266280952718e-58)\n",
      "KstestResult(statistic=0.9999997133484281, pvalue=2.8061866176047734e-131)\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 2 kstest(kolmogorov-smirnov test) # 표본수 2000 초과 // 정규성 검정에만 쓰는 건 아님\n",
    "from scipy.stats import kstest # 두 개의 데이터가 같은 분포인지 아닌지 검정 \n",
    "# 귀무가설 : 두집단의 분포가 같다\n",
    "# 대립가설 : 두집단의 분포가 다르다\n",
    "\n",
    "print(kstest(gA, \"norm\")) # 비교대상으로 \"norm\"으로 정규분포 데이터를 준다 / 정규성 불만족\n",
    "print(kstest(gB, \"norm\")) # 정규성 불만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormaltestResult(statistic=1.173158148726697, pvalue=0.5562268444276741)\n",
      "NormaltestResult(statistic=2.045029526680344, pvalue=0.3596892684891455)\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 3 normaltest  # 표본 20개 이상\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "print(normaltest(gA))  # 정규성 만족\n",
    "print(normaltest(gB))  # 정규성 만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그룹A: 0.4079859754958619 [0.538 0.613 0.736 0.858 1.021] [15.  10.   5.   2.5  1. ]\n",
      "그룹B: 0.360841172628966 [0.538 0.613 0.736 0.858 1.021] [15.  10.   5.   2.5  1. ]\n"
     ]
    }
   ],
   "source": [
    "# 정규성 검정 - 4 anderson darling test # 검정통계량, 임계값 출력 (검정통계량 < 임계값이면 귀무가설 채택) // 정규성 검정에만 쓰는 건 아님\n",
    "# 검정통계량 < 임계값, 정규성이 보장된다고 할 수 있다\n",
    "from scipy.stats import anderson\n",
    "rA = anderson(gA, dist=\"norm\") # static(검정통계량), critical_values(임계값), significance_level(유의수준)\n",
    "rB = anderson(gB, dist=\"norm\")\n",
    "\n",
    "print('그룹A:', *rA)\n",
    "print('그룹B:', *rB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 등분산성 검정 : 분산이 동일한지 검증\n",
    "* 귀무가설 : 데이터셋이 등분산성을 충족한다.\n",
    "* 대립가설 : 데이터셋이 등분산성을 충족하지 않는다.\n",
    "\n",
    "- burtlett : 정규성 충족하는, 데이터셋의 크기가 서로 다른 2개 이상의 집단 사용 가능\n",
    "- levene, fligner : 정규성을 충족하지 않는, 비모수 데이터에 대해서도 사용가능 (중앙을 median으로 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bartlett, levene, fligner\n",
    "bartlett()\n",
    "levene() # center={'median'(정규분포가아닌경우사용), 'mean'(좌우대칭일때사용), 'trimmed'(양 끝단에 데이터가 헤비하게 있을 때 사용)}, proportiontocut=0.05 center='trimmed'일 경우 사용\n",
    "fligner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "# 파일 읽어와 내용 확인\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "print(iris.head(2))\n",
    "\n",
    "# [1] target='sepal_length', 품종별 그룹을 나누어 저장함\n",
    "import pandas as pd\n",
    "target = 'sepal_length'\n",
    "iris['species'].unique()\n",
    "gA = iris.loc[iris['species']=='setosa', target]\n",
    "gB = iris.loc[iris['species']=='versicolor', target]\n",
    "gC = iris.loc[iris['species']=='virginica', target]\n",
    "\n",
    "# [2] burtlett 검정\n",
    "from scipy.stats import bartlett, shapiro\n",
    "# 정규성검정\n",
    "_, pvalueA = shapiro(gA)\n",
    "_, pvalueB = shapiro(gB)\n",
    "_, pvalueC = shapiro(gC)\n",
    "print(pvalueA, pvalueB, pvalueC)    # p-value가 0.05 이상 귀무가설 채택 - 정규성 만족\n",
    "# 등분산성 검정\n",
    "static, pvalue = bartlett(gA, gB, gC)\n",
    "print(static, pvalue)   # p-value가 0.05 미만 - 귀무가설 기각 - 등분산성 불만족\n",
    "\n",
    "# [3] levene 의 center는 'mean'으로 지정\n",
    "from scipy.stats import levene\n",
    "print(levene(gA, gB, gC, center='median')) # p-value가 0.05 미만 - 귀무가설 기각 - 등분산성 불만족\n",
    "\n",
    "# [4] fligner의 center는 'trimmed', proportiontocut=5% 지정\n",
    "from scipy.stats import fligner\n",
    "print(fligner(gA, gB, gC, center='trimmed'))    # p-value가 0.05 미만 - 귀무가설 기각 - 등분산성 불만족"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) T-검정\n",
    "- (1) One sample T test : 1표본 평균 검정 : ttest_1samp(a, popmean, alternative='') 이다아니다 = 'two-sided', 작다='less', 크다='greater'\n",
    "- (2) Paried T test : 쌍체(대응) 표본 t-검정 : ttest_rel(a, b, alternative='two-sided)\n",
    "- (3) Two sample T test : 2표본 평균 검정 : ttest_ind(a, b, alternative='two-sided', equal_var=True/False) 등분산아닌 경우 equal_var=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) ANOVA\n",
    "- 일원분산분석(One-Way ANOVA)범주형 독립변수가 한 개인 경우 사용 : f_oneway(*data)\n",
    "- 이원분산분석(Two-Way ANOVA)주형 독립변수가 두 개인 경우 사용(K-Way ANOVA; 범주형 변수가 K개인 경우 사용) <- 나올확률 낮은 듯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (중요!) 4. 비모수 검정 -> 교차표 만드는 법 알아야함\n",
    "표집분포의 모수를 알지 못한다고 가정\n",
    "### 1) 카이검정\n",
    "- 적합도 검정 : \n",
    "- 동질성 검정 : \n",
    "- 독립성 검정 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 검증하기  \n",
    "1) t-검정  \n",
    "2) 정규성 검정  \n",
    "3) One-way ANOVA  \n",
    "4) Two-way ANOVA  \n",
    "5) 카이제곱 검정  \n",
    "(1) 독립성 검정  \n",
    "(2) 동질성 검정  \n",
    "(3) 적합성 검정  \n",
    "6) 순위 검정  \n",
    "(1) 윌콕슨 순위합 검정  \n",
    "(2) 크루스칼 - 왈리스 검정  \n",
    "07) 부호검정  \n",
    "(1) 부호검정  \n",
    "(2) 맥네마 검정  \n",
    "8) 다변량 추론  \n",
    "(1) 다중 회귀 분석  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
