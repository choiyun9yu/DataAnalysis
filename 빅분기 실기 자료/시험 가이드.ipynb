{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형1 : 데이터 전처리 3문제 * 10 [30점]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소수점 아래 2째자리까지 표시되도록 설정하기\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일 불러오기 \n",
    "df = pd.read_csv('파일경로.csv')\n",
    "## 불러오면서 인덱스 컬럼 설정\n",
    "df_csv1 = pd.read_csv('파일경로.csv', index_col='인덱스로 삼을 칼럼명')\n",
    "## 불러오면서 구분자 지정\n",
    "df_csv2 = pd.read_csv('파일경로.csv', sep='\\t')\n",
    "## 불러올 때 언어 설정\n",
    "df_csv3 = pd.read_csv('파일경로.csv', encoding='euc-kr')  # 한글지원 : euc-kr, cp949\n",
    "## 불러올 때 결측치 필터 사용 끄기\n",
    "df_csv2 = pd.read_csv('파일경로.csv', na_filter=False)\n",
    "\n",
    "\n",
    "# excel 파일 불러오기 (엑셀은 엔진 설정해주기!!)\n",
    "df_excel = pd.read_excel('파일경로.xlsx', engine='openpyxl')\n",
    "\n",
    "# csv 파일 저장하기\n",
    "df.to_csv('경로.csv', index=False)\n",
    "# excel 파일 저장하기\n",
    "df.to_excel('경로.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 구조 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이타가 많은 경우 모두 출력 안되고 ... 으로 생략해서 출력됩니다.\n",
    "# 생략되지 않는 행, 열의 개수를 설정하여 생략되지 않고 출력되도록 합니다.\n",
    "pd.set_option('display.max_rows', 800)    #출력할 max row를 지정\n",
    "pd.set_option('display.max_columns', 100)  #출력할 max columns를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 n개 행의 데이터 확인\n",
    "df.head()\n",
    "# 끝부터 n개 행의 데이터 확인\n",
    "df.tail()\n",
    "\n",
    "# 데이터프레임 row개수, colum개수, Not null,dtype 등 정보 확인\n",
    "df.info(memory_usage='deep')\n",
    "# 데이터프레임 통계 정보 확인\n",
    "df.describe()\n",
    "\n",
    "# 데이터프레임의 행, 열의 수를 (행, 열)형태의 튜블로 반환\n",
    "df.shape\n",
    "# 데이터프레임의 (행 x 열)의 전체 데이터 수를 반환\n",
    "df.size\n",
    "# 데이터프레임 데이터 타입 확인\n",
    "df.dtypes\n",
    "\n",
    "# 시리즈\n",
    "sri = df['특정 컬럼']\n",
    "# 시리즈 데이터 타입 확인\n",
    "sri.dtypes\n",
    "\n",
    "#데이터프레임의 인덱스 확인 (보통 0으로 시작해서 몇으로 끝나고 스텝이 몇인지 등을 보여줌)\n",
    "df.index\n",
    "# 데이터프레임의 컬럼 확인\n",
    "df.columns          # 타입 index\n",
    "df.columns.values   # 타입 array\n",
    "# 데이터프레임의 구성요소 2차원으로 보기\n",
    "df.values           # 타입 array\n",
    "\n",
    "# 시리즈의 구성요소\n",
    "sri.values          # 타입 array 해당 시리즈 값 배열로 쭉 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 중 컬럼에 존재하는 값 종류만 확인(결측치 포함, ndrarray로 반환)\n",
    "sri.unique()       \n",
    "# 시리즈 중 컬럼에 존재하는 값 종류의 개수만 확인\n",
    "sri.nunique()\n",
    "# 시리즈 중 컬럼에 대해 값 별 개수 확인(결측치 미포함, Series 반환)\n",
    "sri.value_counts() \n",
    "\n",
    "# 해당 칼럼의 결측치 개수 까지 같이 보고 싶은 경우\n",
    "sri.value_counts(dropna=False)\n",
    "# 해당 값들의 비율을 보고싶은 경우(결측치 비율까지 같이 나옴)\n",
    "sri.value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 타입 변경\n",
    "데이터 타입 변경 전 데이터 조작이 필요할 수 있음 (예) 불필요 문자/콤마/공백 제거, 단위 변환 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 데이터 타입 변경 ('int', 'int32', 'int64', 'float', 'str', 'category' 등)\n",
    "## 넘파이 타입으로 하려면 넘파이 임포트 하여야함 (np.int16, np.float32, np.datetime64 등)\n",
    "sri.astype('타입')\n",
    "\n",
    "# 원하는 자료형으로 바꾸지 못하는 장애요소 제거하고 변환\n",
    "## replace 사용 시 regex=True 옵션을 사용하면 일부 내용만 변경대상으로 지정할 수 있음\n",
    "## 여기서 regex는 '정규표현식'을 의미\n",
    "sri.replace('변경전', '변경후', regex=True)\n",
    "# 변경할 내용이 두가지 이상일 때 list나 dic을 사용하여 변환\n",
    "list1 = ['변경전1', '변경전2']\n",
    "list2 = ['변경후1', '변경후2']\n",
    "sri.replace(list1, list2, regex=True)\n",
    "dic = {'변경전1':'변경후1', '변경전2':'변경후2'}\n",
    "sri.replace(dic, regex=True).astype('int64')\n",
    "\n",
    "## ,콤마는 메타 문자이므로 제거하고자 하는 경우 \\, 처럼 역슬레시 사용\n",
    "## 메타문자 종류 , . + ? * ^ $ 등\n",
    "sri.replace(['\\메타문자', '변견전2'], '', regex=True)  # 변경후 모습이 같다면 하나만 적어도 됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series는 Accessor라는 것을 가지고 있다. Accessor를 사용하기 위해서라도 데이터 타입 변경 필요\n",
    "- dt : Datetime, Timedelta, Period\n",
    "- str : String\n",
    "- cat : Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessor 사용 .str accessor 사용하면 각 행에 문자열 처럼 접근 \n",
    "sri.str[1:-1].astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 np.datetime64 나 category 타입 변경은 astype으로 변경 가능, 특수한 경우는 아래 방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime\n",
    "import numpy as np\n",
    "sri.astype(np.datetime64)  # 일/월/년 순으로 보기도 함 이런 경우 to_datatime을 사용\n",
    "# 내가 원하는 방식으로 날짜를 읽어오고 싶을 때\n",
    "## %Y: 4글자 년도, %y : 2글자 년도, %m : 2글자 월, %d : 2글자 일\n",
    "pd.to_datetime(sri, format='%y-%m-%d')\n",
    "\n",
    "# category\n",
    "sri.astype('category')  # 이렇게 변환한 자료에 sort_values 적용하면 가나다 순 정렬됨 \n",
    "# 카테고리 순서를 직접 지정하고 싶을 때\n",
    "temp = pd.Categorical(sri, categories=[\"범주1\", \"범주2\"], ordered=True)  # 이건 지금 시리즈는 아님\n",
    "sri = temp  # 이건 시리즈\n",
    "sri.sort_values() # 원하는 순서대로 정렬 할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_index() : 인덱스 정렬\n",
    "# [3-24] '측정일시'를 index로 설정하고,\n",
    "# index 기준으로 오름차순 정렬해서 df1으로 이름 붙입니다.\n",
    "# 그래프에서 y축으로 사용하려고 합니다.\n",
    "df1 = df.set_index('측정일시').sort_index()\n",
    "\n",
    "# sort_values() : 밸류 정렬, 디폴트(내림차순)\n",
    "df.sort_values('정렬기준칼럼', ascending=False)\n",
    "# 오름차순인 경우\n",
    "df.sort_values('정렬기준칼럼', ascending=True)\n",
    "# 정렬 기준이 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[False, False])  \n",
    "# 정렬 기준이 여러개, 내림차순 오름차순도 여러개인 경우\n",
    "df.sort_values(['1차기준', '2차기준'], ascending=[True, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 index, columns 및 Series의 index 는 대입연산을 사용하여 변경 가능 다만, 개수가 동일해야 함  \n",
    "  \n",
    "value는 인덱스를 이용해서 변경 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 개수 확인 -> 컬럼 이름 리스트를 대입 연산자로 넣기(단, 개수 동일하게)\n",
    "df.columns = ['컬럼1', '컬럼2', '컬럼3']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼이름 변경하기\n",
    "- DataFrame.rename(columns={'변경전이름':'변경후이름', ...})\n",
    "- DataFrame.rename({'변경전이름':'변경후이름', ...}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-17] bread의 '상세영업상태코드'라는 컬럼명을 '상태코드'로 변경한 뒤,\n",
    "# 첫 2개의 행을 확인합니다.\n",
    "bread = bread.rename(columns={'상세영업상태코드':'상태코드'})\n",
    "bread.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Series.argmax() : 가장 값이 큰 것의 integer index 구하기\n",
    "- Series.argmin() : 가장 값이 작은 것의 integer index 구하기\n",
    "- Series[Series.argmax()] : 가장 큰 값 구하기\n",
    "- Series[Series.argmin()] : 가장 작은 값 구하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인덱싱 할 때 레이블을 쓰면 ['전':'후'] 에서 후 까지 포함  \n",
    "레이블이 아닌 숫자를 쓰면 [1:9] 뒤에 9 미포함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 통계 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 통계 함수\n",
    "- axis=0 : 기본 값으로 행을 이동하면서, 행과 행의 연산을 수행한다.(수직으로 연산)\n",
    "- axis=1 : 컬럼을 이동하며 컬럼과 컬럼의 연산을 수행한다.(수평으로 연산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 통계량 확인\n",
    "df.describe()\n",
    "\n",
    "# 개수 세기\n",
    "df.count()\n",
    "# 합계\n",
    "df.sum()\n",
    "# 평균\n",
    "df.mean()\n",
    "# 표준편차 \n",
    "df.std()\n",
    "# 분산\n",
    "df.var()\n",
    "# 중앙값\n",
    "df.median()\n",
    "# 최빈값\n",
    "df.mode()  # 시리즈로 나옴\n",
    "# 최빈값 1개\n",
    "df.mode()[0]\n",
    "# 최대값\n",
    "df.max()   # 최대값, 최소값은 문자열에 대해서도 반응하는데 이는 ord(문자) 코드 숫자 기반\n",
    "# 최소값\n",
    "df.min()\n",
    "\n",
    "# 분위수\n",
    "df.quantile([0.25, 0.5, 0.75])    # 1사분위 0.25, 2사분위 0.5, 3사분위 0.75, \n",
    "# IQR\n",
    "df.quantile(0.75) - sri.quantile(0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통계 함수 여러개 적용하고 싶을 때 1\n",
    "df.apply([\"min\", \"max\", \"mean\"])    # apply 사용하면 어떤 함수든 적용 가능\n",
    "# 통계 함수 여러개 적용하고 싶을 때 2\n",
    "df.agg([\"min\", \"max\", \"mean\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 그룹별 통계\n",
    "- df.groupby(그룹명).통계함수() : 적용가능한 모든 단위\n",
    "- df.groupby(그룹명)[칼럼].통계함수 : Series 단위\n",
    "- df.groupby(그룹명)[[[칼럼1, 칼럼2 ... ]].통계함수 : 특정 컬럼 단위  \n",
    "- 그룹별로 통계치 구할 땐 agg(['var', 'std', 'mode'])로 여러개 가능\n",
    "\n",
    "* 통계 함수는 numerical 값에 적용되기 때문에 object는 형변환이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-54] 대륙별 주류 소비량 중앙값을 계산해 봅니다.\n",
    "df.groupby('대륙').median()\n",
    "\n",
    "# [2-55] 대륙별 맥주 소비량 평균은?\n",
    "df.groupby('대륙')['맥주'].mean()\n",
    "\n",
    "# [2-56] 전세계 맥주 소비량 평균보다 많은 맥주를 소비하는 대륙은?\n",
    "temp = df.groupby('대륙')[['맥주']].mean()\n",
    "temp[temp['맥주']> df['맥주'].mean()]\n",
    "\n",
    "# 그룹바이 기준이 두개 이상인 경우 멀티 인덱스로 작업\n",
    "# [3-34] df_dust에서 '년', '월'별 '미세먼지(㎍/㎥)' 데이터의 평균을 구해\n",
    "# DataFrame으로 만들어 meandf 라는 이름을 지정합니다.ㅣ\n",
    "meandf = df_dust.groupby(['년', '월'])[['미세먼지(㎍/㎥)']].mean()\n",
    "meandf.head()\n",
    "\n",
    "# [3-35] meandf에서 2017년 6월까지의 데이터만 출력합니다.  \n",
    "meandf.loc[:(2017,6),:] # 멀티인덱스 인경우 튜플로 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3-39] df_dust의 일자(년, 월, 일)별 '미세먼지(㎍/㎥)'의 평균을 구합니다.\n",
    "# 인덱스가 유지되지 않음\n",
    "df_dust.groupby(['측정일시'])['미세먼지(㎍/㎥)'].mean()  # <- 동일한 값을 갖음.  MultiIndex 아님\n",
    "# 인덱스가 유지되면서 그룹별 함수 적용\n",
    "df_dust.groupby(['측정일시'])['미세먼지(㎍/㎥)'].transform('mean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 피벗 테이블 : 행, 열 모두에 그룹으 지정하여 통계값 구하기\n",
    "- df.pivo_table(index=행방향칼럼, columns=열방향칼럼, values=집계대상칼럼, aggfunc=통계함수)\n",
    "- index, columns는 범주형, values는 연속형 사용\n",
    "\n",
    "* df.pivot_table(index=행방향그룹열이름, columns=열방향그룹열이름, values=집계대상열이름, aggfunc=통계함수)\n",
    "* index, columns는 범주형, values는 연속형 사용\n",
    "* values, aggfunc의 경우 단독의 경우 출력에 표시되지 않으나, 목록은 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-58] pivot_table을 사용하여 대륙별(index), '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.pivot_table(index='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-59] pivot_table을 사용하여 대륙별(columns), '맥주'와 '와인'의 mean, median값을 구합니다.\n",
    "df.pivot_table(columns='대륙', values=['맥주', '와인'], aggfunc=['mean', 'median', 'max'])\n",
    "\n",
    "# [2-60] groupby를 사용하여 대륙별, '맥주'와 '와인'의 mean, median, max 값을 구합니다.\n",
    "df.groupby('대륙')['맥주', '와인'].agg(['mean', 'median', 'max'])\n",
    "# 이건 [2-58] 과 비슷하지만 피벗테이블을 쓰느냐 그룹바이를 쓰느냐에 따라 데이터프레임 구조가 미묘하게 다르다.\n",
    "\n",
    "# [3-48] df_dust의 월/년 별 미세먼지의 'mean', 'min', 'max' 구하기\n",
    "# pivot_table 사용, values의 경우 목록으로 지정시와 단독 지정시가 다르게 표시됨\n",
    "df_dust.pivot_table(index='월', columns='년', values='미세먼지(㎍/㎥)', aggfunc=['mean', 'min', 'max'])\n",
    "\n",
    "# [3-49] df_dust에서 '측정소명'이 '강남구'인 데이터의\n",
    "# 월별(index), 년별(columns), 미세먼지 농도 평균을 조회하여 temp로 저장합니다\n",
    "temp = df_dust.loc[df['측정소명']=='강남구'].pivot_table(index='월', columns='년', values=['미세먼지(㎍/㎥)'], aggfunc=['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3-50] 2016년 ~ 2020년도 미세먼지 농도가 가장 높은 월의 위치\n",
    "temp = df_dust.pivot_table(index='월', columns='년', values='미세먼지(㎍/㎥)', aggfunc='mean')\n",
    "for year in temp.columns:\n",
    "    idx=temp[year].argmax()\n",
    "    print(temp.index[idx])\n",
    "\n",
    "# [3-52] 2016년 ~ 2019년 월별 미세먼지 평균을 구해 temp (DataFrame)로 저장합니다.\n",
    "temp = df_dust.loc[df_dust['년']<=2019].groupby('월')[['미세먼지(㎍/㎥)']].mean()\n",
    "\n",
    "# [3-52] 2016년 ~ 2019년 월별 미세먼지 평균을 구해 temp (DataFrame)로 저장합니다.\n",
    "temp = df_dust.loc[df_dust['년']<=2019].groupby('월')[['미세먼지(㎍/㎥)']].mean()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Index, Columns 상호변경"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Columns to Index : df.set_index(['인덱스로 이동시킬 컬럼 명'..])\n",
    "- Index to Columns : df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-32] 국가별 주류 소비량 합계(맥주, 증류주, 와인의 합)를 구해 봅니다.\n",
    "df.set_index('국가')[['맥주', '증류주', '와인']].sum(axis=1)\n",
    "\n",
    "# [2-33] df를 ['대륙', '국가']를 index로 지정하고, 대륙별, 국가명으로  정렬하여 df로 저장합니다.\n",
    "df = df.set_index(['대륙','국가']).sort_index()\n",
    "\n",
    "# [2-34] df의 index를 모두 columns로 이동합니다.\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 정제하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 결측치\n",
    "- nan : 넘파이 배열에서 결측치 나타내는 경우\n",
    "- NaN : 판다스에서 시리즈나 데이터프레임에서 결측치 나타내는 경우 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 찾기\n",
    "df.isna()\n",
    "df.isnull()\n",
    "# 결측치 합계 구하기\n",
    "df.isna().sum()\n",
    "\n",
    "# 결측치가 아닌 것 찾기\n",
    "df.notna()\n",
    "df.notnull()\n",
    "\n",
    "# 불리언 인덱싱으로 결측치만 찾기\n",
    "df[df['컬럼'].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 범주형 데이터 : 다른 범주로 만들어 채우기\n",
    "- 연속형 데이터 : 0으로 채우기, 평균값으로 채우기, 범주별 평균값으로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임의 결측치 채우기 (모두 같은 값으로)\n",
    "df = df.fillna('대체값')\n",
    "\n",
    "# 특정한 하나의 컬럼 결측치 채우기\n",
    "df['칼럼'] = df['칼럼'].fillna('대체값')\n",
    "df.loc[df['칼럼'].isna(), '대륙'] = '대체값'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "- 결측치 제거에 사용되는 메서드\n",
    "- how='any' : 결측치가 하나라도 포함된 행 삭제\n",
    "- how='all' : 모든 데이터가 결측치인 행 삭제\n",
    "- axis=1 : 컬럼에 대해 동작\n",
    "- thresh=숫자 : 숫자 이상의 데이터를 가진 행은 삭제 안함\n",
    "- subset=[컬럼이름1, ...] : subset으로 지정된 컬럼만 사용하여 삭제 대상 검색"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "df.dropna()\n",
    "\n",
    "# [3-30] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 모든 데이터가 결측치인 행을 제거하여 결과를 temp1으로 저장합니다\n",
    "temp1 = df_dust.dropna(how='all', axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])\n",
    "\n",
    "# [3-32] df_dust 에서 ['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)']에서\n",
    "# 2개 이상의 데이터를 가진 행은 제거하지 않은 결과를 temp3로 저장합니다.\n",
    "# (= 3개의 정보 중 1개의 데이터만 가진 행을 제거함)\n",
    "temp3 = df_dust.dropna(thresh=2, axis=0, subset=['오존농도(ppm)','미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 결측치 대체\n",
    "Series.mask(조건, 조건이 참일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 True인 것에 대해 다른 값을 변경합니다.\n",
    "- sri.isna() : NA값에 대해 True, NA아닌 것은 False\n",
    "\n",
    "Series.where(조건, 조건이 거짓일 때 사용할 값 또는 값 목록)\n",
    "- 조건이 False인 것에 대해서 다른 값으로 변경합니다.\n",
    "- sri.notna() : NA값에 대해 False, NA아닌 것은 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp의 'A' 열에 대해서 결측치인 경우 'B'의 값으로 대체합니다.\n",
    "temp['A'].mask(temp['A'].isna(), temp['B'])\n",
    "# temp의 'A' 열에 대해서 결측치인 경우 'C'의 값으로 대체합니다.\n",
    "temp['A'].where(temp['A'].notna(), temp['C'])\n",
    "\n",
    "# [3-43] meandf에 '결측치대체' 및 '차이2'라는 컬럼을 추가합니다.\n",
    "# '결측치대체' 컬럼은 df_dust에서 '년', '월'별 '미세먼지(㎍/㎥)' 데이터의 평균을 사용합니다.\n",
    "# '차이2' 컬럼은 '미세먼지(㎍/㎥)' - '결측치대체'를 사용합니다.\n",
    "meandf['결측치대체'] = df_dust.groupby(['년','월'])['미세먼지(㎍/㎥)'].mean()\n",
    "\n",
    "# [3-45] df_dust의 '오존농도(ppm)', '초미세먼지(㎍/㎥)' 컬럼에 대해서도\n",
    "# '미세먼지(㎍/㎥)'와 같이 동일한 '년', '월', '일'의 평균 값으로 채우기 합니다.\n",
    "fine_dust = df_dust.groupby('측정일시')['오존농도(ppm)'].transform('mean')\n",
    "s = df_dust['오존농도(ppm)']\n",
    "df_dust['오존농도(ppm)'] = s.mask(s.isna(), fine_dust)\n",
    "\n",
    "fine_dust = df_dust.groupby('측정일시')['초미세먼지(㎍/㎥)'].transform('mean')\n",
    "s = df_dust['초미세먼지(㎍/㎥)']\n",
    "df_dust['초미세먼지(㎍/㎥)'] = s.mask(s.isna(), fine_dust)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 이상치"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 이상치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 이상치 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 파생 변수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 포함된 목록 확인\n",
    "- sri.isin() : \n",
    "- '찾을내용' in str : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].isin(['TV/방송', '게임'])]  # 또는 조건 isin으로도 해결 가능\n",
    "\n",
    "# isin([]) 목록이 한개여도 리스트 타입으로 넣어줘야 함\n",
    "df.loc[df['category'].isin(['음악/댄스/가수'])].sort_values('subscriber', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-26] bread에서 '사업장명' 컬럼을 사용하여\n",
    "# '파리바게트', '파리바게뜨' 이름인 곳을 뽑아 paris로 이름 붙입니다.\n",
    "bread[bread['사업장명'].str.contains('파리바게트', '파리바게뜨')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Accessor 사용\n",
    "Series 타입에 .str을 붙여서 문자열 메소드 쓰는 것\n",
    "- str.contains('문자열') : 특정 문자열을 포함하는지 아닌지를 True/False로 반환 (Boolean Indexing 조건으로 사용 가능)\n",
    "- str.upper() : 영문자 소문자를 대문자로 변경  /  str.lower() : 영문자 대문자를 소문자로 변경  \n",
    "- 세부 내용 : https://pandas.pydata.org/docs/reference/series.html#string-handling\n",
    "\n",
    "Series의 데이터를 list 및 ndarray로 반환 (데이터프레임 말고 시리즈에서 변환하는 것)\n",
    "- Series.to_list() : Series의 values를 list로 반환\n",
    "- Series.to_numpy() : Series의 values를 ndarray로 반환 (Series.values 와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1-44] title에 'KBS'가 포함된 채널 명 목록을 만들어 봅니다.\n",
    "df[df['title'].str.contains('KBS')]\n",
    "# 대소문자 구분 없이 검색 하려면? !str 두번 쓰면 됨\n",
    "df[df['title'].str.lower().str.contains('kbs')]\n",
    "\n",
    "# Series 형태를 list나 array 형태로 변환하기\n",
    "df[df['title'].str.contains('KBS')]['title'].to_list()\n",
    "df[df['title'].str.contains('KBS')]['title'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime에서 년 정보만 가져오기\n",
    "df['측정일시2'].dt.year\n",
    "# datetime에서 월 정보만 가져오기\n",
    "df['측정일시2'].dt.month\n",
    "# datetime에서 일 정보만 가져오기\n",
    "df['측정일시2'].dt.day\n",
    "# 월요일이 0 일요일이 6\n",
    "df['측정일시2'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-8] bread 의 '소재지전체주소' 중 시/도에 대한 정보(목록)를 추출합니다.\n",
    "bread['소재지전체주소'].str.split(' ').str[0]   # str 여러번 붙여 써도 괜찮음\n",
    "\n",
    "# [4-9] bread에서 소재지전체주소의 처음이 '서울특별시'이면서,\n",
    "# '업태구분명'이 '제과점영업'인 것만 추출합니다.\n",
    "bread[(bread['업태구분명']=='제과점영업') & (bread['소재지전체주소'].str.split(' ').str[0]=='서울특별시')]\n",
    "# 이렇게 정리해도 괜찮은 듯\n",
    "condition1 = bread['업태구분명']=='제과점영업'\n",
    "condition2 = bread['소재지전체주소'].str.split(' ').str[0]=='서울특별시'\n",
    "bread = bread[condition1 & condition2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- str.strip('제거할 문자들') : 문자열의 앞/뒤에 불필요한 것을 제거함\n",
    "   - 제거할 문자들을 지정하지 않을 경우 whitespace를 제거함\n",
    "- str.split('구분자')\n",
    "   - 구분자를 지정하지 않을 경우 whitespace를 기준으로 분리함\n",
    "   - 각 구분된 내용은 str[0], str[1], .. 등으로 접근\n",
    "- str.join('구분자')\n",
    "   - 구분자 지정을 생략할 수 없음\n",
    "   - 분리된 문자열을 구분자를 사이에 넣어 하나의 문자열로 만듦\n",
    "- str.replace(전, 후)\n",
    "   - 문자열의 일부 내용을 변경 가능함\n",
    "   - 변경전 내용을 찾아 변경후 내용으로 바꿈   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'A': ['    김   수민 ', '  이  나라     ', '  황   소  라  '],\n",
    "        'B': ['  d2021-10-29.   ', '   \\n\\t\\r2021-10-30c    \\n', '2021-11-01c   '],\n",
    "        'C': ['*7', '6', '*7']}\n",
    "temp = pd.DataFrame(data)\n",
    "print(temp)\n",
    "\n",
    "# [1] 'A' 컬럼의 데이터를 빈칸 없는 이름으로 만들어 temp의 'A'컬럼 뒤에 'A-01'컬럼으로 추가해 보세요.\n",
    "temp.insert(1, column=' A-01', \n",
    "            value=temp['A'].str.split().str.join(''))        \n",
    "\n",
    "# [2] 'B' 컬럼의 데이티를 2021-10-29 처럼 앞/뒤에 공백이나 다른 문자('.dc')가 없도록 만들어\n",
    "# temp에 'B-01' 컬럼으로 추가해 보세요.\n",
    "temp.insert(3, column='B-01', \n",
    "            value=temp['B'].str.strip().str.strip('d.c'))\n",
    "\n",
    "# [3] 'B-01' 컬럼의 데이터에서 '-'를 '/'로 수정해 temp에 'B-02' 컬럼으로 추가해 보세요.\n",
    "temp.insert(4, column='B-02',\n",
    "            value=temp['B-01'].str.replace('-', '/'))\n",
    "\n",
    "# [4] 'C' 컬럼에서 *을 제거하고 숫자로 변경해 'C-01'컬럼으로 추가해 보세요.\n",
    "temp['C-01'] = temp['C'].str.replace('*','').astype(int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 불리언 인덱싱\n",
    "- df.loc[조건] : 조건은 boolean dtype이어야함 (기호 : | 선언, & 연언, ~ 부정) !!참 거짓으로 나올 수 있어야함\n",
    "- df.iloc[행범위, 열범위] : 인덱스 번호로 인덱싱 혹은 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예시 1-36] 'category'가 '음악/댄스/가수'인 채널의 subscriber TOP5를 알아봅니다.\n",
    "df.loc[df['category']=='음악/댄스/가수'].sort_values('subscriber').head()\n",
    "\n",
    "# [2-23] 와인 소비량이 맥주 + 증류주 소비량보다 큰 나라를 검색해,'대륙'을 기준으로 정렬해 보자\n",
    "df.loc[(df['와인']) > (df['맥주']+df['증류주'])].sort_values('대륙')\n",
    "\n",
    "# [2-24] 맥주 소비량이 230 초과이면서, 와인 소비량이 230 초과인 나라를 검색해 보자\n",
    "df.loc[(df['맥주']>230) & (df['와인']>230)]['국가']\n",
    "\n",
    "# [2-25] 대륙이 'AS'인 국가들의 정보를 검색해 보자\n",
    "df.loc[df['대륙']=='AS'].info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참 거짓으로 나오지 않는 사칙연산 같은 건 loc 쓰지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-27] 주류소비량2 = ['맥주', '증류주', '와인']에 대해 DataFrame.sum(axis=1) 함수 사용\n",
    "df['주류소비량2'] = df[['맥주', '증류주', '와인']].sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 추가, 제거 및 병합\n",
    "- 행 추가 : df.append(추가할 데이터프레임)\n",
    "- 컬럼 추가1 : df['새로운칼럼명'] = \n",
    "- 특정 위치에 컬럼 추가2 : df.insert(위치, 컬럼, 값) 단, inplace 동작됨\n",
    "- 열 제거1 : df = df.drop('열이름')    \n",
    "- 열 제거2 : df = df.drop(rows=['열이름1', '열이름2' ...])\n",
    "- 컬럼 제거1 : df = df.drop('칼럼명', axis=1)\n",
    "- 컬럼 제거2 : df = df.drop(columns=[컬럼명...])\n",
    "- 컬럼 제거3 : del df['제거할칼럼명'] 단, inplce 동작됨\n",
    "- 데이터프레임 행/열 전환 : df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-26] 국가별 주류 소비량 합계를 구해 새로운 컬럼 ('주류소비량')를 추가합니다\n",
    "# 주류소비량 = '맥주' + '증류주' + '와인'\n",
    "df['주류소비량'] = (df['맥주'])+(df['증류주'])+(df['와인'])\n",
    "\n",
    "# [2-40] 세계의 각 컬럼별 평균을 구하여 DataFrame으로 만들고,\n",
    "# worldwide라는 이름을 지정합니다\n",
    "# 세계의 각 컬럼별 평균은 DataFrame.mean()을 사용합니다.\n",
    "worldwide = pd.DataFrame(df.mean(axis=0))\n",
    "\n",
    "# [2-41] worldwide의 행과 열을 전환해 wwT로 저장합니다.\n",
    "wwT = worldwide.T\n",
    "wwT\n",
    "\n",
    "# [2-42] wwT의 맨 앞에 '국가' 컬럼을 'World Wide' 값으로 추가합니다.\n",
    "# 여러 번 추가하면 안됨\n",
    "wwT.insert(0,'국가','World Wide') # 값을 여러개 줄 땐 리스트로 묶어줘야 한다.\n",
    "wwT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행 병합 : concat([합칠행1, 합칠행2] ..., axis=0)\n",
    "- 열 병합 : concat([합칠열1, 합칠열2] ..., axis=1)\n",
    "* df.append()와 달리 여러 개의 df를 목록을 주어 한 번에 합칠 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2-44] wwT와 korea 를 합쳐 하나의 DataFrame을 생성하여 df2로 저장합니다.\n",
    "df2 = pd.concat([wwT,korea], axis=0)\n",
    "df2\n",
    "\n",
    "# [3-5] df2016, df2017, df2018, df2019를 합쳐 한 개의 DataFrame으로 만들어 df라는 이름을 지정합니다.\n",
    "dfList = [df2016, df2017, df2018, df2019]\n",
    "df = pd.concat(dfList, axis=0)\n",
    "\n",
    "# concat을 하더라도 인덱스 번호는 유지되기 때문에 인덱스번호 다시 설정해줘야함\n",
    "df.index = pd.RangeIndex(len(df))\n",
    "# concat 할때부터 옵션으로 인덱스 잡아주기\n",
    "df = pd.concat(dfList, ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-20] bread에 '설립년도' 및 '폐업년도' 컬럼을 추가합니다.\n",
    "# '인허가일자'//10000, '폐업일자 // 10000 을 사용하여 구합니다.\n",
    "# 두 개의 컬럼이 추가된 bread의 첫 2개 행을 확인합니다.\n",
    "\n",
    "# 정수형인경우\n",
    "year = bread['인허가일자'] // 10000\n",
    "meonth = bread['인허가일자'] // 100 % 100\n",
    "day = bread['인허가일자'] %100\n",
    "\n",
    "# datetime 으로 바꿔서\n",
    "temp = pd.to_datetime(bread['인허가일자'], foramt='%Y,%m%d')\n",
    "temp.dt.year\n",
    "temp.dt.moth\n",
    "temp.dt.day\n",
    "\n",
    "# 문자열인경우 (슬라이싱으로)\n",
    "\n",
    "\n",
    "# 이번에는 정수형으로 함\n",
    "bread['설립년도'] = bread['인허가일자']//10000\n",
    "bread['폐업년도'] = bread['폐업일자']//10000\n",
    "bread.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-21] bread에 '영업기간' 컬럼을 추가합니다\n",
    "# '영업기간'은 '상태코드'가 1(=영업)인 경우 2021 - 설립년도 +1\n",
    "# '상태코드'가 2(=폐업)인 경우 폐업년도 - 설립년도 + 1로 계산합니다.\n",
    "\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "today.year\n",
    "today.month\n",
    "today.day\n",
    "\n",
    "nyear = today.year\n",
    "bread.loc[bread['상태코드']==1,'영업기간'] = nyear - bread['설립년도'] + 1\n",
    "bread.loc[bread['상태코드']==2, '영업기간'] = bread['폐업년도'] - bread['설립년도'] + 1\n",
    "bread.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-22] bread의 '설립년도'별 데이터 수를 구해 년도별로 정렬하고,\n",
    "# DataFrame으로 변경하여 전치행렬을 구해 temp1 이름을 부여해 출력합니다.\n",
    "temp1 = bread['설립년도'].value_counts().sort_index(ascending=True)\n",
    "temp1 = pd.DataFrame(temp1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4-28] 설립년도가 2000년 이후이면서 영업 중인 곳의 영업기간 정보를 구합니다.\n",
    "# paris, tous에 대해 각각 구해서 temp1, temp2로 이름 붙입니다.\n",
    "temp1 = paris.loc[(paris['설립년도']>=2000) & (paris['상태코드']==1), '영업기간']\n",
    "temp2 = tous.loc[(tous['설립년도']>=2000) & (tous['상태코드']==1), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 이름을 comp로 하는 DataFrame으로 만듭니다.\n",
    "# index => ['파리바게트', '뚜레쥬르'], columns => ['영업']\n",
    "# 선생님방법\n",
    "s = pd.Series([temp1.mean(), temp2.mean()], index=['파리바게트', '뚜레쥬르'])\n",
    "comp = pd.DataFrame(s, columns=['영업'])\n",
    "comp\n",
    "\n",
    "# [4-29] 설립년도 2000년 이후이면서 폐업한 곳의 영업기간 정보를 구합니다.\n",
    "# paris, tous에 대해 각각 구해서 temp1, temp2로 이름 붙입니다.\n",
    "temp1 = paris.loc[(paris['설립년도']>=2000) & (paris['상태코드']==2), '영업기간']\n",
    "temp2 = tous.loc[(tous['설립년도']>=2000) & (tous['상태코드']==2), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 comp에 '폐업' 컬럼으로 추가합니다.\n",
    "comp['폐업']= [temp1.mean(), temp2.mean()]\n",
    "comp\n",
    "\n",
    "# [4-31] other의 2000년 이후 설립된 곳의 영업, 폐업 사업장을 구한 뒤\n",
    "# temp1, temp2 이름을 붙입니다.\n",
    "temp1 = other.loc[(other['설립년도']>=2000) & (other['상태코드']==1), '영업기간']\n",
    "temp2 = other.loc[(other['설립년도']>=2000) & (other['상태코드']==2), '영업기간']\n",
    "# temp1, temp2의 평균을 구해 comp 에 '나머지' 행으로 추가합니다.\n",
    "temp = pd.DataFrame([[temp1.mean(), temp2.mean()]], index=['나머지'], columns=['영업', '폐업'])\n",
    "comp = comp.append(temp)\n",
    "comp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형2 : 데이터 모델링 1문제 * 40 [40점]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유형3 : 통계 검증 2문제 * 15 [30점]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. 범주형 데이터 라벨링\n",
    "        1) LabelEncoding\n",
    "        2) one-hot-encoding(getdummys()?)\n",
    "    5. 뉴메리컬 데이터 스케일링\n",
    "        1) StandardScaler\n",
    "        2) MinMaxScaler\n",
    "        \n",
    "    6. 모델링\n",
    "        1) 지도학습 분류 : RandomforestClassifier\n",
    "        2) 지도학습 회귀 : RandomforestRegressor\n",
    "        3) 비지도학습 군집화\n",
    "    7. 모델 적용\n",
    "        1) train_test_split(층화 추출 옵션stratify = y)\n",
    "        2) new - fit - predict\n",
    "    8. 모델 성능 확인 : 문제에서 요구하는 평가지표에 맞게 수행\n",
    "        1) 지도 학습\n",
    "            - 정확도(accuracy)\n",
    "            - 정밀도(precision)\n",
    "            - 재현율(recall)\n",
    "            - 특이도(Specificity)\n",
    "            - f1_score : \n",
    "            - ROC(Receiver Operating Characteristic) 곡선: \n",
    "            - AUC(Are Under Curve) : \n",
    "        2) 회귀 학습\n",
    "            - SSE(Sum Squared Error) : \n",
    "            - MSE(Mean Squared Error) : \n",
    "            - RMSE(Root Mean Squared Error) :\n",
    "            - MAE(Mean Absolute Error) : \n",
    "            - 결정계수(R2) :\n",
    "            - Adjusted R2 : \n",
    "            - MSPE(Mean Squared Percentage Error) : \n",
    "            - MAPE(Mean Absolute Percentage Error) :\n",
    "            - RMSLE(Root Mean Squared Logarithmic Error) : \n",
    "            - AIC(Akaike Information Criterion) : \n",
    "            - BIC(Bayes Information Criteria) : \n",
    "        3) 비지도 학습\n",
    "            - 실루엣 계수(Silhouette) : \n",
    "            - Dunn lndex : \n",
    "            - 군집 분석\n",
    "            - 연관 분석\n",
    "    9. 예측 값 저장 \n",
    "        submission.columns([''])\n",
    "        to_csv('.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "유형3 : 데이터 검증 2문제(15*2)\n",
    "    1. 데이터 불러오기 : pd.read_csv('.csv', index_col=0)\n",
    "    2. 데이터 파악하기 : data.describe()\n",
    "    3. 데이터 검증하기\n",
    "    귀무가설(h0) 대신 대립가설(h1)을 선택할 시 오류를 범할 최대의 확률 값으로 0.05 이하이면 유의하여 대립가설 채택, 일반적으로 채택/기각은 귀무가설 기준\n",
    "        01) t-검정\n",
    "            (1)\n",
    "            (2)\n",
    "            (3)\n",
    "        02) 정규성 검정\n",
    "            (1)\n",
    "            (2)\n",
    "        03) One-way ANOVA\n",
    "        04) Two-way ANOVA\n",
    "        05) 카이제곱 검정\n",
    "            (1) 독립성 검정\n",
    "            (2) 동질성 검정\n",
    "            (3) 적합성 검정\n",
    "        06) 순위 검정\n",
    "            (1) 윌콕슨 순위합 검정\n",
    "            (2) 크루스칼 - 왈리스 검정\n",
    "        07) 부호검정\n",
    "            (1) 부호검정\n",
    "            (2) 맥네마 검정\n",
    "        08) 다변량 추론\n",
    "            (1) 다중 회귀 분석\n",
    "\n",
    "    4. 결과값 입력하기\n",
    "\n",
    "\n",
    "\n",
    "t-검정 이해하기\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
